{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Vajv7mirM1c"
      },
      "source": [
        "## Download PyG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 921
        },
        "id": "OGWoBajpmzHG",
        "outputId": "2733bd00-b226-497b-a50c-931418c56ecb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://data.pyg.org/whl/torch-1.12.0+cu113.html\n",
            "Collecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-1.12.0%2Bcu113/torch_scatter-2.1.0%2Bpt112cu113-cp38-cp38-linux_x86_64.whl (8.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.9 MB 43.3 MB/s \n",
            "\u001b[?25hCollecting torch-sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-1.12.0%2Bcu113/torch_sparse-0.6.15%2Bpt112cu113-cp38-cp38-linux_x86_64.whl (3.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 14.1 MB/s \n",
            "\u001b[?25hCollecting torch-cluster\n",
            "  Downloading https://data.pyg.org/whl/torch-1.12.0%2Bcu113/torch_cluster-1.6.0%2Bpt112cu113-cp38-cp38-linux_x86_64.whl (2.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.5 MB 19.1 MB/s \n",
            "\u001b[?25hCollecting torch-spline-conv\n",
            "  Downloading https://data.pyg.org/whl/torch-1.12.0%2Bcu113/torch_spline_conv-1.2.1%2Bpt112cu113-cp38-cp38-linux_x86_64.whl (722 kB)\n",
            "\u001b[K     |████████████████████████████████| 722 kB 74.8 MB/s \n",
            "\u001b[?25hCollecting torch-geometric\n",
            "  Downloading torch_geometric-2.2.0.tar.gz (564 kB)\n",
            "\u001b[K     |████████████████████████████████| 564 kB 29.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from torch-sparse) (1.7.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from torch-geometric) (4.64.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torch-geometric) (1.21.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from torch-geometric) (2.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torch-geometric) (2.23.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.8/dist-packages (from torch-geometric) (3.0.9)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from torch-geometric) (1.0.2)\n",
            "Collecting psutil>=5.8.0\n",
            "  Downloading psutil-5.9.4-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (280 kB)\n",
            "\u001b[K     |████████████████████████████████| 280 kB 72.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->torch-geometric) (2.0.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torch-geometric) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torch-geometric) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torch-geometric) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torch-geometric) (2022.9.24)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->torch-geometric) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->torch-geometric) (1.2.0)\n",
            "Building wheels for collected packages: torch-geometric\n",
            "  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-geometric: filename=torch_geometric-2.2.0-py3-none-any.whl size=773302 sha256=f85fc311fbc6f280d1f19d0891cd9e299d4b898b08b9dc42fd953612f70d01ef\n",
            "  Stored in directory: /root/.cache/pip/wheels/59/a3/20/198928106d3169865ae73afcbd3d3d1796cf6b429b55c65378\n",
            "Successfully built torch-geometric\n",
            "Installing collected packages: psutil, torch-spline-conv, torch-sparse, torch-scatter, torch-geometric, torch-cluster\n",
            "  Attempting uninstall: psutil\n",
            "    Found existing installation: psutil 5.4.8\n",
            "    Uninstalling psutil-5.4.8:\n",
            "      Successfully uninstalled psutil-5.4.8\n",
            "Successfully installed psutil-5.9.4 torch-cluster-1.6.0+pt112cu113 torch-geometric-2.2.0 torch-scatter-2.1.0+pt112cu113 torch-sparse-0.6.15+pt112cu113 torch-spline-conv-1.2.1+pt112cu113\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "psutil"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install torch-scatter torch-sparse torch-cluster torch-spline-conv torch-geometric -f https://data.pyg.org/whl/torch-1.12.0+cu113.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1AGQ3PkfmU4g"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch_geometric.utils import add_self_loops, degree\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "from torch_geometric.datasets import TUDataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwllrExEyrmK"
      },
      "source": [
        "# Models definition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84DuCmMwqmJ5"
      },
      "source": [
        "## Laplacian utils"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jv2-1nMlI6dP"
      },
      "source": [
        "### Builders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "mdfsEcEVrr5-"
      },
      "outputs": [],
      "source": [
        "# Copyright 2022 Twitter, Inc.\n",
        "# SPDX-License-Identifier: Apache-2.0\n",
        "\n",
        "import itertools\n",
        "import math\n",
        "\n",
        "import torch\n",
        "import torch_sparse\n",
        "\n",
        "from torch_geometric.utils import degree\n",
        "\n",
        "\n",
        "def remove_duplicate_edges(edge_index):\n",
        "    processed_edges = set()\n",
        "    new_edge_index = []\n",
        "\n",
        "    for e in range(edge_index.size(1)):\n",
        "        source, target = sorted((edge_index[0, e].item(), edge_index[1, e].item()))\n",
        "        if (source, target) in processed_edges:\n",
        "            continue\n",
        "        processed_edges.add((source, target))\n",
        "        new_edge_index.append([source, target])\n",
        "    print(f\"Removed {edge_index.size(1) - len(new_edge_index)} edges\")\n",
        "    return torch.tensor(new_edge_index, dtype=torch.long).t()\n",
        "\n",
        "\n",
        "def build_sheaf_laplacian(N, K, edge_index, maps):\n",
        "    \"\"\"\n",
        "    Builds a sheaf laplacian given the edge_index and the restriction maps\n",
        "\n",
        "    Args:\n",
        "        N: The number of nodes in the graph\n",
        "        K: The dimensionality of the Stalks\n",
        "        edge_index: Edge index of the graph without duplicate edges. We assume that edge i has orientation\n",
        "            edge_index[0, i] --> edge_index[1, i].\n",
        "        maps: Tensor of shape [edge_index.size(1), 2 (source/target), K, K] containing the restriction maps of the sheaf\n",
        "    Returns:\n",
        "        (index, value): The sheaf Laplacian as a sparse matrix of size (N*K, N*K)\n",
        "    \"\"\"\n",
        "    E = edge_index.size(1)\n",
        "    index = []\n",
        "    values = []\n",
        "\n",
        "    for e in range(E):\n",
        "        source = edge_index[0, e]\n",
        "        target = edge_index[1, e]\n",
        "\n",
        "        top_x = e * K\n",
        "        # Generate the positions in the block matrix\n",
        "        top_y = source * K\n",
        "        for i, j in itertools.product(range(K), range(K)):\n",
        "            index.append([top_x + i, top_y + j])\n",
        "            values.append(-maps[e, 0, i, j])\n",
        "\n",
        "        top_y = target * K\n",
        "        for i, j in itertools.product(range(K), range(K)):\n",
        "            index.append([top_x + i, top_y + j])\n",
        "            values.append(maps[e, 1, i, j])\n",
        "\n",
        "    index = torch.tensor(index, dtype=torch.long).T\n",
        "    values = torch.tensor(values)\n",
        "\n",
        "    index_t, values_t = torch_sparse.transpose(index, values, E * K, N * K)\n",
        "    index, value = torch_sparse.spspmm(index_t, values_t, index, values, N * K, E * K, N * K, coalesced=True)\n",
        "    return torch_sparse.coalesce(index, value, N * K, N * K)\n",
        "\n",
        "\n",
        "def sym_matrix_pow(matrix: torch.Tensor, p: float) -> torch.Tensor:\n",
        "    r\"\"\"\n",
        "    Power of a matrix using Eigen Decomposition.\n",
        "    Args:\n",
        "        matrix: a batch of matrices\n",
        "        p: power\n",
        "    Returns:\n",
        "        Power of a matrix\n",
        "    \"\"\"\n",
        "    vals, vecs = torch.linalg.eigh(matrix)\n",
        "    vals[vals > 0] = vals[vals > 0].pow(p)\n",
        "    matrix_pow = vecs @ torch.diag(vals) @ vecs.T\n",
        "    return matrix_pow\n",
        "\n",
        "\n",
        "def build_norm_sheaf_laplacian(N, K, edge_index, maps, augmented=True):\n",
        "    \"\"\"\n",
        "    Builds a normalised sheaf laplacian given the edge_index and the restriction maps.\n",
        "\n",
        "    Args:\n",
        "        N: The number of nodes in the graph\n",
        "        K: The dimensionality of the Stalks\n",
        "        edge_index: Edge index of the graph without duplicate edges. We assume that edge i has orientation\n",
        "            edge_index[0, i] --> edge_index[1, i].\n",
        "        maps: Tensor of shape [edge_index.size(1), 2 (source/target), K, K] containing the restriction maps of the sheaf\n",
        "        augmented: Use D* = D + I instead of D.\n",
        "    Returns:\n",
        "        (index, value): The normalised sheaf Laplacian as a sparse matrix of size (N*K, N*K)\n",
        "    \"\"\"\n",
        "    index, values = build_sheaf_laplacian(N, K, edge_index, maps)\n",
        "    block_diag_indices = []\n",
        "    block_diag_values = []\n",
        "\n",
        "    for i in range(N):\n",
        "        low = i * K\n",
        "        high = low + K\n",
        "\n",
        "        mask1 = torch.logical_and(low <= index[0, :], index[0, :] < high)\n",
        "        mask2 = torch.logical_and(low <= index[1, :], index[1, :] < high)\n",
        "        mask = torch.logical_and(mask1, mask2)\n",
        "\n",
        "        d_index = index[:, mask]\n",
        "        d_values = values[mask]\n",
        "        d_index = d_index - low\n",
        "\n",
        "        Dv = torch.sparse_coo_tensor(d_index, d_values).to_dense()\n",
        "        assert list(Dv.size()) == [K, K]\n",
        "        if augmented:\n",
        "            Dv = Dv + torch.eye(K, K)\n",
        "        Dv_sqrt_inv = sym_matrix_pow(Dv, -0.5).to_sparse()\n",
        "\n",
        "        block_diag_indices.append(Dv_sqrt_inv.indices() + low)\n",
        "        block_diag_values.append(Dv_sqrt_inv.values())\n",
        "\n",
        "    D_sqrt_inv_idx = torch.cat(block_diag_indices, dim=1)\n",
        "    D_sqrt_val = torch.cat(block_diag_values, dim=0)\n",
        "\n",
        "    tmp_idx, tmp_val = torch_sparse.spspmm(D_sqrt_inv_idx, D_sqrt_val, index, values, N * K, N * K, N * K,\n",
        "                                           coalesced=True)\n",
        "    index, value = torch_sparse.spspmm(tmp_idx, tmp_val, D_sqrt_inv_idx, D_sqrt_val, N * K, N * K, N * K,\n",
        "                                       coalesced=True)\n",
        "    return torch_sparse.coalesce(index, value, N * K, N * K)\n",
        "\n",
        "\n",
        "def build_sheaf_difussion_matrix(N, K, edge_index, maps, augmented=True, return_laplacian=False):\n",
        "    \"\"\"\n",
        "    Builds the difussion matrix P := I - D*^{-1/2}LD*^{-1/2}, where D* = D + I\n",
        "\n",
        "    Args:\n",
        "        N: The number of nodes in the graph\n",
        "        K: The dimensionality of the Stalks\n",
        "        edge_index: Edge index of the graph without duplicate edges. We assume that edge i has orientation\n",
        "            edge_index[0, i] --> edge_index[1, i].\n",
        "        maps: Tensor of shape [edge_index.size(1), 2 (source/target), K, K] containing the restriction maps of the sheaf\n",
        "        augmented: Use the augmented sheaf Laplacian.\n",
        "        return_laplacian: Also returns the Laplacian as a second argument.\n",
        "    Returns:\n",
        "        (index, value): The difussion matrix associated with the normalised sheaf Laplacian.\n",
        "    \"\"\"\n",
        "    L_index, L_val = build_norm_sheaf_laplacian(N, K, edge_index, maps, augmented=augmented)\n",
        "\n",
        "    I_index = torch.arange(0, N * K).view(1, -1).tile(2, 1)\n",
        "    I_val = torch.ones((N * K,))\n",
        "\n",
        "    index = torch.cat((L_index, I_index), dim=1)\n",
        "    value = torch.cat((-L_val, I_val), dim=0)\n",
        "\n",
        "    P_index, P_val = torch_sparse.coalesce(index, value, N * K, N * K, op='add')\n",
        "    if return_laplacian:\n",
        "        L_index, L_val = torch_sparse.coalesce(L_index, L_val, N * K, N * K, op='add')\n",
        "        return (P_index, P_val), (L_index, L_val)\n",
        "    return P_index, P_val\n",
        "\n",
        "\n",
        "def dirichlet_energy(L, f, size):\n",
        "    \"\"\"Returns the Dirichlet energy of the signal f under the sheaf Laplacian L.\"\"\"\n",
        "    right = torch_sparse.spmm(L[0], L[1], size, size, f)\n",
        "    energy = f.t() @ right\n",
        "    return energy.item()\n",
        "\n",
        "\n",
        "def get_edge_index_dict(edge_index, undirected=True):\n",
        "    \"\"\"Computes a dictionary mapping the undirected edges in edge_index to an ID.\"\"\"\n",
        "    assert edge_index.size(1) % 2 == 0\n",
        "\n",
        "    E = edge_index.size(1)\n",
        "    edge_idx_dict = dict()\n",
        "    next_id = 0\n",
        "\n",
        "    for e in range(E):\n",
        "        source = edge_index[0, e].item()\n",
        "        target = edge_index[1, e].item()\n",
        "        if undirected:\n",
        "            edge = tuple(sorted([source, target]))\n",
        "        else:\n",
        "            edge = tuple([source, target])\n",
        "\n",
        "        # Generate or retrieve the edge index\n",
        "        if edge not in edge_idx_dict:\n",
        "            edge_idx_dict[edge] = next_id\n",
        "            next_id += 1\n",
        "\n",
        "    return edge_idx_dict\n",
        "\n",
        "\n",
        "def compute_incidence_index(edge_index, d):\n",
        "    \"\"\"Computes the indices of a sheaf coboundary matrix from the edge_index of the graph.\"\"\"\n",
        "    assert edge_index.size(1) % 2 == 0\n",
        "\n",
        "    edge_idx_dict = get_edge_index_dict(edge_index)\n",
        "    index = []\n",
        "\n",
        "    for edge in range(edge_index.size(1)):\n",
        "        source = edge_index[0, edge].item()\n",
        "        target = edge_index[1, edge].item()\n",
        "        edge_key = tuple(sorted([source, target]))\n",
        "\n",
        "        top_x = edge_idx_dict[edge_key] * d\n",
        "        top_y = source * d\n",
        "        for i, j in itertools.product(range(d), range(d)):\n",
        "            index.append([top_x + i, top_y + j])\n",
        "\n",
        "    incidence_index = torch.tensor(index, dtype=torch.long).T\n",
        "    assert list(incidence_index.size()) == [2, edge_index.size(1) * (d ** 2)]\n",
        "    return incidence_index\n",
        "\n",
        "\n",
        "def build_dense_laplacian(size, edge_index, maps, d, normalised=False, diagonal_maps=False, values=None,\n",
        "                          edge_weights=None):\n",
        "    \"\"\"Builds a sheaf laplacian from a given graph using naive dense computations (used for testing).\"\"\"\n",
        "    assert edge_index.size(1) % 2 == 0\n",
        "    if diagonal_maps:\n",
        "        assert len(maps.size()) == 2\n",
        "        assert maps.size(1) == d\n",
        "\n",
        "    E = edge_index.size(1) // 2\n",
        "    N = size\n",
        "    Delta = torch.zeros(size=(E*d, N*d), dtype=torch.float64)\n",
        "    undirected_edge_idx_dict = get_edge_index_dict(edge_index)\n",
        "    directed_edge_idx_dict = get_edge_index_dict(edge_index, undirected=False)\n",
        "\n",
        "    for e in range(edge_index.size(1)):\n",
        "        source = edge_index[0, e].item()\n",
        "        target = edge_index[1, e].item()\n",
        "        edge_key = tuple(sorted([source, target]))\n",
        "\n",
        "        # Generate the positions in the block matrix\n",
        "        top_x = undirected_edge_idx_dict[edge_key] * d\n",
        "        top_y = source * d\n",
        "\n",
        "        orient = -1 if edge_key[0] == source else 1\n",
        "        if edge_weights is not None:\n",
        "            factor1_idx, factor2_idx = (\n",
        "                directed_edge_idx_dict[(source, target)], directed_edge_idx_dict[(target, source)])\n",
        "            assert edge_weights[factor1_idx] == edge_weights[factor2_idx]\n",
        "            maps[e] = maps[e] * edge_weights[factor1_idx]\n",
        "        if diagonal_maps:\n",
        "            diag_idx = torch.arange(0, d)\n",
        "            Delta[top_x + diag_idx, top_y + diag_idx] = orient * maps[e]\n",
        "        else:\n",
        "            Delta[top_x: top_x+d, top_y: top_y+d] = orient * maps[e]\n",
        "\n",
        "    # Compute non-normalised Laplacian.\n",
        "    L_dense = Delta.T @ Delta\n",
        "\n",
        "    if values is not None:\n",
        "        # Append extra entries to the diagonal of the parallel transport maps and update the stalk dimension.\n",
        "        L_dense, d = append_diag_maps_to_existent_laplacian(size, d, L_dense, edge_index, values)\n",
        "\n",
        "    if not normalised:\n",
        "        return L_dense\n",
        "\n",
        "    # Build normalised Laplacian.\n",
        "    D_sqrt_inv = torch.zeros((N*d, N*d), dtype=torch.float64)\n",
        "    for i in range(N):\n",
        "        low = i * d\n",
        "        high = low + d\n",
        "\n",
        "        D_i = L_dense[low:high, low:high]\n",
        "        D_i = D_i + torch.eye(d)\n",
        "        D_i_sqrt_inv = sym_matrix_pow(D_i, -0.5)\n",
        "        D_sqrt_inv[low:high, low:high] = D_i_sqrt_inv\n",
        "\n",
        "    return D_sqrt_inv @ L_dense @ D_sqrt_inv\n",
        "\n",
        "\n",
        "def append_diag_maps_to_existent_laplacian(size, learnable_d, L, edge_index, values):\n",
        "    extra_d = len(values)\n",
        "    total_d = learnable_d + extra_d\n",
        "\n",
        "    deg = degree(edge_index[0], num_nodes=size, dtype=L.dtype)\n",
        "    values = torch.tensor(values, dtype=L.dtype)\n",
        "    new_L = torch.zeros((size*(extra_d + learnable_d), size*(extra_d + learnable_d)), dtype=L.dtype)\n",
        "\n",
        "    for idx in range(edge_index.size(1)):\n",
        "        i, j = edge_index[0][idx], edge_index[1][idx]\n",
        "        assert i != j\n",
        "\n",
        "        # Add to the new Laplacian the entries of the existent Laplacian\n",
        "        new_low_i, new_high_i = i * total_d, i * total_d + learnable_d\n",
        "        new_low_j, new_high_j = j * total_d, j * total_d + learnable_d\n",
        "\n",
        "        low_i, high_i = i * learnable_d, i * learnable_d + learnable_d\n",
        "        low_j, high_j = j * learnable_d, j * learnable_d + learnable_d\n",
        "\n",
        "        new_L[new_low_i:new_high_i, new_low_j:new_high_j] = L[low_i:high_i, low_j:high_j]\n",
        "\n",
        "        # Append extra entries to each parallel transport map\n",
        "        extra_diag_idx = torch.arange(learnable_d, total_d)\n",
        "        new_L[new_low_i + extra_diag_idx, new_low_j + extra_diag_idx] = values\n",
        "\n",
        "    for i in range(size):\n",
        "        # Add to the new Laplacian diagonal, the diagonal entries of the existent Laplacian\n",
        "        new_low_i, new_high_i = i * total_d, i * total_d + learnable_d\n",
        "        low_i, high_i = i * learnable_d, i * learnable_d + learnable_d\n",
        "        new_L[new_low_i:new_high_i, new_low_i:new_high_i] = L[low_i:high_i, low_i:high_i]\n",
        "\n",
        "        # Append the degree on the diagonal for the extra entries\n",
        "        extra_diag_idx = torch.arange(learnable_d, total_d)\n",
        "        new_L[i * total_d + extra_diag_idx, i * total_d + extra_diag_idx] = deg[i]\n",
        "\n",
        "    return new_L, total_d\n",
        "\n",
        "\n",
        "def compute_left_right_map_index(edge_index, full_matrix=False):\n",
        "    \"\"\"Computes indices for lower triangular matrix or full matrix\"\"\"\n",
        "    edge_to_idx = dict()\n",
        "    for e in range(edge_index.size(1)):\n",
        "        source = edge_index[0, e].item()\n",
        "        target = edge_index[1, e].item()\n",
        "        edge_to_idx[(source, target)] = e\n",
        "\n",
        "    left_index, right_index = [], []\n",
        "    row, col = [], []\n",
        "    for e in range(edge_index.size(1)):\n",
        "        source = edge_index[0, e].item()\n",
        "        target = edge_index[1, e].item()\n",
        "        if source < target or full_matrix:\n",
        "            left_index.append(e)\n",
        "            right_index.append(edge_to_idx[(target, source)])\n",
        "\n",
        "            row.append(source)\n",
        "            col.append(target)\n",
        "\n",
        "    left_index = torch.tensor(left_index, dtype=torch.long, device=edge_index.device)\n",
        "    right_index = torch.tensor(right_index, dtype=torch.long, device=edge_index.device)\n",
        "    left_right_index = torch.vstack([left_index, right_index])\n",
        "\n",
        "    row = torch.tensor(row, dtype=torch.long, device=edge_index.device)\n",
        "    col = torch.tensor(col, dtype=torch.long, device=edge_index.device)\n",
        "    new_edge_index = torch.vstack([row, col])\n",
        "\n",
        "    if full_matrix:\n",
        "        assert len(left_index) == edge_index.size(1)\n",
        "    else:\n",
        "        assert len(left_index) == edge_index.size(1) // 2\n",
        "\n",
        "    return left_right_index, new_edge_index\n",
        "\n",
        "\n",
        "def compute_learnable_laplacian_indices(size, edge_index, learned_d, total_d):\n",
        "    assert torch.all(edge_index[0] < edge_index[1])\n",
        "\n",
        "    row, col = edge_index\n",
        "    device = edge_index.device\n",
        "    row_template = torch.arange(0, learned_d, device=device).view(1, -1, 1).tile(1, 1, learned_d)\n",
        "    col_template = torch.transpose(row_template, dim0=1, dim1=2)\n",
        "\n",
        "    non_diag_row_indices = (row_template + total_d*row.reshape(-1, 1, 1)).reshape(1, -1)\n",
        "    non_diag_col_indices = (col_template + total_d*col.reshape(-1, 1, 1)).reshape(1, -1)\n",
        "    non_diag_indices = torch.cat((non_diag_row_indices, non_diag_col_indices), dim=0)\n",
        "\n",
        "    diag = torch.arange(0, size, device=device)\n",
        "    diag_row_indices = (row_template + total_d*diag.reshape(-1, 1, 1)).reshape(1, -1)\n",
        "    diag_col_indices = (col_template + total_d*diag.reshape(-1, 1, 1)).reshape(1, -1)\n",
        "    diag_indices = torch.cat((diag_row_indices, diag_col_indices), dim=0)\n",
        "\n",
        "    return diag_indices, non_diag_indices\n",
        "\n",
        "\n",
        "def compute_learnable_diag_laplacian_indices(size, edge_index, learned_d, total_d):\n",
        "    assert torch.all(edge_index[0] < edge_index[1])\n",
        "    row, col = edge_index\n",
        "    device = edge_index.device\n",
        "    row_template = torch.arange(0, learned_d, device=device).view(1, -1)\n",
        "    col_template = row_template.clone()\n",
        "\n",
        "    non_diag_row_indices = (row_template + total_d*row.unsqueeze(1)).reshape(1, -1)\n",
        "    non_diag_col_indices = (col_template + total_d*col.unsqueeze(1)).reshape(1, -1)\n",
        "    non_diag_indices = torch.cat((non_diag_row_indices, non_diag_col_indices), dim=0)\n",
        "\n",
        "    diag = torch.arange(0, size, device=device)\n",
        "    diag_row_indices = (row_template + total_d*diag.unsqueeze(1)).reshape(1, -1)\n",
        "    diag_col_indices = (col_template + total_d*diag.unsqueeze(1)).reshape(1, -1)\n",
        "    diag_indices = torch.cat((diag_row_indices, diag_col_indices), dim=0)\n",
        "\n",
        "    return diag_indices, non_diag_indices\n",
        "\n",
        "\n",
        "def compute_fixed_diag_laplacian_indices(size, edge_index, learned_d, total_d):\n",
        "    assert torch.all(edge_index[0] < edge_index[1])\n",
        "    row, col = edge_index\n",
        "    device = edge_index.device\n",
        "    row_template = torch.arange(learned_d, total_d, device=device).view(1, -1)\n",
        "    col_template = row_template.clone()\n",
        "\n",
        "    non_diag_row_indices = (row_template + total_d*row.unsqueeze(1)).reshape(1, -1)\n",
        "    non_diag_col_indices = (col_template + total_d*col.unsqueeze(1)).reshape(1, -1)\n",
        "    non_diag_indices = torch.cat((non_diag_row_indices, non_diag_col_indices), dim=0)\n",
        "\n",
        "    diag = torch.arange(0, size, device=device)\n",
        "    diag_row_indices = (row_template + total_d*diag.unsqueeze(1)).reshape(1, -1)\n",
        "    diag_col_indices = (col_template + total_d*diag.unsqueeze(1)).reshape(1, -1)\n",
        "    diag_indices = torch.cat((diag_row_indices, diag_col_indices), dim=0)\n",
        "\n",
        "    return diag_indices, non_diag_indices\n",
        "\n",
        "\n",
        "def batched_sym_matrix_pow(matrices: torch.Tensor, p: float) -> torch.Tensor:\n",
        "    r\"\"\"\n",
        "    Power of a matrix using Eigen Decomposition.\n",
        "    Args:\n",
        "        matrices: A batch of matrices.\n",
        "        p: Power.\n",
        "        positive_definite: If positive definite\n",
        "    Returns:\n",
        "        Power of each matrix in the batch.\n",
        "    \"\"\"\n",
        "    # vals, vecs = torch.linalg.eigh(matrices)\n",
        "    # SVD is much faster than  vals, vecs = torch.linalg.eigh(matrices) for large batches.\n",
        "    vecs, vals, _ = torch.linalg.svd(matrices)\n",
        "    good = vals > vals.max(-1, True).values * vals.size(-1) * torch.finfo(vals.dtype).eps\n",
        "    vals = vals.pow(p).where(good, torch.zeros((), device=matrices.device, dtype=matrices.dtype))\n",
        "    matrix_power = (vecs * vals.unsqueeze(-2)) @ torch.transpose(vecs, -2, -1)\n",
        "    return matrix_power\n",
        "\n",
        "\n",
        "def mergesp(index1, value1, index2, value2):\n",
        "    \"\"\"Merges two sparse matrices with disjoint indices into one.\"\"\"\n",
        "    assert index1.dim() == 2 and index2.dim() == 2\n",
        "    assert value1.dim() == 1 and value2.dim() == 1\n",
        "    assert index1.size(1) == value1.numel()\n",
        "    assert index2.size(1) == value2.numel()\n",
        "    assert index1.size(0) == 2 and index2.size(0) == 2\n",
        "\n",
        "    index = torch.cat([index1, index2], dim=1)\n",
        "    val = torch.cat([value1, value2])\n",
        "    return index, val\n",
        "\n",
        "\n",
        "def get_random_edge_weights(edge_index):\n",
        "    edge_dict = get_edge_index_dict(edge_index, undirected=False)\n",
        "    edge_weights = torch.FloatTensor(size=(edge_index.size(1), 1)).uniform_(0.0, 1.0)\n",
        "\n",
        "    # Make the edge weights symmetric\n",
        "    for i in range(edge_index.size(1)):\n",
        "        v = edge_index[0, i].item()\n",
        "        u = edge_index[1, i].item()\n",
        "        edge_weights[edge_dict[(v, u)]] = edge_weights[edge_dict[(u, v)]]\n",
        "    return edge_weights\n",
        "\n",
        "\n",
        "def get_2d_oracle_rotation_angles(edge_index, y, theta=None):\n",
        "    \"\"\"Returns the class rotation angles for an oracle 2D orthogonal sheaf.\"\"\"\n",
        "    assert y.min() == 0\n",
        "    if theta is None:\n",
        "        # This is to be multiplied by 2pi during the construction of the orthogonal matrix\n",
        "        # in the Connection Laplacian builder.\n",
        "        theta = 2.0 * math.pi / (y.max() + 1)\n",
        "\n",
        "    angles = torch.empty(edge_index.size(1), dtype=torch.float32)\n",
        "    for i in range(edge_index.size(1)):\n",
        "        v = edge_index[0, i].item()\n",
        "        u = edge_index[1, i].item()\n",
        "        cdiff = abs(float(y[u].item() - y[v].item()))\n",
        "        if v < u:\n",
        "            angles[i] = theta * cdiff / 2.0\n",
        "        else:\n",
        "            angles[i] = -theta * cdiff / 2.0\n",
        "    assert angles.max() < 2 * math.pi\n",
        "    return angles.view(-1, 1)\n",
        "\n",
        "\n",
        "def get_1d_oracle_maps(edge_index, y):\n",
        "    \"\"\"Returns the maps for an oracle 2D orthogonal sheaf.\"\"\"\n",
        "    maps = torch.empty(edge_index.size(1), dtype=edge_index.dtype)\n",
        "    for i in range(edge_index.size(1)):\n",
        "        v = edge_index[0, i].item()\n",
        "        u = edge_index[1, i].item()\n",
        "        if v < u or y[v].item() == y[u].item():\n",
        "            maps[i] = 1.0\n",
        "        else:\n",
        "            maps[i] = -1.0\n",
        "    return maps.view(-1, 1)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "v-OjpbwNryl3"
      },
      "source": [
        "### Permutation utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "WRrTnd5-r2Zh"
      },
      "outputs": [],
      "source": [
        "# Copyright 2022 Twitter, Inc.\n",
        "# SPDX-License-Identifier: Apache-2.0\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "from scipy import sparse as sp\n",
        "from torch_geometric.data import Data\n",
        "\n",
        "\n",
        "def permute_graph(graph: Data, P: np.ndarray) -> Data:\n",
        "    assert graph.edge_attr is None\n",
        "\n",
        "    # Check validity of permutation matrix\n",
        "    n = graph.x.size(0)\n",
        "    if not is_valid_permutation_matrix(P, n):\n",
        "        raise AssertionError\n",
        "\n",
        "    # Apply permutation to features\n",
        "    x = graph.x.numpy()\n",
        "    x_perm = torch.FloatTensor(P @ x)\n",
        "\n",
        "    # Apply perm to labels, if per-node\n",
        "    if graph.y is None:\n",
        "        y_perm = None\n",
        "    elif graph.y.size(0) == n:\n",
        "        y = graph.y.numpy()\n",
        "        y_perm = torch.tensor(P @ y)\n",
        "    else:\n",
        "        y_perm = graph.y.clone().detach()\n",
        "\n",
        "    # Apply permutation to adjacencies, if any\n",
        "    if graph.edge_index.size(1) > 0:\n",
        "        inps = (np.ones(graph.edge_index.size(1)), (graph.edge_index[0].numpy(), graph.edge_index[1].numpy()))\n",
        "        A = sp.csr_matrix(inps, shape=(n, n))\n",
        "        P = sp.csr_matrix(P)\n",
        "        A_perm = P.dot(A).dot(P.transpose()).tocoo()\n",
        "        edge_index_perm = torch.LongTensor(np.vstack((A_perm.row, A_perm.col)))\n",
        "    else:\n",
        "        edge_index_perm = graph.edge_index.clone().detach()\n",
        "\n",
        "    # Instantiate new graph\n",
        "    graph_perm = Data(x=x_perm, edge_index=edge_index_perm, y=y_perm)\n",
        "\n",
        "    return graph_perm\n",
        "\n",
        "\n",
        "def is_valid_permutation_matrix(P: np.ndarray, n: int):\n",
        "    valid = True\n",
        "    valid &= P.ndim == 2\n",
        "    valid &= P.shape[0] == n\n",
        "    valid &= np.all(P.sum(0) == np.ones(n))\n",
        "    valid &= np.all(P.sum(1) == np.ones(n))\n",
        "    valid &= np.all(P.max(0) == np.ones(n))\n",
        "    valid &= np.all(P.max(1) == np.ones(n))\n",
        "    if n > 1:\n",
        "        valid &= np.all(P.min(0) == np.zeros(n))\n",
        "        valid &= np.all(P.min(1) == np.zeros(n))\n",
        "        valid &= not np.array_equal(P, np.eye(n))\n",
        "    return valid\n",
        "\n",
        "\n",
        "def generate_permutation_matrices(size, amount=10):\n",
        "    Ps = list()\n",
        "    random_state = np.random.RandomState()\n",
        "    count = 0\n",
        "    while count < amount:\n",
        "        I = np.eye(size)\n",
        "        perm = random_state.permutation(size)\n",
        "        P = I[perm]\n",
        "        if is_valid_permutation_matrix(P, size):\n",
        "            Ps.append(P)\n",
        "            count += 1\n",
        "\n",
        "    return Ps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qe1gwFrss2ug"
      },
      "source": [
        "## Models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OlxjXIOHvs9C"
      },
      "source": [
        "### Sheaf diffusion base model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "craDmHgEs6yD"
      },
      "outputs": [],
      "source": [
        "# Copyright 2022 Twitter, Inc.\n",
        "# SPDX-License-Identifier: Apache-2.0\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "class SheafDiffusion(nn.Module):\n",
        "    \"\"\"Base class for sheaf diffusion models.\"\"\"\n",
        "\n",
        "    def __init__(self, args):\n",
        "        super(SheafDiffusion, self).__init__()\n",
        "\n",
        "        assert args['d'] > 0\n",
        "        self.d = args['d']\n",
        "        # add low pass filters/high pass filters \n",
        "        self.add_lp = args['add_lp']\n",
        "        self.add_hp = args['add_hp']\n",
        "\n",
        "        self.final_d = self.d\n",
        "        if self.add_hp:\n",
        "            self.final_d += 1\n",
        "        if self.add_lp:\n",
        "            self.final_d += 1\n",
        "\n",
        "        self.hidden_dim = args['hidden_channels'] * self.final_d\n",
        "        self.device = args['device']\n",
        "        self.layers = args['layers']\n",
        "        self.normalised = args['normalised']\n",
        "        self.deg_normalised = args['deg_normalised']\n",
        "        self.nonlinear = not args['linear']\n",
        "        self.input_dropout = args['input_dropout']\n",
        "        self.dropout = args['dropout']\n",
        "        self.left_weights = args['left_weights']\n",
        "        self.right_weights = args['right_weights']\n",
        "        self.use_act = args['use_act']\n",
        "        self.input_dim = args['input_dim']\n",
        "        self.hidden_channels = args['hidden_channels']\n",
        "        self.output_dim = args['output_dim']\n",
        "        self.layers = args['layers']\n",
        "        self.sheaf_act = args['sheaf_act']\n",
        "        self.second_linear = args['second_linear']\n",
        "        self.orth_trans = args['orth']\n",
        "        self.use_edge_weights = args['edge_weights']\n",
        "        self.laplacian_builder = None\n",
        "        self.edges_feat = args['edges_feat']\n",
        "        self.readout = args['readout'] \n",
        "        self.input_dim_edge = args['input_dim_edge']\n",
        "        self.dense_intermediate_dim = args['dense_intermediate_dim']\n",
        "        self.dense_output_graph_dim = args['dense_output_graph_dim']\n",
        "        self.max_num_nodes_in_graph = args['max_num_nodes_in_graph']\n",
        "        self.output_nn_intermediate_dim = args['output_nn_intermediate_dim']\n",
        "        self.set_transformer_k = args['set_transformer_k']\n",
        "    \"\"\"\n",
        "    def update_edge_index(self, edge_index):\n",
        "        assert edge_index.max() <= self.graph_size\n",
        "        self.edge_index = edge_index\n",
        "        self.laplacian_builder = self.laplacian_builder.create_with_new_edge_index(edge_index)\n",
        "    \"\"\"\n",
        "\n",
        "    def grouped_parameters(self):\n",
        "        sheaf_learners, others = [], []\n",
        "        for name, param in self.named_parameters():\n",
        "            if \"sheaf_learner\" in name:\n",
        "                sheaf_learners.append(param)\n",
        "            else:\n",
        "                others.append(param)\n",
        "        assert len(sheaf_learners) > 0\n",
        "        assert len(sheaf_learners) + len(others) == len(list(self.parameters()))\n",
        "        return sheaf_learners, others\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WxpI76ss-lR"
      },
      "source": [
        "### Sheaf learners "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "SMayGF3htI6h"
      },
      "outputs": [],
      "source": [
        "# Copyright 2022 Twitter, Inc.\n",
        "# SPDX-License-Identifier: Apache-2.0\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "from typing import Tuple\n",
        "from abc import abstractmethod\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "class SheafLearner(nn.Module):\n",
        "    \"\"\"Base model that learns a sheaf from the features and the graph structure.\"\"\"\n",
        "    def __init__(self):\n",
        "        super(SheafLearner, self).__init__()\n",
        "        self.L = None\n",
        "\n",
        "    @abstractmethod\n",
        "    def forward(self, x, edge_index):\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def set_L(self, weights):\n",
        "        self.L = weights.clone().detach()\n",
        "\n",
        "\n",
        "class LocalConcatSheafLearner(SheafLearner):\n",
        "    \"\"\"Learns a sheaf by concatenating the local node features and passing them through a linear layer + activation.\"\"\"\n",
        "\n",
        "    def __init__(self, d: int, hidden_channels: int, out_shape: Tuple[int, ...], sheaf_act=\"tanh\"):\n",
        "        super(LocalConcatSheafLearner, self).__init__()\n",
        "        assert len(out_shape) in [1, 2]\n",
        "        self.out_shape = out_shape\n",
        "        self.d = d\n",
        "        self.hidden_channels = hidden_channels\n",
        "        self.linear1 = torch.nn.Linear(hidden_channels * 2, int(np.prod(out_shape)), bias=False)\n",
        "        # self.linear2 = torch.nn.Linear(self.d, 1, bias=False)\n",
        "\n",
        "        # std1 = 1.414 * math.sqrt(2. / (hidden_channels * 2 + 1))\n",
        "        # std2 = 1.414 * math.sqrt(2. / (d + 1))\n",
        "        #\n",
        "        # nn.init.normal_(self.linear1.weight, 0.0, std1)\n",
        "        # nn.init.normal_(self.linear2.weight, 0.0, std2)\n",
        "\n",
        "        if sheaf_act == 'id':\n",
        "            self.act = lambda x: x\n",
        "        elif sheaf_act == 'tanh':\n",
        "            self.act = torch.tanh\n",
        "        elif sheaf_act == 'elu':\n",
        "            self.act = F.elu\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported act {sheaf_act}\")\n",
        "\n",
        "    def forward(self, x, edge_attr, edge_index):\n",
        "        row, col = edge_index\n",
        "\n",
        "        x_row = torch.index_select(x, dim=0, index=row)\n",
        "        x_col = torch.index_select(x, dim=0, index=col)\n",
        "        x_cat = torch.cat([x_row, x_col], dim=-1)\n",
        "        x_cat = x_cat.reshape(-1, self.d, self.hidden_channels * 2).sum(dim=1)\n",
        "\n",
        "        x_cat = self.linear1(x_cat)\n",
        "\n",
        "        # x_cat = x_cat.t().reshape(-1, self.d)\n",
        "        # x_cat = self.linear2(x_cat)\n",
        "        # x_cat = x_cat.reshape(-1, edge_index.size(1)).t()\n",
        "\n",
        "        maps = self.act(x_cat)\n",
        "\n",
        "        if len(self.out_shape) == 2:\n",
        "            return maps.view(-1, self.out_shape[0], self.out_shape[1])\n",
        "        else:\n",
        "            return maps.view(-1, self.out_shape[0])\n",
        "\n",
        "class LocalConcatSheafLearnerVariant1(SheafLearner):\n",
        "    \"\"\"Learns a sheaf by concatenating the local node features and passing them through a linear layer + activation.\"\"\"\n",
        "\n",
        "    def __init__(self, d: int, hidden_channels: int, out_shape: Tuple[int, ...], sheaf_act=\"tanh\"):\n",
        "        super(LocalConcatSheafLearnerVariant1, self).__init__()\n",
        "        assert len(out_shape) in [1, 2]\n",
        "        self.out_shape = out_shape\n",
        "        self.d = d\n",
        "        self.hidden_channels = hidden_channels\n",
        "        self.linear1 = torch.nn.Linear(hidden_channels * 2 + 1, int(np.prod(out_shape)), bias=False)\n",
        "\n",
        "        if sheaf_act == 'id':\n",
        "            self.act = lambda x: x\n",
        "        elif sheaf_act == 'tanh':\n",
        "            self.act = torch.tanh\n",
        "        elif sheaf_act == 'elu':\n",
        "            self.act = F.elu\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported act {sheaf_act}\")\n",
        "\n",
        "    def forward(self, x, edge_attr, edge_index):\n",
        "        row, col = edge_index\n",
        "\n",
        "        x_row = torch.index_select(x, dim=0, index=row)\n",
        "        x_col = torch.index_select(x, dim=0, index=col)\n",
        "        x_cat = torch.cat([x_row, x_col, edge_attr], dim=-1)\n",
        "        x_cat = x_cat.reshape(-1, self.d, self.hidden_channels * 2 + 1).sum(dim=1)\n",
        "\n",
        "        x_cat = self.linear1(x_cat)\n",
        "        maps = self.act(x_cat)\n",
        "\n",
        "        if len(self.out_shape) == 2:\n",
        "            return maps.view(-1, self.out_shape[0], self.out_shape[1])\n",
        "        else:\n",
        "            return maps.view(-1, self.out_shape[0])\n",
        "\n",
        "\n",
        "class LocalConcatSheafLearnerVariant2(SheafLearner):\n",
        "    \"\"\"Learns a sheaf by concatenating the local node features and passing them through a linear layer + activation.\"\"\"\n",
        "\n",
        "    def __init__(self, d: int, hidden_channels: int, out_shape: Tuple[int, ...], sheaf_act=\"tanh\"):\n",
        "        super(LocalConcatSheafLearnerVariant2, self).__init__()\n",
        "        assert len(out_shape) in [1, 2]\n",
        "        self.out_shape = out_shape\n",
        "        self.d = d\n",
        "        self.hidden_channels = hidden_channels\n",
        "        self.linear1 = torch.nn.Linear(hidden_channels * 2, int(np.prod(out_shape)), bias=False)\n",
        "        self.linear2 = torch.nn.Linear(self.d * 2, self.d, bias=False)\n",
        "\n",
        "\n",
        "        if sheaf_act == 'id':\n",
        "            self.act = lambda x: x\n",
        "        elif sheaf_act == 'tanh':\n",
        "            self.act = torch.tanh\n",
        "        elif sheaf_act == 'elu':\n",
        "            self.act = F.elu\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported act {sheaf_act}\")\n",
        "\n",
        "    def forward(self, x, edge_attr, edge_index):\n",
        "        row, col = edge_index\n",
        "\n",
        "        x_row = torch.index_select(x, dim=0, index=row)\n",
        "        x_col = torch.index_select(x, dim=0, index=col)\n",
        "        x_cat = torch.cat([x_row, x_col], dim=-1)\n",
        "        x_cat = x_cat.reshape(-1, self.d, self.hidden_channels * 2).sum(dim=1)\n",
        "\n",
        "        x_cat = self.linear1(x_cat)\n",
        "        x_cat_act = self.act(x_cat)\n",
        "\n",
        "        x_cat_2 = self.linear2(torch.cat([x_cat_act, edge_attr], dim = -1))\n",
        "        maps = self.act(x_cat_2)\n",
        "        maps = x_cat_act + maps\n",
        "\n",
        "        if len(self.out_shape) == 2:\n",
        "            return maps.view(-1, self.out_shape[0], self.out_shape[1])\n",
        "        else:\n",
        "            return maps.view(-1, self.out_shape[0])\n",
        "\n",
        "class LocalConcatSheafLearnerVariant2(SheafLearner):\n",
        "    \"\"\"Learns a sheaf by concatenating the local node features and passing them through a linear layer + activation.\"\"\"\n",
        "\n",
        "    def __init__(self, d: int, hidden_channels: int, out_shape: Tuple[int, ...], sheaf_act=\"tanh\"):\n",
        "        super(LocalConcatSheafLearnerVariant2, self).__init__()\n",
        "        assert len(out_shape) in [1, 2]\n",
        "        self.out_shape = out_shape\n",
        "        self.d = d\n",
        "        self.hidden_channels = hidden_channels\n",
        "        self.linear1 = torch.nn.Linear(hidden_channels * 2, int(np.prod(out_shape)), bias=False)\n",
        "        self.linear2 = torch.nn.Linear(self.d * 2, self.d, bias=False)\n",
        "\n",
        "\n",
        "        if sheaf_act == 'id':\n",
        "            self.act = lambda x: x\n",
        "        elif sheaf_act == 'tanh':\n",
        "            self.act = torch.tanh\n",
        "        elif sheaf_act == 'elu':\n",
        "            self.act = F.elu\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported act {sheaf_act}\")\n",
        "\n",
        "    def forward(self, x, edge_attr, edge_index):\n",
        "        row, col = edge_index\n",
        "\n",
        "        x_row = torch.index_select(x, dim=0, index=row)\n",
        "        x_col = torch.index_select(x, dim=0, index=col)\n",
        "        x_cat = torch.cat([x_row, x_col], dim=-1)\n",
        "        x_cat = x_cat.reshape(-1, self.d, self.hidden_channels * 2).sum(dim=1)\n",
        "\n",
        "        x_cat = self.linear1(x_cat)\n",
        "        x_cat_act = self.act(x_cat)\n",
        "\n",
        "        x_cat_2 = self.linear2(torch.cat([x_cat_act, edge_attr], dim = -1))\n",
        "        maps = self.act(x_cat_2)\n",
        "        maps = x_cat_act + maps\n",
        "\n",
        "        if len(self.out_shape) == 2:\n",
        "            return maps.view(-1, self.out_shape[0], self.out_shape[1])\n",
        "        else:\n",
        "            return maps.view(-1, self.out_shape[0])\n",
        "\n",
        "class LocalConcatSheafLearnerVariant3(SheafLearner):\n",
        "    \"\"\"Learns a sheaf by concatenating the local node features and passing them through a linear layer + activation.\"\"\"\n",
        "\n",
        "    def __init__(self, d: int, hidden_channels: int, out_shape: Tuple[int, ...], sheaf_act=\"tanh\"):\n",
        "        super(LocalConcatSheafLearnerVariant3, self).__init__()\n",
        "        assert len(out_shape) in [1, 2]\n",
        "        self.out_shape = out_shape\n",
        "        self.d = d\n",
        "        self.hidden_channels = hidden_channels\n",
        "        self.linear1 = torch.nn.Linear(hidden_channels * 2, int(np.prod(out_shape)), bias=False)\n",
        "        self.bilinear = torch.nn.Bilinear(self.d, self.d, self.d, bias=False)\n",
        "\n",
        "\n",
        "        if sheaf_act == 'id':\n",
        "            self.act = lambda x: x\n",
        "        elif sheaf_act == 'tanh':\n",
        "            self.act = torch.tanh\n",
        "        elif sheaf_act == 'elu':\n",
        "            self.act = F.elu\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported act {sheaf_act}\")\n",
        "\n",
        "    def forward(self, x, edge_attr, edge_index):\n",
        "        row, col = edge_index\n",
        "\n",
        "        x_row = torch.index_select(x, dim=0, index=row)\n",
        "        x_col = torch.index_select(x, dim=0, index=col)\n",
        "        x_cat = torch.cat([x_row, x_col], dim=-1)\n",
        "        x_cat = x_cat.reshape(-1, self.d, self.hidden_channels * 2).sum(dim=1)\n",
        "\n",
        "        x_cat = self.linear1(x_cat)\n",
        "        x_cat_act = self.act(x_cat)\n",
        "\n",
        "        maps = x_cat_act + self.bilinear(x_cat_act, edge_attr)\n",
        "\n",
        "        if len(self.out_shape) == 2:\n",
        "            return maps.view(-1, self.out_shape[0], self.out_shape[1])\n",
        "        else:\n",
        "            return maps.view(-1, self.out_shape[0])\n",
        "\n",
        "\n",
        "class EdgeWeightLearner(SheafLearner):\n",
        "    \"\"\"Learns a sheaf by concatenating the local node features and passing them through a linear layer + activation.\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels: int):\n",
        "        super(EdgeWeightLearner, self).__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.linear1 = torch.nn.Linear(in_channels*2, 1, bias=False)\n",
        "        self.full_left_right_idx = None\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "\n",
        "        self.full_left_right_idx, _ = compute_left_right_map_index(edge_index, full_matrix=True)\n",
        "        _, full_right_idx = self.full_left_right_idx\n",
        "\n",
        "        row, col = edge_index\n",
        "        x_row = torch.index_select(x, dim=0, index=row)\n",
        "        x_col = torch.index_select(x, dim=0, index=col)\n",
        "        weights = self.linear1(torch.cat([x_row, x_col], dim=1))\n",
        "        weights = torch.sigmoid(weights)\n",
        "\n",
        "        edge_weights = weights * torch.index_select(weights, index=full_right_idx, dim=0)\n",
        "        return edge_weights\n",
        "\n",
        "    def update_edge_index(self, edge_index):\n",
        "        self.full_left_right_idx, _ = compute_left_right_map_index(edge_index, full_matrix=True)\n",
        "\n",
        "\n",
        "class QuadraticFormSheafLearner(SheafLearner):\n",
        "    \"\"\"Learns a sheaf by concatenating the local node features and passing them through a linear layer + activation.\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels: int, out_shape: Tuple[int]):\n",
        "        super(QuadraticFormSheafLearner, self).__init__()\n",
        "        assert len(out_shape) in [1, 2]\n",
        "        self.out_shape = out_shape\n",
        "\n",
        "        tensor = torch.eye(in_channels).unsqueeze(0).tile(int(np.prod(out_shape)), 1, 1)\n",
        "        self.tensor = nn.Parameter(tensor)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        row, col = edge_index\n",
        "        x_row = torch.index_select(x, dim=0, index=row)\n",
        "        x_col = torch.index_select(x, dim=0, index=col)\n",
        "        maps = self.map_builder(torch.cat([x_row, x_col], dim=1))\n",
        "\n",
        "        if len(self.out_shape) == 2:\n",
        "            return torch.tanh(maps).view(-1, self.out_shape[0], self.out_shape[1])\n",
        "        else:\n",
        "            return torch.tanh(maps).view(-1, self.out_shape[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHgUFdidmWYH"
      },
      "source": [
        "### Orthogonal module "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C80XHw_JnB0h"
      },
      "outputs": [],
      "source": [
        "!pip install torch-householder==1.0.1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "1Y7ZpSQrmVCK"
      },
      "outputs": [],
      "source": [
        "# Copyright 2022 Twitter, Inc.\n",
        "# SPDX-License-Identifier: Apache-2.0\n",
        "\n",
        "import math\n",
        "import torch\n",
        "\n",
        "from torch import nn\n",
        "from torch_householder import torch_householder_orgqr\n",
        "\n",
        "\n",
        "class Orthogonal(nn.Module):\n",
        "    \"\"\"Based on https://pytorch.org/docs/stable/_modules/torch/nn/utils/parametrizations.html#orthogonal\"\"\"\n",
        "    def __init__(self, d, orthogonal_map):\n",
        "        super().__init__()\n",
        "        assert orthogonal_map in [\"matrix_exp\", \"cayley\", \"householder\", \"euler\"]\n",
        "        self.d = d\n",
        "        self.orthogonal_map = orthogonal_map\n",
        "\n",
        "    def get_2d_rotation(self, params):\n",
        "        # assert params.min() >= -1.0 and params.max() <= 1.0\n",
        "        assert params.size(-1) == 1\n",
        "        sin = torch.sin(params * 2 * math.pi)\n",
        "        cos = torch.cos(params * 2 * math.pi)\n",
        "        return torch.cat([cos, -sin,\n",
        "                          sin, cos], dim=1).view(-1, 2, 2)\n",
        "\n",
        "    def get_3d_rotation(self, params):\n",
        "        assert params.min() >= -1.0 and params.max() <= 1.0\n",
        "        assert params.size(-1) == 3\n",
        "\n",
        "        alpha = params[:, 0].view(-1, 1) * 2 * math.pi\n",
        "        beta = params[:, 1].view(-1, 1) * 2 * math.pi\n",
        "        gamma = params[:, 2].view(-1, 1) * 2 * math.pi\n",
        "\n",
        "        sin_a, cos_a = torch.sin(alpha), torch.cos(alpha)\n",
        "        sin_b, cos_b = torch.sin(beta),  torch.cos(beta)\n",
        "        sin_g, cos_g = torch.sin(gamma), torch.cos(gamma)\n",
        "\n",
        "        return torch.cat(\n",
        "            [cos_a*cos_b, cos_a*sin_b*sin_g - sin_a*cos_g, cos_a*sin_b*cos_g + sin_a*sin_g,\n",
        "             sin_a*cos_b, sin_a*sin_b*sin_g + cos_a*cos_g, sin_a*sin_b*cos_g - cos_a*sin_g,\n",
        "             -sin_b, cos_b*sin_g, cos_b*cos_g], dim=1).view(-1, 3, 3)\n",
        "\n",
        "    def forward(self, params: torch.Tensor) -> torch.Tensor:\n",
        "        if self.orthogonal_map != \"euler\":\n",
        "            # Construct a lower diagonal matrix where to place the parameters.\n",
        "            offset = -1 if self.orthogonal_map == 'householder' else 0\n",
        "            tril_indices = torch.tril_indices(row=self.d, col=self.d, offset=offset, device=params.device)\n",
        "            new_params = torch.zeros(\n",
        "                (params.size(0), self.d, self.d), dtype=params.dtype, device=params.device)\n",
        "            new_params[:, tril_indices[0], tril_indices[1]] = params\n",
        "            params = new_params\n",
        "\n",
        "        if self.orthogonal_map == \"matrix_exp\" or self.orthogonal_map == \"cayley\":\n",
        "            # We just need n x k - k(k-1)/2 parameters\n",
        "            params = params.tril()\n",
        "            A = params - params.transpose(-2, -1)\n",
        "            # A is skew-symmetric (or skew-hermitian)\n",
        "            if self.orthogonal_map == \"matrix_exp\":\n",
        "                Q = torch.matrix_exp(A)\n",
        "            elif self.orthogonal_map == \"cayley\":\n",
        "                # Computes the Cayley retraction (I+A/2)(I-A/2)^{-1}\n",
        "                Id = torch.eye(self.d, dtype=A.dtype, device=A.device)\n",
        "                Q = torch.linalg.solve(torch.add(Id, A, alpha=-0.5), torch.add(Id, A, alpha=0.5))\n",
        "        elif self.orthogonal_map == 'householder':\n",
        "            eye = torch.eye(self.d, device=params.device).unsqueeze(0).repeat(params.size(0), 1, 1)\n",
        "            A = params.tril(diagonal=-1) + eye\n",
        "            Q = torch_householder_orgqr(A)\n",
        "        elif self.orthogonal_map == 'euler':\n",
        "            assert 2 <= self.d <= 3\n",
        "            if self.d == 2:\n",
        "                Q = self.get_2d_rotation(params)\n",
        "            else:\n",
        "                Q = self.get_3d_rotation(params)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported transformations {self.orthogonal_map}\")\n",
        "        return Q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFiGuayjuXfj"
      },
      "source": [
        "### Laplacian builders "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "yHHsaoQbuT09"
      },
      "outputs": [],
      "source": [
        "# Copyright 2022 Twitter, Inc.\n",
        "# SPDX-License-Identifier: Apache-2.0\n",
        "\n",
        "import torch\n",
        "\n",
        "from torch import nn\n",
        "from torch_scatter import scatter_add\n",
        "from torch_geometric.utils import degree\n",
        "\n",
        "\n",
        "class LaplacianBuilder(nn.Module):\n",
        "\n",
        "    def __init__(self, size, edge_index, d, normalised=False, deg_normalised=False, add_hp=False, add_lp=False,\n",
        "                 augmented=True):\n",
        "        super(LaplacianBuilder, self).__init__()\n",
        "        assert not (normalised and deg_normalised)\n",
        "\n",
        "        self.d = d\n",
        "        self.final_d = d\n",
        "        if add_hp:\n",
        "            self.final_d += 1\n",
        "        if add_lp:\n",
        "            self.final_d += 1\n",
        "        self.size = size\n",
        "        self.edges = edge_index.size(1) // 2\n",
        "        self.edge_index = edge_index\n",
        "        self.normalised = normalised\n",
        "        self.deg_normalised = deg_normalised\n",
        "        self.device = edge_index.device\n",
        "        self.add_hp = add_hp\n",
        "        self.add_lp = add_lp\n",
        "        self.augmented = augmented\n",
        "\n",
        "        # Preprocess the sparse indices required to compute the Sheaf Laplacian.\n",
        "        self.full_left_right_idx, _ = compute_left_right_map_index(edge_index, full_matrix=True)\n",
        "        self.left_right_idx, self.vertex_tril_idx = compute_left_right_map_index(edge_index)\n",
        "        if self.add_lp or self.add_hp:\n",
        "            self.fixed_diag_indices, self.fixed_tril_indices = compute_fixed_diag_laplacian_indices(\n",
        "                size, self.vertex_tril_idx, self.d, self.final_d)\n",
        "        self.deg = degree(self.edge_index[0], num_nodes=self.size)\n",
        "\n",
        "    def get_fixed_maps(self, size, dtype):\n",
        "        assert self.add_lp or self.add_hp\n",
        "\n",
        "        fixed_diag, fixed_non_diag = [], []\n",
        "        if self.add_lp:\n",
        "            fixed_diag.append(self.deg.view(-1, 1))\n",
        "            fixed_non_diag.append(torch.ones(size=(size, 1), device=self.device, dtype=dtype))\n",
        "        if self.add_hp:\n",
        "            fixed_diag.append(self.deg.view(-1, 1))\n",
        "            fixed_non_diag.append(-torch.ones(size=(size, 1), device=self.device, dtype=dtype))\n",
        "\n",
        "        fixed_diag = torch.cat(fixed_diag, dim=1)\n",
        "        fixed_non_diag = torch.cat(fixed_non_diag, dim=1)\n",
        "\n",
        "        assert self.fixed_tril_indices.size(1) == fixed_non_diag.numel()\n",
        "        assert self.fixed_diag_indices.size(1) == fixed_diag.numel()\n",
        "\n",
        "        return fixed_diag, fixed_non_diag\n",
        "\n",
        "    def scalar_normalise(self, diag, tril, row, col):\n",
        "        if tril.dim() > 2:\n",
        "            assert tril.size(-1) == tril.size(-2)\n",
        "            assert diag.dim() == 2\n",
        "        d = diag.size(-1)\n",
        "\n",
        "        if self.augmented:\n",
        "            diag_sqrt_inv = (diag + 1).pow(-0.5)\n",
        "        else:\n",
        "            diag_sqrt_inv = diag.pow(-0.5)\n",
        "            diag_sqrt_inv.masked_fill_(diag_sqrt_inv == float('inf'), 0)\n",
        "        diag_sqrt_inv = diag_sqrt_inv.view(-1, 1, 1) if tril.dim() > 2 else diag_sqrt_inv.view(-1, d)\n",
        "        left_norm = diag_sqrt_inv[row]\n",
        "        right_norm = diag_sqrt_inv[col]\n",
        "        non_diag_maps = left_norm * tril * right_norm\n",
        "\n",
        "        diag_sqrt_inv = diag_sqrt_inv.view(-1, 1, 1) if diag.dim() > 2 else diag_sqrt_inv.view(-1, d)\n",
        "        diag_maps = diag_sqrt_inv**2 * diag\n",
        "\n",
        "        return diag_maps, non_diag_maps\n",
        "\n",
        "    def append_fixed_maps(self, size, diag_indices, diag_maps, tril_indices, tril_maps):\n",
        "        if not self.add_lp and not self.add_hp:\n",
        "            return (diag_indices, diag_maps), (tril_indices, tril_maps)\n",
        "\n",
        "        fixed_diag, fixed_non_diag = self.get_fixed_maps(size, tril_maps.dtype)\n",
        "        tril_row, tril_col = self.vertex_tril_idx\n",
        "\n",
        "        # Normalise the fixed parts.\n",
        "        if self.normalised:\n",
        "            fixed_diag, fixed_non_diag = self.scalar_normalise(fixed_diag, fixed_non_diag, tril_row, tril_col)\n",
        "        fixed_diag, fixed_non_diag = fixed_diag.view(-1), fixed_non_diag.view(-1)\n",
        "        # Combine the learnable and fixed parts.\n",
        "        tril_indices, tril_maps = mergesp(self.fixed_tril_indices, fixed_non_diag, tril_indices, tril_maps)\n",
        "        diag_indices, diag_maps = mergesp(self.fixed_diag_indices, fixed_diag, diag_indices, diag_maps)\n",
        "\n",
        "        return (diag_indices, diag_maps), (tril_indices, tril_maps)\n",
        "\n",
        "    def create_with_new_edge_index(self, edge_index):\n",
        "        assert edge_index.max() <= self.size\n",
        "        new_builder = self.__class__(\n",
        "            self.size, edge_index, self.d,\n",
        "            normalised=self.normalised, deg_normalised=self.deg_normalised, add_hp=self.add_hp, add_lp=self.add_lp,\n",
        "            augmented=self.augmented)\n",
        "        new_builder.train(self.training)\n",
        "        return new_builder\n",
        "\n",
        "\n",
        "class DiagLaplacianBuilder(LaplacianBuilder):\n",
        "    \"\"\"Learns a a Sheaf Laplacian with diagonal restriction maps\"\"\"\n",
        "\n",
        "    def __init__(self, size, edge_index, d, normalised=False, deg_normalised=False, add_hp=False, add_lp=False,\n",
        "                 augmented=True):\n",
        "        super(DiagLaplacianBuilder, self).__init__(\n",
        "            size, edge_index, d, normalised, deg_normalised, add_hp, add_lp, augmented)\n",
        "\n",
        "        self.diag_indices, self.tril_indices = compute_learnable_diag_laplacian_indices(\n",
        "            size, self.vertex_tril_idx, self.d, self.final_d)\n",
        "\n",
        "    def normalise(self, diag, tril, row, col):\n",
        "        if self.normalised:\n",
        "            d_sqrt_inv = (diag + 1).pow(-0.5) if self.augmented else diag.pow(-0.5)\n",
        "            left_norm, right_norm = d_sqrt_inv[row], d_sqrt_inv[col]\n",
        "            tril = left_norm * tril * right_norm\n",
        "            diag = d_sqrt_inv * diag * d_sqrt_inv\n",
        "        elif self.deg_normalised:\n",
        "            deg_sqrt_inv = (self.deg + 1).pow(-0.5) if self.augmented else self.deg.pow(-0.5)\n",
        "            deg_sqrt_inv = deg_sqrt_inv.unsqueeze(-1)\n",
        "            deg_sqrt_inv.masked_fill_(deg_sqrt_inv == float('inf'), 0)\n",
        "            left_norm, right_norm = deg_sqrt_inv[row], deg_sqrt_inv[col]\n",
        "            tril = left_norm * tril * right_norm\n",
        "            diag = deg_sqrt_inv * diag * deg_sqrt_inv\n",
        "        return diag, tril\n",
        "\n",
        "    def forward(self, maps):\n",
        "        assert len(maps.size()) == 2\n",
        "        assert maps.size(1) == self.d\n",
        "        left_idx, right_idx = self.left_right_idx\n",
        "        tril_row, tril_col = self.vertex_tril_idx\n",
        "        row, _ = self.edge_index\n",
        "\n",
        "        # Compute the un-normalised Laplacian entries.\n",
        "        left_maps = torch.index_select(maps, index=left_idx, dim=0)\n",
        "        right_maps = torch.index_select(maps, index=right_idx, dim=0)\n",
        "        tril_maps = -left_maps * right_maps\n",
        "        saved_tril_maps = tril_maps.detach().clone()\n",
        "        diag_maps = scatter_add(maps**2, row, dim=0, dim_size=self.size)\n",
        "\n",
        "        # Normalise the entries if the normalised Laplacian is used.\n",
        "        diag_maps, tril_maps = self.normalise(diag_maps, tril_maps, tril_row, tril_col)\n",
        "        tril_indices, diag_indices = self.tril_indices, self.diag_indices\n",
        "        tril_maps, diag_maps = tril_maps.view(-1), diag_maps.view(-1)\n",
        "\n",
        "        # Append fixed diagonal values in the non-learnable dimensions.\n",
        "        (diag_indices, diag_maps), (tril_indices, tril_maps) = self.append_fixed_maps(\n",
        "            len(left_maps), diag_indices, diag_maps, tril_indices, tril_maps)\n",
        "\n",
        "        # Add the upper triangular part\n",
        "        triu_indices = torch.empty_like(tril_indices)\n",
        "        triu_indices[0], triu_indices[1] = tril_indices[1], tril_indices[0]\n",
        "        non_diag_indices, non_diag_values = mergesp(tril_indices, tril_maps, triu_indices, tril_maps)\n",
        "\n",
        "        # Merge diagonal and non-diagonal\n",
        "        edge_index, weights = mergesp(non_diag_indices, non_diag_values, diag_indices, diag_maps)\n",
        "\n",
        "        return (edge_index, weights), saved_tril_maps\n",
        "\n",
        "\n",
        "class NormConnectionLaplacianBuilder(LaplacianBuilder):\n",
        "    \"\"\"Learns a a Sheaf Laplacian with diagonal restriction maps\"\"\"\n",
        "\n",
        "    def __init__(self, size, edge_index, d, add_hp=False, add_lp=False, orth_map=None, augmented=True):\n",
        "        super(NormConnectionLaplacianBuilder, self).__init__(\n",
        "            size, edge_index, d, add_hp=add_hp, add_lp=add_lp, normalised=True, augmented=augmented)\n",
        "        self.orth_transform = Orthogonal(d=self.d, orthogonal_map=orth_map)\n",
        "        self.orth_map = orth_map\n",
        "\n",
        "        _, self.tril_indices = compute_learnable_laplacian_indices(\n",
        "            size, self.vertex_tril_idx, self.d, self.final_d)\n",
        "        self.diag_indices, _ = compute_learnable_diag_laplacian_indices(\n",
        "            size, self.vertex_tril_idx, self.d, self.final_d)\n",
        "\n",
        "    def create_with_new_edge_index(self, edge_index):\n",
        "        assert edge_index.max() <= self.size\n",
        "        new_builder = self.__class__(\n",
        "            self.size, edge_index, self.d, add_hp=self.add_hp, add_lp=self.add_lp, augmented=self.augmented,\n",
        "            orth_map=self.orth_map)\n",
        "        new_builder.train(self.training)\n",
        "        return new_builder\n",
        "\n",
        "    def normalise(self, diag, tril, row, col):\n",
        "        if tril.dim() > 2:\n",
        "            assert tril.size(-1) == tril.size(-2)\n",
        "            assert diag.dim() == 2\n",
        "        d = diag.size(-1)\n",
        "\n",
        "        if self.augmented:\n",
        "            diag_sqrt_inv = (diag + 1).pow(-0.5)\n",
        "        else:\n",
        "            diag_sqrt_inv = diag.pow(-0.5)\n",
        "            diag_sqrt_inv.masked_fill_(diag_sqrt_inv == float('inf'), 0)\n",
        "        diag_sqrt_inv = diag_sqrt_inv.view(-1, 1, 1) if tril.dim() > 2 else diag_sqrt_inv.view(-1, d)\n",
        "        left_norm = diag_sqrt_inv[row]\n",
        "        right_norm = diag_sqrt_inv[col]\n",
        "        non_diag_maps = left_norm * tril * right_norm\n",
        "\n",
        "        diag_sqrt_inv = diag_sqrt_inv.view(-1, 1, 1) if diag.dim() > 2 else diag_sqrt_inv.view(-1, d)\n",
        "        diag_maps = diag_sqrt_inv**2 * diag\n",
        "\n",
        "        return diag_maps, non_diag_maps\n",
        "\n",
        "    def forward(self, map_params, edge_weights=None):\n",
        "        if edge_weights is not None:\n",
        "            assert edge_weights.size(1) == 1\n",
        "        assert len(map_params.size()) == 2\n",
        "        if self.orth_map in [\"matrix_exp\", \"cayley\"]:\n",
        "            assert map_params.size(1) == self.d * (self.d + 1) // 2\n",
        "        else:\n",
        "            assert map_params.size(1) == self.d * (self.d - 1) // 2\n",
        "\n",
        "        _, full_right_idx = self.full_left_right_idx\n",
        "        left_idx, right_idx = self.left_right_idx\n",
        "        tril_row, tril_col = self.vertex_tril_idx\n",
        "        tril_indices, diag_indices = self.tril_indices, self.diag_indices\n",
        "        row, _ = self.edge_index\n",
        "\n",
        "        # Convert the parameters to orthogonal matrices.\n",
        "        maps = self.orth_transform(map_params)\n",
        "        if edge_weights is None:\n",
        "            diag_maps = self.deg.unsqueeze(-1)\n",
        "        else:\n",
        "            diag_maps = scatter_add(edge_weights ** 2, row, dim=0, dim_size=self.size)\n",
        "            maps = maps * edge_weights.unsqueeze(-1)\n",
        "\n",
        "        # Compute the transport maps.\n",
        "        left_maps = torch.index_select(maps, index=left_idx, dim=0)\n",
        "        right_maps = torch.index_select(maps, index=right_idx, dim=0)\n",
        "        tril_maps = -torch.bmm(torch.transpose(left_maps, -1, -2), right_maps)\n",
        "        saved_tril_maps = tril_maps.detach().clone()\n",
        "\n",
        "        # Normalise the entries if the normalised Laplacian is used.\n",
        "        diag_maps, tril_maps = self.scalar_normalise(diag_maps, tril_maps, tril_row, tril_col)\n",
        "        tril_maps, diag_maps = tril_maps.view(-1), diag_maps.expand(-1, self.d).reshape(-1)\n",
        "\n",
        "        # Append fixed diagonal values in the non-learnable dimensions.\n",
        "        (diag_indices, diag_maps), (tril_indices, tril_maps) = self.append_fixed_maps(\n",
        "            len(left_maps), diag_indices, diag_maps, tril_indices, tril_maps)\n",
        "\n",
        "        # Add the upper triangular part\n",
        "        triu_indices = torch.empty_like(tril_indices)\n",
        "        triu_indices[0], triu_indices[1] = tril_indices[1], tril_indices[0]\n",
        "        non_diag_indices, non_diag_values = mergesp(tril_indices, tril_maps, triu_indices, tril_maps)\n",
        "\n",
        "        # Merge diagonal and non-diagonal\n",
        "        edge_index, weights = mergesp(non_diag_indices, non_diag_values, diag_indices, diag_maps)\n",
        "\n",
        "        return (edge_index, weights), saved_tril_maps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LdsNxcEvW2v"
      },
      "source": [
        "### Discrete models "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CyMDlsv_TeT"
      },
      "source": [
        "#### Imports "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "O32DaUKV_Vne"
      },
      "outputs": [],
      "source": [
        "from IPython.core.display import display_html\n",
        "# Copyright 2022 Twitter, Inc.\n",
        "# SPDX-License-Identifier: Apache-2.0\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch_sparse\n",
        "from torch_geometric.nn import SAGPooling\n",
        "from torch_geometric.nn import global_mean_pool, global_max_pool, global_add_pool\n",
        "from torch import nn\n",
        "from torch_geometric.utils import to_dense_batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9bkUAmD_am_"
      },
      "source": [
        "#### Diagonal sheaf diffusion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "qreZXhSN_f6y"
      },
      "outputs": [],
      "source": [
        "class DiscreteDiagSheafDiffusion(SheafDiffusion):\n",
        "\n",
        "    def __init__(self, args):\n",
        "        super(DiscreteDiagSheafDiffusion, self).__init__(args)\n",
        "        assert args['d'] > 0\n",
        "\n",
        "        self.lin_right_weights = nn.ModuleList()\n",
        "        self.lin_left_weights = nn.ModuleList()\n",
        "\n",
        "        self.batch_norms = nn.ModuleList()\n",
        "        if self.right_weights:\n",
        "            for i in range(self.layers):\n",
        "                self.lin_right_weights.append(nn.Linear(self.hidden_channels, self.hidden_channels, bias=False))\n",
        "                nn.init.orthogonal_(self.lin_right_weights[-1].weight.data)\n",
        "        if self.left_weights:\n",
        "            for i in range(self.layers):\n",
        "                self.lin_left_weights.append(nn.Linear(self.final_d, self.final_d, bias=False))\n",
        "                nn.init.eye_(self.lin_left_weights[-1].weight.data)\n",
        "\n",
        "        self.sheaf_learners = nn.ModuleList()\n",
        "\n",
        "        # Define the restriction map with a specific edge handling technique \n",
        "        num_sheaf_learners = min(self.layers, self.layers if self.nonlinear else 1)\n",
        "        for i in range(num_sheaf_learners):\n",
        "            if self.edges_feat == \"none\":    \n",
        "                self.sheaf_learners.append(LocalConcatSheafLearner(self.final_d,\n",
        "                    self.hidden_channels, out_shape=(self.d,), sheaf_act=self.sheaf_act))\n",
        "            elif self.edges_feat == \"concat\":\n",
        "                self.sheaf_learners.append(LocalConcatSheafLearnerVariant1(self.final_d,\n",
        "                    self.hidden_channels, out_shape=(self.d,), sheaf_act=self.sheaf_act))\n",
        "            elif self.edges_feat == \"linear\":\n",
        "                self.sheaf_learners.append(LocalConcatSheafLearnerVariant2(self.final_d,\n",
        "                    self.hidden_channels, out_shape=(self.d,), sheaf_act=self.sheaf_act))\n",
        "            elif self.edges_feat == \"bilinear\":\n",
        "                self.sheaf_learners.append(LocalConcatSheafLearnerVariant3(self.final_d,\n",
        "                    self.hidden_channels, out_shape=(self.d,), sheaf_act=self.sheaf_act))\n",
        "                \n",
        "        # Define linear map for edge features only if needed\n",
        "        if self.input_dim_edge > 0 and self.edges_feat != \"none\":\n",
        "            self.lin3 = nn.Linear(self.input_dim_edge, self.final_d)\n",
        "\n",
        "        self.epsilons = nn.ParameterList()\n",
        "        for i in range(self.layers):\n",
        "            self.epsilons.append(nn.Parameter(torch.zeros((self.final_d, 1))))\n",
        "\n",
        "        # input dim = f1 (n. of input channels) \n",
        "        # hidden dim = n. of hidden channels * d\n",
        "        # final_d = d\n",
        "        # lin1 --> linear layer to produce hidden_dim features from input_dim features \n",
        "        self.lin1 = nn.Linear(self.input_dim, self.hidden_dim)\n",
        "        if self.second_linear:\n",
        "            # Additional linear layer \n",
        "            self.lin12 = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
        "\n",
        "        # Mean, max, sum readout\n",
        "        if self.readout in [\"mean\", \"max\", \"sum\"]:\n",
        "\n",
        "            # Output linear layer \n",
        "            self.lin2 = nn.Linear(self.hidden_dim, self.output_dim)\n",
        "\n",
        "\n",
        "        # Concat readout\n",
        "        if self.readout == \"concat\":\n",
        "\n",
        "            # Output linear layer \n",
        "            self.lin2 = nn.Linear(self.hidden_dim, self.output_dim)\n",
        "            self.lin4 = nn.Linear(self.hidden_dim*3, self.hidden_dim)\n",
        "\n",
        "        # SAG readout layer\n",
        "        if self.readout == \"sag\":\n",
        "\n",
        "            self.pooling_layers = SAGPooling(self.hidden_channels * self.final_d, 0.3)\n",
        "\n",
        "            # Output linear layer \n",
        "            self.lin4 = nn.Linear(self.hidden_dim*3, self.hidden_dim)\n",
        "            self.lin2 = nn.Linear(self.hidden_dim, self.output_dim)\n",
        "\n",
        "        # MLP readout layer\n",
        "        if self.readout == \"mlp\":\n",
        "\n",
        "          self.gnn_output_node_dim = self.hidden_dim\n",
        "\n",
        "          # MLP readout\n",
        "          self.dense_agg = torch.nn.Sequential(\n",
        "              nn.Linear(in_features=self.max_num_nodes_in_graph * self.gnn_output_node_dim, out_features=self.dense_intermediate_dim),\n",
        "              nn.BatchNorm1d(self.dense_intermediate_dim),\n",
        "              nn.ReLU(),\n",
        "              nn.Dropout(p=0.7), # Originally there was not\n",
        "              nn.Linear(in_features=self.dense_intermediate_dim, out_features=self.dense_output_graph_dim),\n",
        "              nn.BatchNorm1d(self.dense_output_graph_dim),\n",
        "              nn.ReLU(),\n",
        "              nn.Dropout(p=0.7)  # Original p = 0.4\n",
        "          )\n",
        "          # Output NN for MLP\n",
        "          self.output_nn = torch.nn.Sequential(\n",
        "          nn.Linear(in_features=self.dense_output_graph_dim, out_features=self.output_nn_intermediate_dim),\n",
        "          nn.ReLU(),\n",
        "          nn.Dropout(p=0.4),\n",
        "          nn.Linear(in_features=self.output_nn_intermediate_dim, out_features=self.output_dim)\n",
        "          #nn.Linear(in_features=self.dense_output_graph_dim, out_features=self.output_dim)\n",
        "          )\n",
        "\n",
        "\n",
        "    def forward(self, x, edge_index, edge_attr, batch):\n",
        "        x = F.dropout(x, p=self.input_dropout, training=self.training)\n",
        "        #(n * b, input features) -> (n* b, hidden feat * d)\n",
        "        x = self.lin1(x)\n",
        "        if self.use_act:\n",
        "            x = F.elu(x)\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        if self.second_linear:\n",
        "            x = self.lin12(x)\n",
        "        dim = x.size(dim=0)\n",
        "        x = x.view(dim * self.final_d, -1)\n",
        "\n",
        "        if self.input_dim_edge > 0 and self.edges_feat != \"none\":\n",
        "            #(e * b, input_edge features) -> (e* b,hidden feat * d)\n",
        "            edge_attr = self.lin3(edge_attr)\n",
        "            if self.use_act:\n",
        "              edge_attr = torch.tanh(edge_attr)\n",
        "\n",
        "        laplacian_builder = DiagLaplacianBuilder(dim, edge_index, d=self.d,\n",
        "                                                         normalised=self.normalised,\n",
        "                                                         deg_normalised=self.deg_normalised,\n",
        "                                                         add_hp=self.add_hp, add_lp=self.add_lp)\n",
        "        x0 = x\n",
        "        for layer in range(self.layers):\n",
        "\n",
        "            if layer == 0 or self.nonlinear:\n",
        "                x_maps = F.dropout(x, p=self.dropout if layer > 0 else 0., training=self.training)\n",
        "                maps = self.sheaf_learners[layer](x_maps.reshape(dim, -1), edge_attr, edge_index)\n",
        "                L, trans_maps = laplacian_builder(maps)\n",
        "                self.sheaf_learners[layer].set_L(trans_maps)\n",
        "\n",
        "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "\n",
        "            if self.left_weights:\n",
        "                x = x.t().reshape(-1, self.final_d)\n",
        "                x = self.lin_left_weights[layer](x)\n",
        "                x = x.reshape(-1, dim * self.final_d).t()\n",
        "\n",
        "            if self.right_weights:\n",
        "                x = self.lin_right_weights[layer](x)\n",
        "\n",
        "            x = torch_sparse.spmm(L[0], L[1], x.size(0), x.size(0), x)\n",
        "\n",
        "            if self.use_act:\n",
        "                x = F.elu(x)\n",
        "\n",
        "            coeff = (1 + torch.tanh(self.epsilons[layer]).tile(dim, 1))\n",
        "            x0 = coeff * x0 - x\n",
        "            x = x0\n",
        "\n",
        "        # Readout layers\n",
        "        x = x.reshape(dim, -1)\n",
        "\n",
        "        if self.readout == \"mean\":\n",
        "            # Simple mean readout\n",
        "            x = global_mean_pool(x, batch)  # [batch_size, hidden_channels = hidden feat * d]\n",
        "\n",
        "            # Linear classifier\n",
        "            x = self.lin2(x)\n",
        "\n",
        "        if self.readout == \"sum\":\n",
        "            # Simple sum readout\n",
        "            x = global_add_pool(x, batch) \n",
        "\n",
        "            # Linear classifier\n",
        "            x = self.lin2(x)\n",
        "\n",
        "        if self.readout == \"max\":\n",
        "            # Simple max readout\n",
        "            x = global_max_pool(x, batch) \n",
        "\n",
        "            # Linear classifier\n",
        "            x = self.lin2(x)\n",
        "\n",
        "        if self.readout == \"concat\":\n",
        "            # Concatenation of [mean, sum, max] readout\n",
        "            x = torch.cat([global_mean_pool(x, batch), global_add_pool(x, batch), global_max_pool(x, batch)], dim=1)\n",
        "            x = self.lin4(x)\n",
        "            x = F.elu(x)\n",
        "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "\n",
        "            # Linear classifier\n",
        "            x = self.lin2(x)\n",
        "\n",
        "        if self.readout == \"sag\":\n",
        "            # SAG pooling layer\n",
        "            x, edge_index, _, batch, _, _ = self.pooling_layers(x, edge_index, batch = batch) \n",
        "            x = torch.cat([global_mean_pool(x, batch), global_add_pool(x, batch), global_max_pool(x, batch)], dim=1)\n",
        "            x = self.lin4(x)\n",
        "            x = F.elu(x)\n",
        "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "\n",
        "            # Linear classifier\n",
        "            x = self.lin2(x)\n",
        "\n",
        "        if self.readout == \"mlp\":\n",
        "            graph_x, _ = to_dense_batch(x, batch, fill_value=0, max_num_nodes=self.max_num_nodes_in_graph)\n",
        "            graph_x = self.dense_agg(graph_x.view(-1, graph_x.shape[1] * graph_x.shape[2]))\n",
        "\n",
        "            # MLP classifier\n",
        "            x = self.output_nn(graph_x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zijOLV2iGllS"
      },
      "source": [
        "#### Diagonal sheaf diffusion with alternate sheaf diffusion/pooling layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "QN9A63c9IxKV"
      },
      "outputs": [],
      "source": [
        "class DiscreteDiagPoolSheafDiffusion(SheafDiffusion):\n",
        "\n",
        "    def __init__(self, args):\n",
        "        super(DiscreteDiagPoolSheafDiffusion, self).__init__(args)\n",
        "        assert args['d'] > 0\n",
        "\n",
        "        self.lin_right_weights = nn.ModuleList()\n",
        "        self.lin_left_weights = nn.ModuleList()\n",
        "\n",
        "        self.batch_norms = nn.ModuleList()\n",
        "        if self.right_weights:\n",
        "            for i in range(self.layers):\n",
        "                self.lin_right_weights.append(nn.Linear(self.hidden_channels, self.hidden_channels, bias=False))\n",
        "                nn.init.orthogonal_(self.lin_right_weights[-1].weight.data)\n",
        "        if self.left_weights:\n",
        "            for i in range(self.layers):\n",
        "                self.lin_left_weights.append(nn.Linear(self.final_d, self.final_d, bias=False))\n",
        "                nn.init.eye_(self.lin_left_weights[-1].weight.data)\n",
        "\n",
        "        # Definition of SAG Pooling layers, one for each layer \n",
        "        self.pooling_layers = nn.ModuleList()\n",
        "        for i in range(self.layers):\n",
        "            self.pooling_layers.append(SAGPooling(self.hidden_channels * self.final_d, 0.4))\n",
        "\n",
        "        # Readout results\n",
        "        self.readouts = []\n",
        "\n",
        "        # Define the restriction map with a specific edge handling technique \n",
        "        self.sheaf_learners = nn.ModuleList()\n",
        "        num_sheaf_learners = min(self.layers, self.layers if self.nonlinear else 1)\n",
        "        for i in range(num_sheaf_learners):\n",
        "            if self.edges_feat == \"none\":    \n",
        "                self.sheaf_learners.append(LocalConcatSheafLearner(self.final_d,\n",
        "                    self.hidden_channels, out_shape=(self.d,), sheaf_act=self.sheaf_act))\n",
        "            elif self.edges_feat == \"concat\":\n",
        "                self.sheaf_learners.append(LocalConcatSheafLearnerVariant1(self.final_d,\n",
        "                    self.hidden_channels, out_shape=(self.d,), sheaf_act=self.sheaf_act))\n",
        "            elif self.edges_feat == \"linear\":\n",
        "                self.sheaf_learners.append(LocalConcatSheafLearnerVariant2(self.final_d,\n",
        "                    self.hidden_channels, out_shape=(self.d,), sheaf_act=self.sheaf_act))\n",
        "            elif self.edges_feat == \"bilinear\":\n",
        "                self.sheaf_learners.append(LocalConcatSheafLearnerVariant3(self.final_d,\n",
        "                    self.hidden_channels, out_shape=(self.d,), sheaf_act=self.sheaf_act))\n",
        "\n",
        "        # Define linear map for edge features only if needed\n",
        "        if self.input_dim_edge > 0 and self.edges_feat != \"none\":\n",
        "            self.lin3 = nn.Linear(self.input_dim_edge, self.final_d)\n",
        "\n",
        "        self.epsilons = nn.ParameterList()\n",
        "        for i in range(self.layers):\n",
        "            self.epsilons.append(nn.Parameter(torch.zeros((self.final_d, 1))))\n",
        "\n",
        "        self.lin1 = nn.Linear(self.input_dim, self.hidden_dim)\n",
        "        if self.second_linear:\n",
        "            self.lin12 = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
        "        self.lin2 = nn.Linear(self.hidden_dim * 3, self.output_dim)\n",
        "\n",
        "    def forward(self, x, edge_index, edge_attr, batch):\n",
        "        x = F.dropout(x, p=self.input_dropout, training=self.training)\n",
        "        #(n * b, input features) -> (n* b, hidden feat * d)\n",
        "        x = self.lin1(x)\n",
        "        if self.use_act:\n",
        "            x = F.elu(x)\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        if self.second_linear:\n",
        "            x = self.lin12(x)\n",
        "        dim = x.size(dim=0)\n",
        "        x = x.view(dim * self.final_d, -1)\n",
        "\n",
        "        if self.input_dim_edge > 0 and self.edges_feat != \"none\":\n",
        "            #(e * b, input_edge features) -> (e* b,hidden feat * d)\n",
        "            edge_attr = self.lin3(edge_attr)\n",
        "            if self.use_act:\n",
        "              edge_attr = torch.tanh(edge_attr)\n",
        "\n",
        "        x0 = x\n",
        "        for layer in range(self.layers):\n",
        "\n",
        "            laplacian_builder = DiagLaplacianBuilder(dim, edge_index, d=self.d,\n",
        "                                                            normalised=self.normalised,\n",
        "                                                            deg_normalised=self.deg_normalised,\n",
        "                                                            add_hp=self.add_hp, add_lp=self.add_lp)\n",
        "            #for weight in self.lin_left_weights.parameters():\n",
        "              #print(\"Left weights: \", weight)\n",
        "            #print(\"Sheaf learner:\", self.sheaf_learners.weight)\n",
        "            #for weight in self.sheaf_learners.parameters():\n",
        "              #print(\"Sheaf weights: \", weight)\n",
        "            if layer == 0 or self.nonlinear:\n",
        "                x_maps = F.dropout(x, p=self.dropout if layer > 0 else 0., training=self.training)\n",
        "                maps = self.sheaf_learners[layer](x_maps.reshape(dim, -1), edge_attr, edge_index)\n",
        "                L, trans_maps = laplacian_builder(maps)\n",
        "                self.sheaf_learners[layer].set_L(trans_maps)\n",
        "\n",
        "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "\n",
        "            if self.left_weights:\n",
        "                x = x.t().reshape(-1, self.final_d)\n",
        "                x = self.lin_left_weights[layer](x)\n",
        "                x = x.reshape(-1, dim * self.final_d).t()\n",
        "\n",
        "            if self.right_weights:\n",
        "                x = self.lin_right_weights[layer](x)\n",
        "\n",
        "            x = torch_sparse.spmm(L[0], L[1], x.size(0), x.size(0), x)\n",
        "\n",
        "            if self.use_act:\n",
        "                x = F.elu(x)\n",
        "\n",
        "            coeff = (1 + torch.tanh(self.epsilons[layer]).tile(dim, 1))\n",
        "            x = coeff * x0 - x\n",
        "\n",
        "            # Pooling layer \n",
        "            x = x.reshape(dim, -1)\n",
        "            x, edge_index, edge_attr, batch, _, _ = self.pooling_layers[layer](x, edge_index, edge_attr, batch = batch) \n",
        "\n",
        "            dim = x.size(dim=0)\n",
        "            x = x.view(dim * self.final_d, -1)\n",
        "\n",
        "            x0 = x\n",
        "\n",
        "\n",
        "        # Readout layer\n",
        "        x = x.reshape(dim, -1)\n",
        "        x = torch.cat([global_mean_pool(x, batch), global_add_pool(x, batch), global_max_pool(x, batch)], dim=1)\n",
        "\n",
        "        # Linear classifier\n",
        "        x = self.lin2(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xE1dlDA__sN9"
      },
      "source": [
        "#### Bundle sheaf diffusion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "G2UT2Aqp_rHF"
      },
      "outputs": [],
      "source": [
        "class DiscreteBundleSheafDiffusion(SheafDiffusion):\n",
        "\n",
        "    def __init__(self, args):\n",
        "        super(DiscreteBundleSheafDiffusion, self).__init__(args)\n",
        "        assert args['d'] > 1\n",
        "        assert not self.deg_normalised\n",
        "\n",
        "        self.lin_right_weights = nn.ModuleList()\n",
        "        self.lin_left_weights = nn.ModuleList()\n",
        "\n",
        "        self.batch_norms = nn.ModuleList()\n",
        "        if self.right_weights:\n",
        "            for i in range(self.layers):\n",
        "                self.lin_right_weights.append(nn.Linear(self.hidden_channels, self.hidden_channels, bias=False))\n",
        "                nn.init.orthogonal_(self.lin_right_weights[-1].weight.data)\n",
        "        if self.left_weights:\n",
        "            for i in range(self.layers):\n",
        "                self.lin_left_weights.append(nn.Linear(self.final_d, self.final_d, bias=False))\n",
        "                nn.init.eye_(self.lin_left_weights[-1].weight.data)\n",
        "\n",
        "        self.sheaf_learners = nn.ModuleList()\n",
        "        self.weight_learners = nn.ModuleList()\n",
        "\n",
        "        # Define the restriction map with a specific edge handling technique \n",
        "        num_sheaf_learners = min(self.layers, self.layers if self.nonlinear else 1)\n",
        "        for i in range(num_sheaf_learners):\n",
        "            if self.use_edge_weights:\n",
        "                self.weight_learners.append(EdgeWeightLearner(self.hidden_dim))\n",
        "            if self.edges_feat == \"none\":    \n",
        "                self.sheaf_learners.append(LocalConcatSheafLearner(self.final_d,\n",
        "                    self.hidden_channels, out_shape=(self.d,), sheaf_act=self.sheaf_act))\n",
        "            elif self.edges_feat == \"concat\":\n",
        "                self.sheaf_learners.append(LocalConcatSheafLearnerVariant1(self.final_d,\n",
        "                    self.hidden_channels, out_shape=(self.d,), sheaf_act=self.sheaf_act))\n",
        "            elif self.edges_feat == \"linear\":\n",
        "                self.sheaf_learners.append(LocalConcatSheafLearnerVariant2(self.final_d,\n",
        "                    self.hidden_channels, out_shape=(self.d,), sheaf_act=self.sheaf_act))\n",
        "            elif self.edges_feat == \"bilinear\":\n",
        "                self.sheaf_learners.append(LocalConcatSheafLearnerVariant3(self.final_d,\n",
        "                    self.hidden_channels, out_shape=(self.d,), sheaf_act=self.sheaf_act))\n",
        "\n",
        "        # Define linear map for edge features only if needed\n",
        "        if self.input_dim_edge > 0 and self.edges_feat != \"none\":\n",
        "            self.lin3 = nn.Linear(self.input_dim_edge, self.final_d)\n",
        "\n",
        "        self.epsilons = nn.ParameterList()\n",
        "        for i in range(self.layers):\n",
        "            self.epsilons.append(nn.Parameter(torch.zeros((self.final_d, 1))))\n",
        "\n",
        "        self.lin1 = nn.Linear(self.input_dim, self.hidden_dim)\n",
        "        if self.second_linear:\n",
        "            self.lin12 = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
        "\n",
        "        # Mean, max, sum readout\n",
        "        if self.readout in [\"mean\", \"max\", \"sum\"]:\n",
        "\n",
        "            # Output linear layer \n",
        "            self.lin2 = nn.Linear(self.hidden_dim, self.output_dim)\n",
        "\n",
        "\n",
        "        # Concat readout\n",
        "        if self.readout == \"concat\":\n",
        "\n",
        "            # Output linear layer \n",
        "            self.lin2 = nn.Linear(self.hidden_dim, self.output_dim)\n",
        "            self.lin4 = nn.Linear(self.hidden_dim*3, self.hidden_dim)\n",
        "\n",
        "        # SAG readout layer\n",
        "        if self.readout == \"sag\":\n",
        "\n",
        "            self.pooling_layers = SAGPooling(self.hidden_channels * self.final_d, 0.3)\n",
        "\n",
        "            # Output linear layer \n",
        "            self.lin4 = nn.Linear(self.hidden_dim*3, self.hidden_dim)\n",
        "            self.lin2 = nn.Linear(self.hidden_dim, self.output_dim)\n",
        "\n",
        "        # MLP readout layer\n",
        "        if self.readout == \"mlp\":\n",
        "\n",
        "          self.gnn_output_node_dim = self.hidden_dim\n",
        "\n",
        "          # MLP readout\n",
        "          self.dense_agg = torch.nn.Sequential(\n",
        "              nn.Linear(in_features=self.max_num_nodes_in_graph * self.gnn_output_node_dim, out_features=self.dense_intermediate_dim),\n",
        "              nn.BatchNorm1d(self.dense_intermediate_dim),\n",
        "              nn.ReLU(),\n",
        "              nn.Dropout(p=0.7), # Originally there was not\n",
        "              nn.Linear(in_features=self.dense_intermediate_dim, out_features=self.dense_output_graph_dim),\n",
        "              nn.BatchNorm1d(self.dense_output_graph_dim),\n",
        "              nn.ReLU(),\n",
        "              nn.Dropout(p=0.7)  # Original p = 0.4\n",
        "          )\n",
        "          # Output NN for MLP\n",
        "          self.output_nn = torch.nn.Sequential(\n",
        "          nn.Linear(in_features=self.dense_output_graph_dim, out_features=self.output_nn_intermediate_dim),\n",
        "          nn.ReLU(),\n",
        "          nn.Dropout(p=0.4),\n",
        "          nn.Linear(in_features=self.output_nn_intermediate_dim, out_features=self.output_dim)\n",
        "          #nn.Linear(in_features=self.dense_output_graph_dim, out_features=self.output_dim)\n",
        "          )\n",
        "\n",
        "\n",
        "    def get_param_size(self):\n",
        "        if self.orth_trans in ['matrix_exp', 'cayley']:\n",
        "            return self.d * (self.d + 1) // 2\n",
        "        else:\n",
        "            return self.d * (self.d - 1) // 2\n",
        "\n",
        "\n",
        "    def update_edge_index(self, edge_index):\n",
        "        super().update_edge_index(edge_index)\n",
        "        for weight_learner in self.weight_learners:\n",
        "            weight_learner.update_edge_index(edge_index)\n",
        "\n",
        "    def forward(self, x, edge_index, edge_attr, batch):\n",
        "        x = F.dropout(x, p=self.input_dropout, training=self.training)\n",
        "        #(n * b, input features) -> (n* b, hidden feat * d)\n",
        "        x = self.lin1(x)\n",
        "        if self.use_act:\n",
        "            x = F.elu(x)\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        if self.second_linear:\n",
        "            x = self.lin12(x)\n",
        "        dim = x.size(dim=0)\n",
        "        x = x.view(dim * self.final_d, -1)\n",
        "\n",
        "        if self.input_dim_edge > 0 and self.edges_feat != \"none\":\n",
        "            #(e * b, input_edge features) -> (e* b,hidden feat * d)\n",
        "            edge_attr = self.lin3(edge_attr)\n",
        "            if self.use_act:\n",
        "              edge_attr = torch.tanh(edge_attr)\n",
        "\n",
        "        laplacian_builder = NormConnectionLaplacianBuilder(\n",
        "            dim, edge_index, d=self.d, add_hp=self.add_hp,\n",
        "            add_lp=self.add_lp, orth_map=self.orth_trans)\n",
        "\n",
        "        x0, L = x, None\n",
        "        for layer in range(self.layers):\n",
        "            if layer == 0 or self.nonlinear:\n",
        "                x_maps = F.dropout(x, p=self.dropout if layer > 0 else 0., training=self.training)\n",
        "                x_maps = x_maps.reshape(dim, -1)\n",
        "                maps = self.sheaf_learners[layer](x_maps, edge_attr, edge_index)\n",
        "                edge_weights = self.weight_learners[layer](x_maps, edge_index) if self.use_edge_weights else None\n",
        "                L, trans_maps = laplacian_builder(maps, edge_weights)\n",
        "                self.sheaf_learners[layer].set_L(trans_maps)\n",
        "\n",
        "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "\n",
        "            if self.left_weights:\n",
        "                x = x.t().reshape(-1, self.final_d)\n",
        "                x = self.lin_left_weights[layer](x)\n",
        "                x = x.reshape(-1, dim * self.final_d).t()\n",
        "\n",
        "            if self.right_weights:\n",
        "                x = self.lin_right_weights[layer](x)\n",
        "\n",
        "            # Use the adjacency matrix rather than the diagonal\n",
        "            x = torch_sparse.spmm(L[0], L[1], x.size(0), x.size(0), x)\n",
        "\n",
        "            if self.use_act:\n",
        "                x = F.elu(x)\n",
        "\n",
        "            coeff = (1 + torch.tanh(self.epsilons[layer]).tile(dim, 1))\n",
        "            x0 = coeff * x0 - x\n",
        "            x = x0\n",
        "\n",
        "        # Readout layers\n",
        "        x = x.reshape(dim, -1)\n",
        "\n",
        "        if self.readout == \"mean\":\n",
        "            # Simple mean readout\n",
        "            x = global_mean_pool(x, batch)  # [batch_size, hidden_channels = hidden feat * d]\n",
        "\n",
        "            # Linear classifier\n",
        "            x = self.lin2(x)\n",
        "\n",
        "        if self.readout == \"sum\":\n",
        "            # Simple sum readout\n",
        "            x = global_add_pool(x, batch) \n",
        "\n",
        "            # Linear classifier\n",
        "            x = self.lin2(x)\n",
        "\n",
        "        if self.readout == \"max\":\n",
        "            # Simple max readout\n",
        "            x = global_max_pool(x, batch) \n",
        "\n",
        "            # Linear classifier\n",
        "            x = self.lin2(x)\n",
        "\n",
        "        if self.readout == \"concat\":\n",
        "            # Concatenation of [mean, sum, max] readout\n",
        "            x = torch.cat([global_mean_pool(x, batch), global_add_pool(x, batch), global_max_pool(x, batch)], dim=1)\n",
        "            x = self.lin4(x)\n",
        "            x = F.elu(x)\n",
        "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "\n",
        "            # Linear classifier\n",
        "            x = self.lin2(x)\n",
        "\n",
        "        if self.readout == \"sag\":\n",
        "            # SAG pooling layer\n",
        "            x, edge_index, _, batch, _, _ = self.pooling_layers(x, edge_index, batch = batch) \n",
        "            x = torch.cat([global_mean_pool(x, batch), global_add_pool(x, batch), global_max_pool(x, batch)], dim=1)\n",
        "            x = self.lin4(x)\n",
        "            x = F.elu(x)\n",
        "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "\n",
        "            # Linear classifier\n",
        "            x = self.lin2(x)\n",
        "\n",
        "        if self.readout == \"mlp\":\n",
        "            graph_x, _ = to_dense_batch(x, batch, fill_value=0, max_num_nodes=self.max_num_nodes_in_graph)\n",
        "            graph_x = self.dense_agg(graph_x.view(-1, graph_x.shape[1] * graph_x.shape[2]))\n",
        "\n",
        "            # MLP classifier\n",
        "            x = self.output_nn(graph_x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfXPzpsx-nS5"
      },
      "source": [
        "# Experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZvgHVJ5X832"
      },
      "source": [
        "#### Imports and common parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "RWux1NYNYSTE"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Set the seed for everything\n",
        "torch.manual_seed(43)\n",
        "torch.cuda.manual_seed(43)\n",
        "torch.cuda.manual_seed_all(43)\n",
        "np.random.seed(43)\n",
        "random.seed(43)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ylp3KIdIaaJO"
      },
      "source": [
        "Parameters for model construction, training and test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "uK6rCb8zaOdT"
      },
      "outputs": [],
      "source": [
        "class Parameters():\n",
        "    def __init__(self):\n",
        "        super(Parameters, self).__init__()\n",
        "\n",
        "        # Optimisation params\n",
        "        self.epochs=200\n",
        "        self.lr=0.01\n",
        "        self.weight_decay=0.0005\n",
        "        self.sheaf_decay=None\n",
        "        self.patience = 15        # Early stopping on validation set \n",
        "\n",
        "        # Model configuration\n",
        "        self.second_linear = False\n",
        "        self.d=3\n",
        "        self.layers=3\n",
        "        self.normalised=True\n",
        "        self.deg_normalised=False\n",
        "        self.linear=False\n",
        "        self.hidden_channels= 15\n",
        "        self.input_dropout=0.0\n",
        "        self.dropout=0.1\n",
        "        self.left_weights=True\n",
        "        self.right_weights=True\n",
        "        self.add_lp=False\n",
        "        self.add_hp=False\n",
        "        self.use_act=True\n",
        "        self.sheaf_act=\"tanh\"\n",
        "        self.edge_weights=True\n",
        "        self.second_linear =False\n",
        "        self.orth = \"householder\"   #choices=['matrix_exp', 'cayley', 'householder', 'euler'], parametrization used for the orthogonal group\n",
        "        self.edges_feat = \"none\"    #choices=['none', 'concat', 'linear', 'bilinear']\n",
        "        self.readout = \"mean\"       #choices=['mean', 'sum', 'max', 'sag', 'mlp', 'concat']\n",
        "\n",
        "        # Dense readout layer \n",
        "        self.dense_intermediate_dim = 256 \n",
        "        self.dense_output_graph_dim = 128 \n",
        "        self.output_nn_intermediate_dim = 64\n",
        "        \n",
        "        # Transformer layer \n",
        "        self.set_transformer_k = 8\n",
        "\n",
        "args = Parameters()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zRSDfJW-Cov"
      },
      "source": [
        "#### Training phase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "fpo78uUt9pCB"
      },
      "outputs": [],
      "source": [
        "def train(model, optimizer, criterion, train_loader, device):\n",
        "    model.train()\n",
        "\n",
        "    for data in train_loader:  # Iterate in batches over the training dataset.\n",
        "         data = data.to(device)\n",
        "         out = model(data.x, data.edge_index, data.edge_attr, data.batch)  # Perform a single forward pass.\n",
        "         loss = criterion(out, data.y)  # Compute the loss.\n",
        "         loss.backward()  # Derive gradients.\n",
        "         optimizer.step()  # Update parameters based on gradients.\n",
        "         optimizer.zero_grad()  # Clear gradients."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ujSGqIlh-Egm"
      },
      "source": [
        "#### Test phase "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "xzSao5X2-LrE"
      },
      "outputs": [],
      "source": [
        "def test(model, criterion, test_loader, device):\n",
        "     model.eval()\n",
        "\n",
        "     correct = 0\n",
        "     tot_loss = 0\n",
        "\n",
        "     with torch.no_grad():\n",
        "        for data in test_loader:  # Iterate in batches over the training/test dataset.\n",
        "            data = data.to(device)\n",
        "            out = model(data.x, data.edge_index, data.edge_attr, data.batch) \n",
        "            tot_loss = tot_loss + criterion(out, data.y).item()  # Compute the loss.\n",
        "            pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
        "            correct += int((pred == data.y).sum())  # Check against ground-truth labels.\n",
        "     acc = correct / len(test_loader.dataset) * 100  # Derive ratio of correct predictions.\n",
        "     final_loss = tot_loss / len(test_loader.dataset)\n",
        "     return acc, final_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPO-s3FhbM1W"
      },
      "source": [
        "## ENZYMES"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ppe8BGSbdeS"
      },
      "source": [
        "### Download of the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sFKTIE4pm65Z",
        "outputId": "fc1a123b-02d9-4696-b380-29febb27e176"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset: ENZYMES(600):\n",
            "====================\n",
            "Number of graphs: 600\n",
            "Number of node features: 3\n",
            "Number of edge features: 0\n",
            "Number of classes: 6\n",
            "====================\n",
            "Data(edge_index=[2, 168], x=[37, 3], y=[1])\n",
            "\n",
            "Contains isolated nodes: False\n",
            "Contains self-loops: False\n",
            "Is undirected: True\n"
          ]
        }
      ],
      "source": [
        "import torch_geometric.transforms as T\n",
        "from torch_geometric.utils.undirected import to_undirected\n",
        "from torch_geometric.utils import remove_self_loops\n",
        "\n",
        "dataset = TUDataset(root='data/TUDataset', name='ENZYMES', transform=T.NormalizeFeatures())\n",
        "\n",
        "print(f'Dataset: {dataset}:')\n",
        "print('====================')\n",
        "print(f'Number of graphs: {len(dataset)}')\n",
        "print(f'Number of node features: {dataset.num_node_features}')\n",
        "print(f'Number of edge features: {dataset.num_edge_features}')\n",
        "print(f'Number of classes: {dataset.num_classes}')\n",
        "\n",
        "# In order not to mess with laplacian: must be undirected and not contain self loops\n",
        "# Check that does not contain self loops\n",
        "\n",
        "max_num_nodes_in_graph = 0\n",
        "\n",
        "for data in dataset:\n",
        "\n",
        "    # Remove self-loops\n",
        "    data.edge_index, _ = remove_self_loops(data.edge_index)\n",
        "\n",
        "    # Make the graph undirected\n",
        "    data.edge_index = to_undirected(data.edge_index)\n",
        "    num_nodes = data.x.size(dim=0)\n",
        "    \n",
        "    if num_nodes > max_num_nodes_in_graph:\n",
        "      max_num_nodes_in_graph = num_nodes\n",
        "\n",
        "print('====================')\n",
        "graph1 = dataset[0]  # Get the first graph object.\n",
        "print(graph1)\n",
        "print()\n",
        "\n",
        "print(f'Contains isolated nodes: {graph1.has_isolated_nodes()}')\n",
        "print(f'Contains self-loops: {graph1.has_self_loops()}')\n",
        "print(f'Is undirected: {graph1.is_undirected()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "taf9wQ5peOsD"
      },
      "source": [
        "### Split into training, validation and test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cOMBqm0neNtj",
        "outputId": "56865cce-db10-4bf6-be69-3adad62146e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of training graphs: 440\n",
            "Number of validation graphs: 80\n",
            "Number of test graphs: 80\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(12345)\n",
        "dataset = dataset.shuffle()\n",
        "\n",
        "train_dataset = dataset[:440]\n",
        "valid_dataset = dataset[440:520]\n",
        "test_dataset = dataset[520:]\n",
        "\n",
        "print(f'Number of training graphs: {len(train_dataset)}')\n",
        "print(f'Number of validation graphs: {len(valid_dataset)}')\n",
        "print(f'Number of test graphs: {len(test_dataset)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-mhje3jfgWS"
      },
      "source": [
        "### Creation of Dataloader \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jxj06CKpfmld",
        "outputId": "f6b6eb1f-24bb-422e-8f8a-2e27f0e4f5f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 1:\n",
            "=======\n",
            "Number of graphs in the current batch: 128\n",
            "DataBatch(edge_index=[2, 15760], x=[4119, 3], y=[128], batch=[4119], ptr=[129])\n",
            "\n",
            "Step 2:\n",
            "=======\n",
            "Number of graphs in the current batch: 128\n",
            "DataBatch(edge_index=[2, 16030], x=[4167, 3], y=[128], batch=[4167], ptr=[129])\n",
            "\n",
            "Step 3:\n",
            "=======\n",
            "Number of graphs in the current batch: 128\n",
            "DataBatch(edge_index=[2, 15794], x=[4298, 3], y=[128], batch=[4298], ptr=[129])\n",
            "\n",
            "Step 4:\n",
            "=======\n",
            "Number of graphs in the current batch: 56\n",
            "DataBatch(edge_index=[2, 7002], x=[1857, 3], y=[56], batch=[1857], ptr=[57])\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from torch_geometric.loader import DataLoader\n",
        "\n",
        "batch_size = 128\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size = batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size = batch_size, shuffle=False)\n",
        "\n",
        "for step, data in enumerate(train_loader):\n",
        "    print(f'Step {step + 1}:')\n",
        "    print('=======')\n",
        "    print(f'Number of graphs in the current batch: {data.num_graphs}')\n",
        "    print(data)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDMNGaSBl0np"
      },
      "source": [
        "### Running of experiments \n",
        "\n",
        "For the ENZYMES dataset, since the graphs don't have edge features, the model extensions that handle edge features are not tested, hence we always set\n",
        "\n",
        "```\n",
        "args.edges_feat = \"none\"\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "lMDu_u-koH_E"
      },
      "outputs": [],
      "source": [
        "# Add extra arguments\n",
        "args.input_dim = dataset.num_features\n",
        "args.output_dim = dataset.num_classes\n",
        "args.input_dim_edge = dataset.num_edge_features\n",
        "args.max_num_nodes_in_graph = max_num_nodes_in_graph\n",
        "args.device = torch.device(f'cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "assert args.normalised or args.deg_normalised\n",
        "if args.sheaf_decay is None:\n",
        "    args.sheaf_decay = args.weight_decay"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcuPmrpy6STP"
      },
      "source": [
        "#### Baseline scalar model (**d=1**), [mean, sum, max] readout "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mS3bduQ5GoGR",
        "outputId": "14663687-b83b-42af-a87c-1bbf752d2028"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epochs': 200, 'lr': 0.005, 'weight_decay': 0.0005, 'sheaf_decay': 0.0005, 'patience': 15, 'second_linear': False, 'd': 1, 'layers': 4, 'normalised': True, 'deg_normalised': False, 'linear': False, 'hidden_channels': 15, 'input_dropout': 0.0, 'dropout': 0.1, 'left_weights': True, 'right_weights': True, 'add_lp': False, 'add_hp': False, 'use_act': True, 'sheaf_act': 'tanh', 'edge_weights': True, 'orth': 'householder', 'edges_feat': 'none', 'readout': 'concat', 'dense_intermediate_dim': 256, 'dense_output_graph_dim': 128, 'output_nn_intermediate_dim': 64, 'set_transformer_k': 8, 'input_dim': 3, 'output_dim': 6, 'input_dim_edge': 0, 'max_num_nodes_in_graph': 126, 'device': device(type='cuda', index=0)}\n",
            "Epoch: 001, Train Acc: 20.9091, Train Loss: 1.8667, Valid Loss: 1.9287, Test Acc: 20.0000\n",
            "Epoch: 002, Train Acc: 18.6364, Train Loss: 1.8867, Valid Loss: 1.9315, Test Acc: 17.5000\n",
            "Epoch: 003, Train Acc: 19.3182, Train Loss: 1.8330, Valid Loss: 1.8289, Test Acc: 17.5000\n",
            "Epoch: 004, Train Acc: 20.9091, Train Loss: 1.7820, Valid Loss: 1.7668, Test Acc: 22.5000\n",
            "Epoch: 005, Train Acc: 21.5909, Train Loss: 1.7682, Valid Loss: 1.7879, Test Acc: 20.0000\n",
            "Epoch: 006, Train Acc: 27.2727, Train Loss: 1.7651, Valid Loss: 1.8006, Test Acc: 27.5000\n",
            "Epoch: 007, Train Acc: 27.2727, Train Loss: 1.7595, Valid Loss: 1.7880, Test Acc: 30.0000\n",
            "Epoch: 008, Train Acc: 22.2727, Train Loss: 1.7526, Valid Loss: 1.7617, Test Acc: 23.7500\n",
            "Epoch: 009, Train Acc: 22.7273, Train Loss: 1.7505, Valid Loss: 1.7606, Test Acc: 22.5000\n",
            "Epoch: 010, Train Acc: 21.1364, Train Loss: 1.7502, Valid Loss: 1.7632, Test Acc: 20.0000\n",
            "Epoch: 011, Train Acc: 21.5909, Train Loss: 1.7500, Valid Loss: 1.7592, Test Acc: 21.2500\n",
            "Epoch: 012, Train Acc: 21.1364, Train Loss: 1.7432, Valid Loss: 1.7586, Test Acc: 18.7500\n",
            "Epoch: 013, Train Acc: 23.4091, Train Loss: 1.7407, Valid Loss: 1.7596, Test Acc: 25.0000\n",
            "Epoch: 014, Train Acc: 22.9545, Train Loss: 1.7354, Valid Loss: 1.7375, Test Acc: 20.0000\n",
            "Epoch: 015, Train Acc: 20.4545, Train Loss: 1.7333, Valid Loss: 1.7497, Test Acc: 18.7500\n",
            "Epoch: 016, Train Acc: 22.7273, Train Loss: 1.7318, Valid Loss: 1.7504, Test Acc: 18.7500\n",
            "Epoch: 017, Train Acc: 22.5000, Train Loss: 1.7267, Valid Loss: 1.7455, Test Acc: 20.0000\n",
            "Epoch: 018, Train Acc: 25.4545, Train Loss: 1.7192, Valid Loss: 1.7202, Test Acc: 21.2500\n",
            "Epoch: 019, Train Acc: 25.6818, Train Loss: 1.7156, Valid Loss: 1.7258, Test Acc: 18.7500\n",
            "Epoch: 020, Train Acc: 22.2727, Train Loss: 1.7197, Valid Loss: 1.7475, Test Acc: 13.7500\n",
            "Epoch: 021, Train Acc: 22.2727, Train Loss: 1.7251, Valid Loss: 1.7390, Test Acc: 16.2500\n",
            "Epoch: 022, Train Acc: 27.0455, Train Loss: 1.7089, Valid Loss: 1.7008, Test Acc: 15.0000\n",
            "Epoch: 023, Train Acc: 28.1818, Train Loss: 1.7002, Valid Loss: 1.7067, Test Acc: 18.7500\n",
            "Epoch: 024, Train Acc: 26.8182, Train Loss: 1.6990, Valid Loss: 1.7201, Test Acc: 17.5000\n",
            "Epoch: 025, Train Acc: 30.2273, Train Loss: 1.6949, Valid Loss: 1.7152, Test Acc: 16.2500\n",
            "Epoch: 026, Train Acc: 25.9091, Train Loss: 1.6983, Valid Loss: 1.6909, Test Acc: 17.5000\n",
            "Epoch: 027, Train Acc: 29.0909, Train Loss: 1.6845, Valid Loss: 1.6821, Test Acc: 18.7500\n",
            "Epoch: 028, Train Acc: 30.2273, Train Loss: 1.6807, Valid Loss: 1.7061, Test Acc: 18.7500\n",
            "Epoch: 029, Train Acc: 29.3182, Train Loss: 1.6874, Valid Loss: 1.6861, Test Acc: 22.5000\n",
            "Epoch: 030, Train Acc: 31.5909, Train Loss: 1.6760, Valid Loss: 1.7017, Test Acc: 16.2500\n",
            "Epoch: 031, Train Acc: 30.0000, Train Loss: 1.6722, Valid Loss: 1.6644, Test Acc: 16.2500\n",
            "Epoch: 032, Train Acc: 30.6818, Train Loss: 1.6704, Valid Loss: 1.7129, Test Acc: 18.7500\n",
            "Epoch: 033, Train Acc: 29.0909, Train Loss: 1.6597, Valid Loss: 1.7047, Test Acc: 17.5000\n",
            "Epoch: 034, Train Acc: 28.8636, Train Loss: 1.6591, Valid Loss: 1.6850, Test Acc: 16.2500\n",
            "Epoch: 035, Train Acc: 29.7727, Train Loss: 1.6486, Valid Loss: 1.6921, Test Acc: 17.5000\n",
            "Epoch: 036, Train Acc: 32.2727, Train Loss: 1.6497, Valid Loss: 1.6586, Test Acc: 20.0000\n",
            "Epoch: 037, Train Acc: 36.5909, Train Loss: 1.6338, Valid Loss: 1.6902, Test Acc: 23.7500\n",
            "Epoch: 038, Train Acc: 34.3182, Train Loss: 1.6354, Valid Loss: 1.6896, Test Acc: 22.5000\n",
            "Epoch: 039, Train Acc: 37.2727, Train Loss: 1.6136, Valid Loss: 1.6691, Test Acc: 27.5000\n",
            "Epoch: 040, Train Acc: 35.0000, Train Loss: 1.6066, Valid Loss: 1.6389, Test Acc: 31.2500\n",
            "Epoch: 041, Train Acc: 37.5000, Train Loss: 1.6076, Valid Loss: 1.6416, Test Acc: 25.0000\n",
            "Epoch: 042, Train Acc: 34.3182, Train Loss: 1.6053, Valid Loss: 1.6281, Test Acc: 31.2500\n",
            "Epoch: 043, Train Acc: 31.8182, Train Loss: 1.6029, Valid Loss: 1.6165, Test Acc: 26.2500\n",
            "Epoch: 044, Train Acc: 37.5000, Train Loss: 1.5789, Valid Loss: 1.6261, Test Acc: 27.5000\n",
            "Epoch: 045, Train Acc: 35.6818, Train Loss: 1.5918, Valid Loss: 1.6243, Test Acc: 25.0000\n",
            "Epoch: 046, Train Acc: 34.0909, Train Loss: 1.5831, Valid Loss: 1.6077, Test Acc: 28.7500\n",
            "Epoch: 047, Train Acc: 33.1818, Train Loss: 1.5765, Valid Loss: 1.6032, Test Acc: 26.2500\n",
            "Epoch: 048, Train Acc: 39.5455, Train Loss: 1.5571, Valid Loss: 1.6276, Test Acc: 26.2500\n",
            "Epoch: 049, Train Acc: 36.3636, Train Loss: 1.5587, Valid Loss: 1.6283, Test Acc: 27.5000\n",
            "Epoch: 050, Train Acc: 34.0909, Train Loss: 1.5391, Valid Loss: 1.5846, Test Acc: 23.7500\n",
            "Epoch: 051, Train Acc: 34.0909, Train Loss: 1.5359, Valid Loss: 1.5794, Test Acc: 23.7500\n",
            "Epoch: 052, Train Acc: 36.3636, Train Loss: 1.5371, Valid Loss: 1.5997, Test Acc: 22.5000\n",
            "Epoch: 053, Train Acc: 37.0455, Train Loss: 1.5258, Valid Loss: 1.5792, Test Acc: 21.2500\n",
            "Epoch: 054, Train Acc: 35.6818, Train Loss: 1.5086, Valid Loss: 1.5527, Test Acc: 23.7500\n",
            "Epoch: 055, Train Acc: 38.8636, Train Loss: 1.5059, Valid Loss: 1.5814, Test Acc: 28.7500\n",
            "Epoch: 056, Train Acc: 41.5909, Train Loss: 1.5022, Valid Loss: 1.6018, Test Acc: 30.0000\n",
            "Epoch: 057, Train Acc: 40.9091, Train Loss: 1.5074, Valid Loss: 1.6059, Test Acc: 30.0000\n",
            "Epoch: 058, Train Acc: 33.6364, Train Loss: 1.5030, Valid Loss: 1.5533, Test Acc: 28.7500\n",
            "Epoch: 059, Train Acc: 37.9545, Train Loss: 1.4745, Valid Loss: 1.5730, Test Acc: 25.0000\n",
            "Epoch: 060, Train Acc: 37.5000, Train Loss: 1.5027, Valid Loss: 1.6284, Test Acc: 27.5000\n",
            "Epoch: 061, Train Acc: 38.4091, Train Loss: 1.4840, Valid Loss: 1.6100, Test Acc: 31.2500\n",
            "Epoch: 062, Train Acc: 37.2727, Train Loss: 1.4922, Valid Loss: 1.5546, Test Acc: 35.0000\n",
            "Epoch: 063, Train Acc: 41.3636, Train Loss: 1.4754, Valid Loss: 1.5918, Test Acc: 32.5000\n",
            "Epoch: 064, Train Acc: 42.5000, Train Loss: 1.4702, Valid Loss: 1.5804, Test Acc: 31.2500\n",
            "Epoch: 065, Train Acc: 41.3636, Train Loss: 1.4594, Valid Loss: 1.5642, Test Acc: 33.7500\n",
            "Epoch: 066, Train Acc: 40.9091, Train Loss: 1.4712, Valid Loss: 1.5639, Test Acc: 40.0000\n",
            "Epoch: 067, Train Acc: 44.3182, Train Loss: 1.4577, Valid Loss: 1.6057, Test Acc: 32.5000\n",
            "Epoch: 068, Train Acc: 40.9091, Train Loss: 1.4668, Valid Loss: 1.5925, Test Acc: 32.5000\n",
            "\n",
            "\n",
            "Best test accuracy:  40.0\n",
            "Best epoch:  66\n"
          ]
        }
      ],
      "source": [
        "#Model-specific arguments\n",
        "\n",
        "# Optimisation params\n",
        "args.lr=0.005\n",
        "args.weight_decay=0.0005\n",
        "\n",
        "# Model configuration\n",
        "args.d=1\n",
        "args.layers=4\n",
        "args.hidden_channels= 15\n",
        "args.input_dropout=0.0\n",
        "args.dropout=0.1\n",
        "args.orth = \"householder\"   #choices=['matrix_exp', 'cayley', 'householder', 'euler'], parametrization for the orthogonal group\n",
        "args.edges_feat = \"none\"    #choices=['none', 'concat', 'linear', 'bilinear']\n",
        "args.readout = \"concat\"     #choices=['mean', 'sum', 'max', 'sag', 'mlp', 'concat']\n",
        "\n",
        "\n",
        "config=args.__dict__\n",
        "print(config)\n",
        "\n",
        "model = DiscreteDiagSheafDiffusion(config)\n",
        "#model = DiscreteDiagPoolSheafDiffusion(config)\n",
        "#model = DiscreteBundleSheafDiffusion(config)\n",
        "model = model.to(args.device)\n",
        "\n",
        "sheaf_learner_params, other_params = model.grouped_parameters()\n",
        "\n",
        "# Optimizers \n",
        "optimizer = torch.optim.Adam([\n",
        "    {'params': sheaf_learner_params, 'weight_decay': config['sheaf_decay']},\n",
        "    {'params': other_params, 'weight_decay': config['weight_decay']}\n",
        "], lr=config['lr'])\n",
        "\n",
        "\n",
        "# Training and test criterions\n",
        "criterion_train = torch.nn.CrossEntropyLoss()\n",
        "# In order not to make test loss change with batch_size \n",
        "criterion_test = torch.nn.CrossEntropyLoss(reduction = 'sum')\n",
        "\n",
        "best_test_acc = 0\n",
        "best_epoch = 0\n",
        "# Initial best loss value, used for the early stopping technique\n",
        "best_loss = 100.00\n",
        "patience = args.patience\n",
        "\n",
        "for epoch in range(1, args.epochs):\n",
        "    # Training epoch\n",
        "    train(model, optimizer, criterion_train, train_loader, args.device)\n",
        "\n",
        "\n",
        "    # Compute training, validation and test accuracy\n",
        "    train_acc, train_loss = test(model, criterion_test, train_loader, args.device)\n",
        "    valid_acc, valid_loss = test(model, criterion_test, valid_loader, args.device)\n",
        "    test_acc, test_loss = test(model, criterion_test, test_loader, args.device)\n",
        "\n",
        "    if best_test_acc < test_acc:\n",
        "        best_test_acc = test_acc\n",
        "        best_epoch = epoch\n",
        "\n",
        "    if valid_loss < best_loss:\n",
        "        best_loss = valid_loss\n",
        "        patience = args.patience\n",
        "    else:\n",
        "        patience -= 1\n",
        "        \n",
        "    if patience <= 0: \n",
        "        # Early stopping with patience\n",
        "        break \n",
        "\n",
        "    print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Train Loss: {train_loss:.4f}, Valid Loss: {valid_loss:.4f}, Test Acc: {test_acc:.4f}')\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"Best test accuracy: \", best_test_acc)\n",
        "print(\"Best epoch: \", best_epoch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bfH6SRQinBA"
      },
      "source": [
        "#### **Diagonal** sheaf diffusion,  **[mean, sum, max]** readout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8e-T-UifZG26",
        "outputId": "bdf68595-5c82-4916-cc80-3789d413ce86"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epochs': 200, 'lr': 0.005, 'weight_decay': 0.0005, 'sheaf_decay': 0.0005, 'patience': 15, 'second_linear': False, 'd': 3, 'layers': 4, 'normalised': True, 'deg_normalised': False, 'linear': False, 'hidden_channels': 15, 'input_dropout': 0.0, 'dropout': 0.1, 'left_weights': True, 'right_weights': True, 'add_lp': False, 'add_hp': False, 'use_act': True, 'sheaf_act': 'tanh', 'edge_weights': True, 'orth': 'householder', 'edges_feat': 'none', 'readout': 'concat', 'dense_intermediate_dim': 256, 'dense_output_graph_dim': 128, 'output_nn_intermediate_dim': 64, 'set_transformer_k': 8, 'input_dim': 3, 'output_dim': 6, 'input_dim_edge': 0, 'max_num_nodes_in_graph': 126, 'device': device(type='cuda', index=0)}\n",
            "Epoch: 001, Train Acc: 14.7727, Train Loss: 1.9131, Valid Loss: 1.8577, Test Acc: 12.5000\n",
            "Epoch: 002, Train Acc: 18.1818, Train Loss: 1.8258, Valid Loss: 1.9215, Test Acc: 21.2500\n",
            "Epoch: 003, Train Acc: 20.2273, Train Loss: 1.7886, Valid Loss: 1.8388, Test Acc: 21.2500\n",
            "Epoch: 004, Train Acc: 22.9545, Train Loss: 1.7758, Valid Loss: 1.8245, Test Acc: 25.0000\n",
            "Epoch: 005, Train Acc: 19.7727, Train Loss: 1.7607, Valid Loss: 1.7896, Test Acc: 25.0000\n",
            "Epoch: 006, Train Acc: 19.7727, Train Loss: 1.7570, Valid Loss: 1.7779, Test Acc: 23.7500\n",
            "Epoch: 007, Train Acc: 22.7273, Train Loss: 1.7536, Valid Loss: 1.7710, Test Acc: 23.7500\n",
            "Epoch: 008, Train Acc: 26.1364, Train Loss: 1.7401, Valid Loss: 1.7694, Test Acc: 30.0000\n",
            "Epoch: 009, Train Acc: 23.6364, Train Loss: 1.7317, Valid Loss: 1.7522, Test Acc: 27.5000\n",
            "Epoch: 010, Train Acc: 26.8182, Train Loss: 1.7318, Valid Loss: 1.7435, Test Acc: 25.0000\n",
            "Epoch: 011, Train Acc: 26.5909, Train Loss: 1.7171, Valid Loss: 1.7319, Test Acc: 33.7500\n",
            "Epoch: 012, Train Acc: 27.5000, Train Loss: 1.7185, Valid Loss: 1.7363, Test Acc: 27.5000\n",
            "Epoch: 013, Train Acc: 25.6818, Train Loss: 1.7166, Valid Loss: 1.7408, Test Acc: 21.2500\n",
            "Epoch: 014, Train Acc: 25.2273, Train Loss: 1.6945, Valid Loss: 1.7279, Test Acc: 28.7500\n",
            "Epoch: 015, Train Acc: 30.4545, Train Loss: 1.7093, Valid Loss: 1.7622, Test Acc: 28.7500\n",
            "Epoch: 016, Train Acc: 31.1364, Train Loss: 1.6809, Valid Loss: 1.7271, Test Acc: 30.0000\n",
            "Epoch: 017, Train Acc: 27.7273, Train Loss: 1.6959, Valid Loss: 1.7511, Test Acc: 31.2500\n",
            "Epoch: 018, Train Acc: 29.0909, Train Loss: 1.6715, Valid Loss: 1.7299, Test Acc: 28.7500\n",
            "Epoch: 019, Train Acc: 31.3636, Train Loss: 1.6518, Valid Loss: 1.6960, Test Acc: 25.0000\n",
            "Epoch: 020, Train Acc: 27.0455, Train Loss: 1.7220, Valid Loss: 1.8050, Test Acc: 25.0000\n",
            "Epoch: 021, Train Acc: 30.6818, Train Loss: 1.6560, Valid Loss: 1.7695, Test Acc: 28.7500\n",
            "Epoch: 022, Train Acc: 28.1818, Train Loss: 1.6922, Valid Loss: 1.8018, Test Acc: 26.2500\n",
            "Epoch: 023, Train Acc: 30.2273, Train Loss: 1.6794, Valid Loss: 1.7849, Test Acc: 27.5000\n",
            "Epoch: 024, Train Acc: 29.3182, Train Loss: 1.6634, Valid Loss: 1.7385, Test Acc: 26.2500\n",
            "Epoch: 025, Train Acc: 27.2727, Train Loss: 1.6674, Valid Loss: 1.7525, Test Acc: 27.5000\n",
            "Epoch: 026, Train Acc: 29.5455, Train Loss: 1.6912, Valid Loss: 1.7570, Test Acc: 26.2500\n",
            "Epoch: 027, Train Acc: 30.6818, Train Loss: 1.6267, Valid Loss: 1.6992, Test Acc: 26.2500\n",
            "Epoch: 028, Train Acc: 27.9545, Train Loss: 1.6711, Valid Loss: 1.7844, Test Acc: 25.0000\n",
            "Epoch: 029, Train Acc: 27.9545, Train Loss: 1.6575, Valid Loss: 1.7754, Test Acc: 26.2500\n",
            "Epoch: 030, Train Acc: 33.4091, Train Loss: 1.5846, Valid Loss: 1.6828, Test Acc: 30.0000\n",
            "Epoch: 031, Train Acc: 28.1818, Train Loss: 1.6687, Valid Loss: 1.7986, Test Acc: 25.0000\n",
            "Epoch: 032, Train Acc: 39.7727, Train Loss: 1.5386, Valid Loss: 1.6550, Test Acc: 42.5000\n",
            "Epoch: 033, Train Acc: 31.5909, Train Loss: 1.6308, Valid Loss: 1.7351, Test Acc: 28.7500\n",
            "Epoch: 034, Train Acc: 35.4545, Train Loss: 1.5602, Valid Loss: 1.6586, Test Acc: 30.0000\n",
            "Epoch: 035, Train Acc: 29.5455, Train Loss: 1.5967, Valid Loss: 1.7095, Test Acc: 26.2500\n",
            "Epoch: 036, Train Acc: 41.3636, Train Loss: 1.5089, Valid Loss: 1.6189, Test Acc: 37.5000\n",
            "Epoch: 037, Train Acc: 31.8182, Train Loss: 1.5795, Valid Loss: 1.7257, Test Acc: 30.0000\n",
            "Epoch: 038, Train Acc: 35.6818, Train Loss: 1.5634, Valid Loss: 1.7043, Test Acc: 37.5000\n",
            "Epoch: 039, Train Acc: 33.6364, Train Loss: 1.5501, Valid Loss: 1.7013, Test Acc: 36.2500\n",
            "Epoch: 040, Train Acc: 41.3636, Train Loss: 1.4996, Valid Loss: 1.6734, Test Acc: 32.5000\n",
            "Epoch: 041, Train Acc: 38.1818, Train Loss: 1.5028, Valid Loss: 1.6642, Test Acc: 31.2500\n",
            "Epoch: 042, Train Acc: 39.7727, Train Loss: 1.4570, Valid Loss: 1.6396, Test Acc: 33.7500\n",
            "Epoch: 043, Train Acc: 33.1818, Train Loss: 1.5527, Valid Loss: 1.6760, Test Acc: 30.0000\n",
            "Epoch: 044, Train Acc: 42.2727, Train Loss: 1.4768, Valid Loss: 1.6327, Test Acc: 38.7500\n",
            "Epoch: 045, Train Acc: 37.0455, Train Loss: 1.5171, Valid Loss: 1.7267, Test Acc: 31.2500\n",
            "Epoch: 046, Train Acc: 41.5909, Train Loss: 1.4538, Valid Loss: 1.7074, Test Acc: 33.7500\n",
            "Epoch: 047, Train Acc: 33.4091, Train Loss: 1.5637, Valid Loss: 1.8315, Test Acc: 26.2500\n",
            "Epoch: 048, Train Acc: 42.2727, Train Loss: 1.4138, Valid Loss: 1.6624, Test Acc: 38.7500\n",
            "Epoch: 049, Train Acc: 38.8636, Train Loss: 1.4651, Valid Loss: 1.7181, Test Acc: 27.5000\n",
            "Epoch: 050, Train Acc: 44.7727, Train Loss: 1.4157, Valid Loss: 1.6332, Test Acc: 40.0000\n",
            "\n",
            "\n",
            "Best test accuracy:  42.5\n",
            "Best epoch:  32\n"
          ]
        }
      ],
      "source": [
        "#Model-specific arguments\n",
        "\n",
        "# Optimisation params\n",
        "args.lr=0.005\n",
        "args.weight_decay=0.0005\n",
        "\n",
        "# Model configuration\n",
        "args.d=3\n",
        "args.layers=4\n",
        "args.hidden_channels= 15\n",
        "args.input_dropout=0.0\n",
        "args.dropout=0.1\n",
        "args.orth = \"householder\"   #choices=['matrix_exp', 'cayley', 'householder', 'euler'], parametrization for the orthogonal group \n",
        "args.edges_feat = \"none\"    #choices=['none', 'concat', 'linear', 'bilinear']\n",
        "args.readout = \"concat\"     #choices=['mean', 'sum', 'max', 'sag', 'mlp', 'concat']\n",
        "\n",
        "\n",
        "config=args.__dict__\n",
        "print(config)\n",
        "\n",
        "model = DiscreteDiagSheafDiffusion(config)\n",
        "#model = DiscreteDiagPoolSheafDiffusion(config)\n",
        "#model = DiscreteBundleSheafDiffusion(config)\n",
        "model = model.to(args.device)\n",
        "\n",
        "sheaf_learner_params, other_params = model.grouped_parameters()\n",
        "\n",
        "# Optimizers \n",
        "optimizer = torch.optim.Adam([\n",
        "    {'params': sheaf_learner_params, 'weight_decay': config['sheaf_decay']},\n",
        "    {'params': other_params, 'weight_decay': config['weight_decay']}\n",
        "], lr=config['lr'])\n",
        "\n",
        "\n",
        "# Training and test criterions\n",
        "criterion_train = torch.nn.CrossEntropyLoss()\n",
        "# In order not to make test loss change with batch_size \n",
        "criterion_test = torch.nn.CrossEntropyLoss(reduction = 'sum')\n",
        "\n",
        "best_test_acc = 0\n",
        "best_epoch = 0\n",
        "# Initial best loss value, used for the early stopping technique\n",
        "best_loss = 100.00\n",
        "patience = args.patience\n",
        "\n",
        "for epoch in range(1, args.epochs):\n",
        "    # Training epoch\n",
        "    train(model, optimizer, criterion_train, train_loader, args.device)\n",
        "\n",
        "\n",
        "    # Compute training, validation and test accuracy\n",
        "    train_acc, train_loss = test(model, criterion_test, train_loader, args.device)\n",
        "    valid_acc, valid_loss = test(model, criterion_test, valid_loader, args.device)\n",
        "    test_acc, test_loss = test(model, criterion_test, test_loader, args.device)\n",
        "\n",
        "    if best_test_acc < test_acc:\n",
        "        best_test_acc = test_acc\n",
        "        best_epoch = epoch\n",
        "\n",
        "    if valid_loss < best_loss:\n",
        "        best_loss = valid_loss\n",
        "        patience = args.patience\n",
        "    else:\n",
        "        patience -= 1\n",
        "        \n",
        "    if patience <= 0: \n",
        "        # Early stopping with patience\n",
        "        break \n",
        "\n",
        "    print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Train Loss: {train_loss:.4f}, Valid Loss: {valid_loss:.4f}, Test Acc: {test_acc:.4f}')\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"Best test accuracy: \", best_test_acc)\n",
        "print(\"Best epoch: \", best_epoch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QzORbkWuaPig"
      },
      "source": [
        "#### **Bundle** sheaf diffusion,  **[mean, sum, max]** readout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NLo9LjpTaPih",
        "outputId": "e43d4743-2a56-4ad0-c02a-644a416c62b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epochs': 200, 'lr': 0.01, 'weight_decay': 0.0005, 'sheaf_decay': 0.0005, 'patience': 15, 'second_linear': False, 'd': 2, 'layers': 4, 'normalised': True, 'deg_normalised': False, 'linear': False, 'hidden_channels': 15, 'input_dropout': 0.0, 'dropout': 0.1, 'left_weights': True, 'right_weights': True, 'add_lp': False, 'add_hp': False, 'use_act': True, 'sheaf_act': 'tanh', 'edge_weights': True, 'orth': 'householder', 'edges_feat': 'none', 'readout': 'concat', 'dense_intermediate_dim': 256, 'dense_output_graph_dim': 128, 'output_nn_intermediate_dim': 64, 'set_transformer_k': 8, 'input_dim': 3, 'output_dim': 6, 'input_dim_edge': 0, 'max_num_nodes_in_graph': 126, 'device': device(type='cuda', index=0)}\n",
            "Epoch: 001, Train Acc: 15.4545, Train Loss: 2.0803, Valid Loss: 1.9413, Test Acc: 20.0000\n",
            "Epoch: 002, Train Acc: 20.0000, Train Loss: 1.8537, Valid Loss: 1.9046, Test Acc: 23.7500\n",
            "Epoch: 003, Train Acc: 20.4545, Train Loss: 1.7904, Valid Loss: 1.8588, Test Acc: 23.7500\n",
            "Epoch: 004, Train Acc: 20.9091, Train Loss: 1.7482, Valid Loss: 1.7641, Test Acc: 27.5000\n",
            "Epoch: 005, Train Acc: 25.6818, Train Loss: 1.7331, Valid Loss: 1.7265, Test Acc: 28.7500\n",
            "Epoch: 006, Train Acc: 29.3182, Train Loss: 1.7289, Valid Loss: 1.7223, Test Acc: 30.0000\n",
            "Epoch: 007, Train Acc: 30.0000, Train Loss: 1.7231, Valid Loss: 1.7453, Test Acc: 33.7500\n",
            "Epoch: 008, Train Acc: 28.1818, Train Loss: 1.7184, Valid Loss: 1.7410, Test Acc: 32.5000\n",
            "Epoch: 009, Train Acc: 29.3182, Train Loss: 1.7123, Valid Loss: 1.7292, Test Acc: 27.5000\n",
            "Epoch: 010, Train Acc: 28.6364, Train Loss: 1.7109, Valid Loss: 1.7042, Test Acc: 30.0000\n",
            "Epoch: 011, Train Acc: 26.8182, Train Loss: 1.7064, Valid Loss: 1.6901, Test Acc: 31.2500\n",
            "Epoch: 012, Train Acc: 30.4545, Train Loss: 1.7068, Valid Loss: 1.6837, Test Acc: 30.0000\n",
            "Epoch: 013, Train Acc: 27.2727, Train Loss: 1.7098, Valid Loss: 1.7068, Test Acc: 31.2500\n",
            "Epoch: 014, Train Acc: 25.0000, Train Loss: 1.7040, Valid Loss: 1.7216, Test Acc: 28.7500\n",
            "Epoch: 015, Train Acc: 29.0909, Train Loss: 1.6977, Valid Loss: 1.7008, Test Acc: 33.7500\n",
            "Epoch: 016, Train Acc: 31.5909, Train Loss: 1.6893, Valid Loss: 1.6769, Test Acc: 35.0000\n",
            "Epoch: 017, Train Acc: 26.3636, Train Loss: 1.6919, Valid Loss: 1.6766, Test Acc: 28.7500\n",
            "Epoch: 018, Train Acc: 29.3182, Train Loss: 1.6921, Valid Loss: 1.6961, Test Acc: 27.5000\n",
            "Epoch: 019, Train Acc: 30.2273, Train Loss: 1.6879, Valid Loss: 1.6990, Test Acc: 32.5000\n",
            "Epoch: 020, Train Acc: 31.8182, Train Loss: 1.6875, Valid Loss: 1.6906, Test Acc: 33.7500\n",
            "Epoch: 021, Train Acc: 27.5000, Train Loss: 1.6961, Valid Loss: 1.7106, Test Acc: 28.7500\n",
            "Epoch: 022, Train Acc: 29.0909, Train Loss: 1.6859, Valid Loss: 1.6795, Test Acc: 30.0000\n",
            "Epoch: 023, Train Acc: 32.0455, Train Loss: 1.6791, Valid Loss: 1.6685, Test Acc: 40.0000\n",
            "Epoch: 024, Train Acc: 31.8182, Train Loss: 1.6750, Valid Loss: 1.6806, Test Acc: 38.7500\n",
            "Epoch: 025, Train Acc: 30.2273, Train Loss: 1.6751, Valid Loss: 1.7171, Test Acc: 33.7500\n",
            "Epoch: 026, Train Acc: 30.9091, Train Loss: 1.6634, Valid Loss: 1.6893, Test Acc: 35.0000\n",
            "Epoch: 027, Train Acc: 32.5000, Train Loss: 1.6589, Valid Loss: 1.6490, Test Acc: 32.5000\n",
            "Epoch: 028, Train Acc: 31.3636, Train Loss: 1.6552, Valid Loss: 1.6597, Test Acc: 30.0000\n",
            "Epoch: 029, Train Acc: 32.5000, Train Loss: 1.6513, Valid Loss: 1.6750, Test Acc: 31.2500\n",
            "Epoch: 030, Train Acc: 31.5909, Train Loss: 1.6509, Valid Loss: 1.6945, Test Acc: 31.2500\n",
            "Epoch: 031, Train Acc: 34.0909, Train Loss: 1.6437, Valid Loss: 1.6539, Test Acc: 28.7500\n",
            "Epoch: 032, Train Acc: 32.7273, Train Loss: 1.6329, Valid Loss: 1.6656, Test Acc: 27.5000\n",
            "Epoch: 033, Train Acc: 32.0455, Train Loss: 1.6393, Valid Loss: 1.7035, Test Acc: 30.0000\n",
            "Epoch: 034, Train Acc: 32.0455, Train Loss: 1.6427, Valid Loss: 1.6954, Test Acc: 30.0000\n",
            "Epoch: 035, Train Acc: 34.0909, Train Loss: 1.6322, Valid Loss: 1.6693, Test Acc: 36.2500\n",
            "Epoch: 036, Train Acc: 33.8636, Train Loss: 1.6281, Valid Loss: 1.6976, Test Acc: 30.0000\n",
            "Epoch: 037, Train Acc: 31.5909, Train Loss: 1.6372, Valid Loss: 1.6996, Test Acc: 28.7500\n",
            "Epoch: 038, Train Acc: 35.4545, Train Loss: 1.6224, Valid Loss: 1.6558, Test Acc: 35.0000\n",
            "Epoch: 039, Train Acc: 32.0455, Train Loss: 1.6201, Valid Loss: 1.6757, Test Acc: 31.2500\n",
            "Epoch: 040, Train Acc: 33.1818, Train Loss: 1.6225, Valid Loss: 1.6996, Test Acc: 28.7500\n",
            "Epoch: 041, Train Acc: 35.6818, Train Loss: 1.6019, Valid Loss: 1.7129, Test Acc: 36.2500\n",
            "\n",
            "\n",
            "Best test accuracy:  40.0\n",
            "Best epoch:  23\n"
          ]
        }
      ],
      "source": [
        "#Model-specific arguments\n",
        "\n",
        "# Optimisation params\n",
        "args.lr=0.01\n",
        "args.weight_decay=0.0005\n",
        "\n",
        "# Model configuration\n",
        "args.d=3\n",
        "args.layers=4\n",
        "#args.hidden_channels= 15\n",
        "#args.input_dropout=0.0\n",
        "#args.dropout=0.1\n",
        "#args.orth = \"householder\"   #choices=['matrix_exp', 'cayley', 'householder', 'euler'], parametrization for the orthogonal group\n",
        "args.edges_feat = \"none\"    #choices=['none', 'concat', 'linear', 'bilinear']\n",
        "args.readout = \"concat\"      #choices=['mean', 'sum', 'max', 'sag', 'mlp', 'concat']\n",
        "\n",
        "\n",
        "config=args.__dict__\n",
        "print(config)\n",
        "\n",
        "#model = DiscreteDiagSheafDiffusion(config)\n",
        "#model = DiscreteDiagPoolSheafDiffusion(config)\n",
        "model = DiscreteBundleSheafDiffusion(config)\n",
        "model = model.to(args.device)\n",
        "\n",
        "sheaf_learner_params, other_params = model.grouped_parameters()\n",
        "\n",
        "# Optimizers \n",
        "optimizer = torch.optim.Adam([\n",
        "    {'params': sheaf_learner_params, 'weight_decay': config['sheaf_decay']},\n",
        "    {'params': other_params, 'weight_decay': config['weight_decay']}\n",
        "], lr=config['lr'])\n",
        "\n",
        "\n",
        "# Training and test criterions\n",
        "criterion_train = torch.nn.CrossEntropyLoss()\n",
        "# In order not to make test loss change with batch_size \n",
        "criterion_test = torch.nn.CrossEntropyLoss(reduction = 'sum')\n",
        "\n",
        "best_test_acc = 0\n",
        "best_epoch = 0\n",
        "# Initial best loss value, used for the early stopping technique\n",
        "best_loss = 100.00\n",
        "patience = args.patience\n",
        "\n",
        "for epoch in range(1, args.epochs):\n",
        "    # Training epoch\n",
        "    train(model, optimizer, criterion_train, train_loader, args.device)\n",
        "\n",
        "\n",
        "    # Compute training, validation and test accuracy\n",
        "    train_acc, train_loss = test(model, criterion_test, train_loader, args.device)\n",
        "    valid_acc, valid_loss = test(model, criterion_test, valid_loader, args.device)\n",
        "    test_acc, test_loss = test(model, criterion_test, test_loader, args.device)\n",
        "\n",
        "    if best_test_acc < test_acc:\n",
        "        best_test_acc = test_acc\n",
        "        best_epoch = epoch\n",
        "\n",
        "    if valid_loss < best_loss:\n",
        "        best_loss = valid_loss\n",
        "        patience = args.patience\n",
        "    else:\n",
        "        patience -= 1\n",
        "        \n",
        "    if patience <= 0: \n",
        "        # Early stopping with patience\n",
        "        break \n",
        "\n",
        "    print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Train Loss: {train_loss:.4f}, Valid Loss: {valid_loss:.4f}, Test Acc: {test_acc:.4f}')\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"Best test accuracy: \", best_test_acc)\n",
        "print(\"Best epoch: \", best_epoch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oX6mNpIeoNUK"
      },
      "source": [
        "#### Diagonal sheaf diffusion, **mean** readout "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9l7hk_mjtQsy",
        "outputId": "a311841a-1e2d-4c5f-d886-6b4ba3bb9606"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epochs': 200, 'lr': 0.01, 'weight_decay': 0.0005, 'sheaf_decay': 0.0005, 'patience': 15, 'second_linear': False, 'd': 3, 'layers': 4, 'normalised': True, 'deg_normalised': False, 'linear': False, 'hidden_channels': 15, 'input_dropout': 0.0, 'dropout': 0.1, 'left_weights': True, 'right_weights': True, 'add_lp': False, 'add_hp': False, 'use_act': True, 'sheaf_act': 'tanh', 'edge_weights': True, 'orth': 'householder', 'edges_feat': 'none', 'readout': 'mean', 'dense_intermediate_dim': 256, 'dense_output_graph_dim': 128, 'output_nn_intermediate_dim': 64, 'set_transformer_k': 8, 'input_dim': 3, 'output_dim': 6, 'input_dim_edge': 0, 'max_num_nodes_in_graph': 126, 'device': device(type='cuda', index=0)}\n",
            "Epoch: 001, Train Acc: 17.7273, Train Loss: 1.7887, Valid Loss: 1.8142, Test Acc: 10.0000\n",
            "Epoch: 002, Train Acc: 19.3182, Train Loss: 1.7792, Valid Loss: 1.8028, Test Acc: 15.0000\n",
            "Epoch: 003, Train Acc: 24.0909, Train Loss: 1.7713, Valid Loss: 1.7811, Test Acc: 22.5000\n",
            "Epoch: 004, Train Acc: 20.2273, Train Loss: 1.7631, Valid Loss: 1.7760, Test Acc: 26.2500\n",
            "Epoch: 005, Train Acc: 27.9545, Train Loss: 1.7529, Valid Loss: 1.7748, Test Acc: 30.0000\n",
            "Epoch: 006, Train Acc: 26.5909, Train Loss: 1.7437, Valid Loss: 1.7794, Test Acc: 23.7500\n",
            "Epoch: 007, Train Acc: 22.9545, Train Loss: 1.7359, Valid Loss: 1.7719, Test Acc: 23.7500\n",
            "Epoch: 008, Train Acc: 24.0909, Train Loss: 1.7341, Valid Loss: 1.7707, Test Acc: 27.5000\n",
            "Epoch: 009, Train Acc: 21.8182, Train Loss: 1.7306, Valid Loss: 1.7676, Test Acc: 22.5000\n",
            "Epoch: 010, Train Acc: 26.1364, Train Loss: 1.7221, Valid Loss: 1.7589, Test Acc: 27.5000\n",
            "Epoch: 011, Train Acc: 26.3636, Train Loss: 1.7187, Valid Loss: 1.7574, Test Acc: 26.2500\n",
            "Epoch: 012, Train Acc: 27.5000, Train Loss: 1.7162, Valid Loss: 1.7664, Test Acc: 26.2500\n",
            "Epoch: 013, Train Acc: 25.0000, Train Loss: 1.7087, Valid Loss: 1.7481, Test Acc: 22.5000\n",
            "Epoch: 014, Train Acc: 26.1364, Train Loss: 1.7081, Valid Loss: 1.7489, Test Acc: 26.2500\n",
            "Epoch: 015, Train Acc: 28.6364, Train Loss: 1.7056, Valid Loss: 1.7663, Test Acc: 27.5000\n",
            "Epoch: 016, Train Acc: 28.6364, Train Loss: 1.7012, Valid Loss: 1.7605, Test Acc: 28.7500\n",
            "Epoch: 017, Train Acc: 27.2727, Train Loss: 1.6979, Valid Loss: 1.7634, Test Acc: 26.2500\n",
            "Epoch: 018, Train Acc: 30.0000, Train Loss: 1.6896, Valid Loss: 1.7553, Test Acc: 27.5000\n",
            "Epoch: 019, Train Acc: 29.7727, Train Loss: 1.6820, Valid Loss: 1.7626, Test Acc: 28.7500\n",
            "Epoch: 020, Train Acc: 29.5455, Train Loss: 1.6815, Valid Loss: 1.7599, Test Acc: 27.5000\n",
            "Epoch: 021, Train Acc: 29.5455, Train Loss: 1.6895, Valid Loss: 1.7412, Test Acc: 28.7500\n",
            "Epoch: 022, Train Acc: 29.5455, Train Loss: 1.6759, Valid Loss: 1.7355, Test Acc: 30.0000\n",
            "Epoch: 023, Train Acc: 30.0000, Train Loss: 1.6788, Valid Loss: 1.7631, Test Acc: 30.0000\n",
            "Epoch: 024, Train Acc: 30.2273, Train Loss: 1.6695, Valid Loss: 1.7322, Test Acc: 32.5000\n",
            "Epoch: 025, Train Acc: 29.5455, Train Loss: 1.6777, Valid Loss: 1.7564, Test Acc: 28.7500\n",
            "Epoch: 026, Train Acc: 33.1818, Train Loss: 1.6669, Valid Loss: 1.7554, Test Acc: 35.0000\n",
            "Epoch: 027, Train Acc: 32.0455, Train Loss: 1.6615, Valid Loss: 1.7433, Test Acc: 33.7500\n",
            "Epoch: 028, Train Acc: 29.0909, Train Loss: 1.6733, Valid Loss: 1.7426, Test Acc: 25.0000\n",
            "Epoch: 029, Train Acc: 32.9545, Train Loss: 1.6574, Valid Loss: 1.7503, Test Acc: 32.5000\n",
            "Epoch: 030, Train Acc: 30.0000, Train Loss: 1.6566, Valid Loss: 1.7502, Test Acc: 30.0000\n",
            "Epoch: 031, Train Acc: 32.9545, Train Loss: 1.6388, Valid Loss: 1.7169, Test Acc: 33.7500\n",
            "Epoch: 032, Train Acc: 33.1818, Train Loss: 1.6474, Valid Loss: 1.7399, Test Acc: 36.2500\n",
            "Epoch: 033, Train Acc: 31.3636, Train Loss: 1.6486, Valid Loss: 1.7434, Test Acc: 40.0000\n",
            "Epoch: 034, Train Acc: 35.0000, Train Loss: 1.6350, Valid Loss: 1.7637, Test Acc: 37.5000\n",
            "Epoch: 035, Train Acc: 34.7727, Train Loss: 1.6096, Valid Loss: 1.7368, Test Acc: 42.5000\n",
            "Epoch: 036, Train Acc: 36.3636, Train Loss: 1.6048, Valid Loss: 1.7207, Test Acc: 42.5000\n",
            "Epoch: 037, Train Acc: 36.3636, Train Loss: 1.6131, Valid Loss: 1.7211, Test Acc: 41.2500\n",
            "Epoch: 038, Train Acc: 35.0000, Train Loss: 1.6129, Valid Loss: 1.7188, Test Acc: 37.5000\n",
            "Epoch: 039, Train Acc: 35.4545, Train Loss: 1.6034, Valid Loss: 1.7231, Test Acc: 41.2500\n",
            "Epoch: 040, Train Acc: 38.8636, Train Loss: 1.5960, Valid Loss: 1.7182, Test Acc: 40.0000\n",
            "Epoch: 041, Train Acc: 32.9545, Train Loss: 1.6116, Valid Loss: 1.7059, Test Acc: 35.0000\n",
            "Epoch: 042, Train Acc: 36.3636, Train Loss: 1.5999, Valid Loss: 1.7355, Test Acc: 41.2500\n",
            "Epoch: 043, Train Acc: 31.1364, Train Loss: 1.6268, Valid Loss: 1.6703, Test Acc: 40.0000\n",
            "Epoch: 044, Train Acc: 32.0455, Train Loss: 1.6301, Valid Loss: 1.7164, Test Acc: 35.0000\n",
            "Epoch: 045, Train Acc: 35.2273, Train Loss: 1.5969, Valid Loss: 1.7261, Test Acc: 41.2500\n",
            "Epoch: 046, Train Acc: 32.9545, Train Loss: 1.6092, Valid Loss: 1.7386, Test Acc: 40.0000\n",
            "Epoch: 047, Train Acc: 35.6818, Train Loss: 1.5831, Valid Loss: 1.7243, Test Acc: 40.0000\n",
            "Epoch: 048, Train Acc: 34.3182, Train Loss: 1.5929, Valid Loss: 1.7212, Test Acc: 38.7500\n",
            "Epoch: 049, Train Acc: 35.4545, Train Loss: 1.5804, Valid Loss: 1.7276, Test Acc: 40.0000\n",
            "Epoch: 050, Train Acc: 35.4545, Train Loss: 1.6021, Valid Loss: 1.7351, Test Acc: 38.7500\n",
            "Epoch: 051, Train Acc: 33.1818, Train Loss: 1.5816, Valid Loss: 1.7210, Test Acc: 38.7500\n",
            "Epoch: 052, Train Acc: 33.6364, Train Loss: 1.5718, Valid Loss: 1.7066, Test Acc: 40.0000\n",
            "Epoch: 053, Train Acc: 32.0455, Train Loss: 1.6089, Valid Loss: 1.7360, Test Acc: 40.0000\n",
            "Epoch: 054, Train Acc: 35.9091, Train Loss: 1.5697, Valid Loss: 1.7262, Test Acc: 41.2500\n",
            "Epoch: 055, Train Acc: 36.8182, Train Loss: 1.5726, Valid Loss: 1.7527, Test Acc: 45.0000\n",
            "Epoch: 056, Train Acc: 32.5000, Train Loss: 1.5903, Valid Loss: 1.7348, Test Acc: 37.5000\n",
            "Epoch: 057, Train Acc: 35.2273, Train Loss: 1.5762, Valid Loss: 1.7594, Test Acc: 38.7500\n",
            "\n",
            "\n",
            "Best test accuracy:  45.0\n",
            "Best epoch:  55\n"
          ]
        }
      ],
      "source": [
        "#Model-specific arguments\n",
        "\n",
        "# Optimisation params\n",
        "args.lr=0.01\n",
        "args.weight_decay=0.0005\n",
        "\n",
        "# Model configuration\n",
        "args.d=3\n",
        "args.layers=4\n",
        "args.hidden_channels= 15\n",
        "args.input_dropout=0.0\n",
        "args.dropout=0.1\n",
        "args.orth = \"householder\"   #choices=['matrix_exp', 'cayley', 'householder', 'euler'], parametrization for the orthogonal group\n",
        "args.edges_feat = \"none\"    #choices=['none', 'concat', 'linear', 'bilinear']\n",
        "args.readout = \"mean\"       #choices=['mean', 'sum', 'max', 'sag', 'mlp', 'concat']\n",
        "\n",
        "\n",
        "config=args.__dict__\n",
        "print(config)\n",
        "\n",
        "model = DiscreteDiagSheafDiffusion(config)\n",
        "#model = DiscreteDiagPoolSheafDiffusion(config)\n",
        "#model = DiscreteBundleSheafDiffusion(config)\n",
        "model = model.to(args.device)\n",
        "\n",
        "sheaf_learner_params, other_params = model.grouped_parameters()\n",
        "\n",
        "# Optimizers \n",
        "optimizer = torch.optim.Adam([\n",
        "    {'params': sheaf_learner_params, 'weight_decay': config['sheaf_decay']},\n",
        "    {'params': other_params, 'weight_decay': config['weight_decay']}\n",
        "], lr=config['lr'])\n",
        "\n",
        "\n",
        "# Training and test criterions\n",
        "criterion_train = torch.nn.CrossEntropyLoss()\n",
        "# In order not to make test loss change with batch_size \n",
        "criterion_test = torch.nn.CrossEntropyLoss(reduction = 'sum')\n",
        "\n",
        "best_test_acc = 0\n",
        "best_epoch = 0\n",
        "# Initial best loss value, used for the early stopping technique\n",
        "best_loss = 100.00\n",
        "patience = args.patience\n",
        "\n",
        "for epoch in range(1, args.epochs):\n",
        "    # Training epoch\n",
        "    train(model, optimizer, criterion_train, train_loader, args.device)\n",
        "\n",
        "\n",
        "    # Compute training, validation and test accuracy\n",
        "    train_acc, train_loss = test(model, criterion_test, train_loader, args.device)\n",
        "    valid_acc, valid_loss = test(model, criterion_test, valid_loader, args.device)\n",
        "    test_acc, test_loss = test(model, criterion_test, test_loader, args.device)\n",
        "\n",
        "    if best_test_acc < test_acc:\n",
        "        best_test_acc = test_acc\n",
        "        best_epoch = epoch\n",
        "\n",
        "    if valid_loss < best_loss:\n",
        "        best_loss = valid_loss\n",
        "        patience = args.patience\n",
        "    else:\n",
        "        patience -= 1\n",
        "        \n",
        "    if patience <= 0: \n",
        "        # Early stopping with patience\n",
        "        break \n",
        "\n",
        "    print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Train Loss: {train_loss:.4f}, Valid Loss: {valid_loss:.4f}, Test Acc: {test_acc:.4f}')\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"Best test accuracy: \", best_test_acc)\n",
        "print(\"Best epoch: \", best_epoch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rc6kvXJZfzNX"
      },
      "source": [
        "#### Diagonal sheaf diffusion, **sum** readout "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1E4pcyIEfzNY",
        "outputId": "7c625baa-1e03-4352-98b6-22bd614edf8f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epochs': 200, 'lr': 0.01, 'weight_decay': 0.0005, 'sheaf_decay': 0.0005, 'patience': 15, 'second_linear': False, 'd': 3, 'layers': 4, 'normalised': True, 'deg_normalised': False, 'linear': False, 'hidden_channels': 15, 'input_dropout': 0.0, 'dropout': 0.1, 'left_weights': True, 'right_weights': True, 'add_lp': False, 'add_hp': False, 'use_act': True, 'sheaf_act': 'tanh', 'edge_weights': True, 'orth': 'householder', 'edges_feat': 'none', 'readout': 'sum', 'dense_intermediate_dim': 256, 'dense_output_graph_dim': 128, 'output_nn_intermediate_dim': 64, 'set_transformer_k': 8, 'input_dim': 3, 'output_dim': 6, 'input_dim_edge': 0, 'max_num_nodes_in_graph': 126, 'device': device(type='cuda', index=0)}\n",
            "Epoch: 001, Train Acc: 8.8636, Train Loss: 3.6313, Valid Loss: 3.2132, Test Acc: 15.0000\n",
            "Epoch: 002, Train Acc: 17.9545, Train Loss: 3.1746, Valid Loss: 3.0441, Test Acc: 15.0000\n",
            "Epoch: 003, Train Acc: 17.2727, Train Loss: 1.9633, Valid Loss: 2.0150, Test Acc: 18.7500\n",
            "Epoch: 004, Train Acc: 20.0000, Train Loss: 1.8745, Valid Loss: 1.9369, Test Acc: 18.7500\n",
            "Epoch: 005, Train Acc: 19.5455, Train Loss: 2.0285, Valid Loss: 2.1387, Test Acc: 20.0000\n",
            "Epoch: 006, Train Acc: 19.7727, Train Loss: 1.8652, Valid Loss: 1.9407, Test Acc: 17.5000\n",
            "Epoch: 007, Train Acc: 19.3182, Train Loss: 1.8385, Valid Loss: 1.8500, Test Acc: 20.0000\n",
            "Epoch: 008, Train Acc: 20.9091, Train Loss: 1.8216, Valid Loss: 1.8823, Test Acc: 18.7500\n",
            "Epoch: 009, Train Acc: 22.7273, Train Loss: 1.7939, Valid Loss: 1.8504, Test Acc: 21.2500\n",
            "Epoch: 010, Train Acc: 23.8636, Train Loss: 1.7578, Valid Loss: 1.7572, Test Acc: 18.7500\n",
            "Epoch: 011, Train Acc: 21.3636, Train Loss: 1.7920, Valid Loss: 1.7669, Test Acc: 21.2500\n",
            "Epoch: 012, Train Acc: 20.9091, Train Loss: 1.7776, Valid Loss: 1.7937, Test Acc: 21.2500\n",
            "Epoch: 013, Train Acc: 27.5000, Train Loss: 1.7318, Valid Loss: 1.7675, Test Acc: 22.5000\n",
            "Epoch: 014, Train Acc: 20.0000, Train Loss: 1.7708, Valid Loss: 1.7767, Test Acc: 20.0000\n",
            "Epoch: 015, Train Acc: 24.0909, Train Loss: 1.7707, Valid Loss: 1.8108, Test Acc: 26.2500\n",
            "Epoch: 016, Train Acc: 26.8182, Train Loss: 1.7383, Valid Loss: 1.7889, Test Acc: 26.2500\n",
            "Epoch: 017, Train Acc: 25.4545, Train Loss: 1.7292, Valid Loss: 1.7829, Test Acc: 17.5000\n",
            "Epoch: 018, Train Acc: 21.5909, Train Loss: 1.7385, Valid Loss: 1.7823, Test Acc: 23.7500\n",
            "Epoch: 019, Train Acc: 26.5909, Train Loss: 1.7304, Valid Loss: 1.7743, Test Acc: 27.5000\n",
            "Epoch: 020, Train Acc: 22.0455, Train Loss: 1.7317, Valid Loss: 1.7705, Test Acc: 21.2500\n",
            "Epoch: 021, Train Acc: 22.5000, Train Loss: 1.7258, Valid Loss: 1.7668, Test Acc: 21.2500\n",
            "Epoch: 022, Train Acc: 24.5455, Train Loss: 1.7327, Valid Loss: 1.7963, Test Acc: 26.2500\n",
            "Epoch: 023, Train Acc: 28.1818, Train Loss: 1.7102, Valid Loss: 1.7861, Test Acc: 30.0000\n",
            "Epoch: 024, Train Acc: 24.0909, Train Loss: 1.7193, Valid Loss: 1.7700, Test Acc: 20.0000\n",
            "\n",
            "\n",
            "Best test accuracy:  30.0\n",
            "Best epoch:  23\n"
          ]
        }
      ],
      "source": [
        "#Model-specific arguments\n",
        "\n",
        "# Optimisation params\n",
        "args.lr=0.01\n",
        "args.weight_decay=0.0005\n",
        "\n",
        "# Model configuration\n",
        "args.d=3\n",
        "args.layers=4\n",
        "args.hidden_channels= 15\n",
        "args.input_dropout=0.0\n",
        "args.dropout=0.1\n",
        "args.orth = \"householder\"   #choices=['matrix_exp', 'cayley', 'householder', 'euler'], parametrization for the orthogonal group\n",
        "args.edges_feat = \"none\"    #choices=['none', 'concat', 'linear', 'bilinear']\n",
        "args.readout = \"sum\"        #choices=['mean', 'sum', 'max', 'sag', 'mlp', 'concat']\n",
        "\n",
        "\n",
        "config=args.__dict__\n",
        "print(config)\n",
        "\n",
        "model = DiscreteDiagSheafDiffusion(config)\n",
        "#model = DiscreteDiagPoolSheafDiffusion(config)\n",
        "#model = DiscreteBundleSheafDiffusion(config)\n",
        "model = model.to(args.device)\n",
        "\n",
        "sheaf_learner_params, other_params = model.grouped_parameters()\n",
        "\n",
        "# Optimizers \n",
        "optimizer = torch.optim.Adam([\n",
        "    {'params': sheaf_learner_params, 'weight_decay': config['sheaf_decay']},\n",
        "    {'params': other_params, 'weight_decay': config['weight_decay']}\n",
        "], lr=config['lr'])\n",
        "\n",
        "\n",
        "# Training and test criterions\n",
        "criterion_train = torch.nn.CrossEntropyLoss()\n",
        "# In order not to make test loss change with batch_size \n",
        "criterion_test = torch.nn.CrossEntropyLoss(reduction = 'sum')\n",
        "\n",
        "best_test_acc = 0\n",
        "best_epoch = 0\n",
        "# Initial best loss value, used for the early stopping technique\n",
        "best_loss = 100.00\n",
        "patience = args.patience\n",
        "\n",
        "for epoch in range(1, args.epochs):\n",
        "    # Training epoch\n",
        "    train(model, optimizer, criterion_train, train_loader, args.device)\n",
        "\n",
        "\n",
        "    # Compute training, validation and test accuracy\n",
        "    train_acc, train_loss = test(model, criterion_test, train_loader, args.device)\n",
        "    valid_acc, valid_loss = test(model, criterion_test, valid_loader, args.device)\n",
        "    test_acc, test_loss = test(model, criterion_test, test_loader, args.device)\n",
        "\n",
        "    if best_test_acc < test_acc:\n",
        "        best_test_acc = test_acc\n",
        "        best_epoch = epoch\n",
        "\n",
        "    if valid_loss < best_loss:\n",
        "        best_loss = valid_loss\n",
        "        patience = args.patience\n",
        "    else:\n",
        "        patience -= 1\n",
        "        \n",
        "    if patience <= 0: \n",
        "        # Early stopping with patience\n",
        "        break \n",
        "\n",
        "    print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Train Loss: {train_loss:.4f}, Valid Loss: {valid_loss:.4f}, Test Acc: {test_acc:.4f}')\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"Best test accuracy: \", best_test_acc)\n",
        "print(\"Best epoch: \", best_epoch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZU36zG43f-P1"
      },
      "source": [
        "#### Diagonal sheaf diffusion, **max** readout "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M8vg5BLZf-P2",
        "outputId": "28700d47-780a-49d5-9a9c-e64410797e33"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epochs': 200, 'lr': 0.01, 'weight_decay': 0.0005, 'sheaf_decay': 0.0005, 'patience': 15, 'second_linear': False, 'd': 3, 'layers': 4, 'normalised': True, 'deg_normalised': False, 'linear': False, 'hidden_channels': 15, 'input_dropout': 0.0, 'dropout': 0.1, 'left_weights': True, 'right_weights': True, 'add_lp': False, 'add_hp': False, 'use_act': True, 'sheaf_act': 'tanh', 'edge_weights': True, 'orth': 'householder', 'edges_feat': 'none', 'readout': 'max', 'dense_intermediate_dim': 256, 'dense_output_graph_dim': 128, 'output_nn_intermediate_dim': 64, 'set_transformer_k': 8, 'input_dim': 3, 'output_dim': 6, 'input_dim_edge': 0, 'max_num_nodes_in_graph': 126, 'device': device(type='cuda', index=0)}\n",
            "Epoch: 001, Train Acc: 16.5909, Train Loss: 1.8175, Valid Loss: 1.8384, Test Acc: 18.7500\n",
            "Epoch: 002, Train Acc: 17.0455, Train Loss: 1.7918, Valid Loss: 1.7957, Test Acc: 13.7500\n",
            "Epoch: 003, Train Acc: 18.1818, Train Loss: 1.7924, Valid Loss: 1.8002, Test Acc: 13.7500\n",
            "Epoch: 004, Train Acc: 18.6364, Train Loss: 1.7883, Valid Loss: 1.8006, Test Acc: 13.7500\n",
            "Epoch: 005, Train Acc: 20.2273, Train Loss: 1.7871, Valid Loss: 1.8010, Test Acc: 17.5000\n",
            "Epoch: 006, Train Acc: 21.1364, Train Loss: 1.7872, Valid Loss: 1.8036, Test Acc: 21.2500\n",
            "Epoch: 007, Train Acc: 18.4091, Train Loss: 1.7858, Valid Loss: 1.8030, Test Acc: 21.2500\n",
            "Epoch: 008, Train Acc: 22.0455, Train Loss: 1.7822, Valid Loss: 1.8009, Test Acc: 23.7500\n",
            "Epoch: 009, Train Acc: 20.9091, Train Loss: 1.7775, Valid Loss: 1.7977, Test Acc: 22.5000\n",
            "Epoch: 010, Train Acc: 25.4545, Train Loss: 1.7713, Valid Loss: 1.7908, Test Acc: 27.5000\n",
            "Epoch: 011, Train Acc: 22.9545, Train Loss: 1.7665, Valid Loss: 1.7936, Test Acc: 21.2500\n",
            "Epoch: 012, Train Acc: 24.0909, Train Loss: 1.7640, Valid Loss: 1.7947, Test Acc: 28.7500\n",
            "Epoch: 013, Train Acc: 22.5000, Train Loss: 1.7582, Valid Loss: 1.7821, Test Acc: 21.2500\n",
            "Epoch: 014, Train Acc: 28.6364, Train Loss: 1.7541, Valid Loss: 1.8037, Test Acc: 25.0000\n",
            "Epoch: 015, Train Acc: 22.0455, Train Loss: 1.7522, Valid Loss: 1.8032, Test Acc: 25.0000\n",
            "Epoch: 016, Train Acc: 24.0909, Train Loss: 1.7488, Valid Loss: 1.8118, Test Acc: 22.5000\n",
            "Epoch: 017, Train Acc: 26.5909, Train Loss: 1.7352, Valid Loss: 1.7691, Test Acc: 27.5000\n",
            "Epoch: 018, Train Acc: 22.9545, Train Loss: 1.7497, Valid Loss: 1.7790, Test Acc: 23.7500\n",
            "Epoch: 019, Train Acc: 25.0000, Train Loss: 1.7481, Valid Loss: 1.8143, Test Acc: 22.5000\n",
            "Epoch: 020, Train Acc: 26.8182, Train Loss: 1.7531, Valid Loss: 1.7958, Test Acc: 26.2500\n",
            "Epoch: 021, Train Acc: 29.0909, Train Loss: 1.7197, Valid Loss: 1.7664, Test Acc: 26.2500\n",
            "Epoch: 022, Train Acc: 27.5000, Train Loss: 1.7373, Valid Loss: 1.7903, Test Acc: 25.0000\n",
            "Epoch: 023, Train Acc: 26.1364, Train Loss: 1.7415, Valid Loss: 1.7786, Test Acc: 26.2500\n",
            "Epoch: 024, Train Acc: 27.7273, Train Loss: 1.7384, Valid Loss: 1.7914, Test Acc: 31.2500\n",
            "Epoch: 025, Train Acc: 27.2727, Train Loss: 1.7149, Valid Loss: 1.7769, Test Acc: 26.2500\n",
            "Epoch: 026, Train Acc: 27.0455, Train Loss: 1.7083, Valid Loss: 1.7484, Test Acc: 27.5000\n",
            "Epoch: 027, Train Acc: 27.5000, Train Loss: 1.7305, Valid Loss: 1.7759, Test Acc: 27.5000\n",
            "Epoch: 028, Train Acc: 25.9091, Train Loss: 1.7374, Valid Loss: 1.7938, Test Acc: 26.2500\n",
            "Epoch: 029, Train Acc: 29.3182, Train Loss: 1.7044, Valid Loss: 1.7608, Test Acc: 26.2500\n",
            "Epoch: 030, Train Acc: 31.8182, Train Loss: 1.7140, Valid Loss: 1.7732, Test Acc: 30.0000\n",
            "Epoch: 031, Train Acc: 31.8182, Train Loss: 1.7182, Valid Loss: 1.7918, Test Acc: 27.5000\n",
            "Epoch: 032, Train Acc: 26.3636, Train Loss: 1.7250, Valid Loss: 1.7950, Test Acc: 27.5000\n",
            "Epoch: 033, Train Acc: 27.0455, Train Loss: 1.7393, Valid Loss: 1.8170, Test Acc: 26.2500\n",
            "Epoch: 034, Train Acc: 33.1818, Train Loss: 1.6856, Valid Loss: 1.7738, Test Acc: 28.7500\n",
            "Epoch: 035, Train Acc: 32.2727, Train Loss: 1.7376, Valid Loss: 1.8603, Test Acc: 26.2500\n",
            "Epoch: 036, Train Acc: 27.5000, Train Loss: 1.7602, Valid Loss: 1.8631, Test Acc: 27.5000\n",
            "Epoch: 037, Train Acc: 30.0000, Train Loss: 1.6779, Valid Loss: 1.7779, Test Acc: 23.7500\n",
            "Epoch: 038, Train Acc: 30.6818, Train Loss: 1.6976, Valid Loss: 1.8042, Test Acc: 31.2500\n",
            "Epoch: 039, Train Acc: 32.5000, Train Loss: 1.6969, Valid Loss: 1.7741, Test Acc: 30.0000\n",
            "Epoch: 040, Train Acc: 29.5455, Train Loss: 1.6941, Valid Loss: 1.7895, Test Acc: 26.2500\n",
            "\n",
            "\n",
            "Best test accuracy:  31.25\n",
            "Best epoch:  24\n"
          ]
        }
      ],
      "source": [
        "#Model-specific arguments\n",
        "\n",
        "# Optimisation params\n",
        "args.lr=0.01\n",
        "args.weight_decay=0.0005\n",
        "\n",
        "# Model configuration\n",
        "args.d=3\n",
        "args.layers=4\n",
        "args.hidden_channels= 15\n",
        "args.input_dropout=0.0\n",
        "args.dropout=0.1\n",
        "args.orth = \"householder\"   #choices=['matrix_exp', 'cayley', 'householder', 'euler'], parametrization for the orthogonal group\n",
        "args.edges_feat = \"none\"    #choices=['none', 'concat', 'linear', 'bilinear']\n",
        "args.readout = \"max\"        #choices=['mean', 'sum', 'max', 'sag', 'mlp', 'concat']\n",
        "\n",
        "\n",
        "config=args.__dict__\n",
        "print(config)\n",
        "\n",
        "model = DiscreteDiagSheafDiffusion(config)\n",
        "#model = DiscreteDiagPoolSheafDiffusion(config)\n",
        "#model = DiscreteBundleSheafDiffusion(config)\n",
        "model = model.to(args.device)\n",
        "\n",
        "sheaf_learner_params, other_params = model.grouped_parameters()\n",
        "\n",
        "# Optimizers \n",
        "optimizer = torch.optim.Adam([\n",
        "    {'params': sheaf_learner_params, 'weight_decay': config['sheaf_decay']},\n",
        "    {'params': other_params, 'weight_decay': config['weight_decay']}\n",
        "], lr=config['lr'])\n",
        "\n",
        "\n",
        "# Training and test criterions\n",
        "criterion_train = torch.nn.CrossEntropyLoss()\n",
        "# In order not to make test loss change with batch_size \n",
        "criterion_test = torch.nn.CrossEntropyLoss(reduction = 'sum')\n",
        "\n",
        "best_test_acc = 0\n",
        "best_epoch = 0\n",
        "# Initial best loss value, used for the early stopping technique\n",
        "best_loss = 100.00\n",
        "patience = args.patience\n",
        "\n",
        "for epoch in range(1, args.epochs):\n",
        "    # Training epoch\n",
        "    train(model, optimizer, criterion_train, train_loader, args.device)\n",
        "\n",
        "\n",
        "    # Compute training, validation and test accuracy\n",
        "    train_acc, train_loss = test(model, criterion_test, train_loader, args.device)\n",
        "    valid_acc, valid_loss = test(model, criterion_test, valid_loader, args.device)\n",
        "    test_acc, test_loss = test(model, criterion_test, test_loader, args.device)\n",
        "\n",
        "    if best_test_acc < test_acc:\n",
        "        best_test_acc = test_acc\n",
        "        best_epoch = epoch\n",
        "\n",
        "    if valid_loss < best_loss:\n",
        "        best_loss = valid_loss\n",
        "        patience = args.patience\n",
        "    else:\n",
        "        patience -= 1\n",
        "        \n",
        "    if patience <= 0: \n",
        "        # Early stopping with patience\n",
        "        break \n",
        "\n",
        "    print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Train Loss: {train_loss:.4f}, Valid Loss: {valid_loss:.4f}, Test Acc: {test_acc:.4f}')\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"Best test accuracy: \", best_test_acc)\n",
        "print(\"Best epoch: \", best_epoch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qL-z5N92JFB"
      },
      "source": [
        "#### Diagonal sheaf diffusion, **MLP** readout "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86M0g9Fw73j-",
        "outputId": "28827a5a-c868-4d5f-eacf-3b5dc34cc54c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epochs': 200, 'lr': 0.001, 'weight_decay': 0.0005, 'sheaf_decay': 0.0005, 'patience': 15, 'second_linear': False, 'd': 3, 'layers': 4, 'normalised': True, 'deg_normalised': False, 'linear': False, 'hidden_channels': 15, 'input_dropout': 0.0, 'dropout': 0.1, 'left_weights': True, 'right_weights': True, 'add_lp': False, 'add_hp': False, 'use_act': True, 'sheaf_act': 'tanh', 'edge_weights': True, 'orth': 'householder', 'edges_feat': 'none', 'readout': 'mlp', 'dense_intermediate_dim': 256, 'dense_output_graph_dim': 128, 'output_nn_intermediate_dim': 64, 'set_transformer_k': 8, 'input_dim': 3, 'output_dim': 6, 'input_dim_edge': 0, 'max_num_nodes_in_graph': 126, 'device': device(type='cuda', index=0)}\n",
            "Epoch: 001, Train Acc: 18.4091, Train Loss: 1.7894, Valid Loss: 1.7835, Test Acc: 13.7500\n",
            "Epoch: 002, Train Acc: 21.1364, Train Loss: 1.7808, Valid Loss: 1.7771, Test Acc: 15.0000\n",
            "Epoch: 003, Train Acc: 22.2727, Train Loss: 1.7704, Valid Loss: 1.7714, Test Acc: 17.5000\n",
            "Epoch: 004, Train Acc: 26.3636, Train Loss: 1.7608, Valid Loss: 1.7652, Test Acc: 17.5000\n",
            "Epoch: 005, Train Acc: 26.8182, Train Loss: 1.7524, Valid Loss: 1.7614, Test Acc: 17.5000\n",
            "Epoch: 006, Train Acc: 26.8182, Train Loss: 1.7452, Valid Loss: 1.7571, Test Acc: 18.7500\n",
            "Epoch: 007, Train Acc: 25.2273, Train Loss: 1.7391, Valid Loss: 1.7533, Test Acc: 17.5000\n",
            "Epoch: 008, Train Acc: 26.3636, Train Loss: 1.7326, Valid Loss: 1.7500, Test Acc: 17.5000\n",
            "Epoch: 009, Train Acc: 25.9091, Train Loss: 1.7265, Valid Loss: 1.7475, Test Acc: 17.5000\n",
            "Epoch: 010, Train Acc: 26.1364, Train Loss: 1.7201, Valid Loss: 1.7457, Test Acc: 16.2500\n",
            "Epoch: 011, Train Acc: 27.9545, Train Loss: 1.7137, Valid Loss: 1.7436, Test Acc: 17.5000\n",
            "Epoch: 012, Train Acc: 29.0909, Train Loss: 1.7082, Valid Loss: 1.7429, Test Acc: 17.5000\n",
            "Epoch: 013, Train Acc: 30.2273, Train Loss: 1.7030, Valid Loss: 1.7442, Test Acc: 18.7500\n",
            "Epoch: 014, Train Acc: 32.7273, Train Loss: 1.6983, Valid Loss: 1.7443, Test Acc: 22.5000\n",
            "Epoch: 015, Train Acc: 33.4091, Train Loss: 1.6915, Valid Loss: 1.7415, Test Acc: 22.5000\n",
            "Epoch: 016, Train Acc: 35.2273, Train Loss: 1.6838, Valid Loss: 1.7387, Test Acc: 25.0000\n",
            "Epoch: 017, Train Acc: 36.1364, Train Loss: 1.6765, Valid Loss: 1.7368, Test Acc: 23.7500\n",
            "Epoch: 018, Train Acc: 37.9545, Train Loss: 1.6678, Valid Loss: 1.7372, Test Acc: 23.7500\n",
            "Epoch: 019, Train Acc: 37.5000, Train Loss: 1.6592, Valid Loss: 1.7383, Test Acc: 22.5000\n",
            "Epoch: 020, Train Acc: 38.1818, Train Loss: 1.6526, Valid Loss: 1.7354, Test Acc: 23.7500\n",
            "Epoch: 021, Train Acc: 38.4091, Train Loss: 1.6458, Valid Loss: 1.7323, Test Acc: 25.0000\n",
            "Epoch: 022, Train Acc: 38.1818, Train Loss: 1.6363, Valid Loss: 1.7300, Test Acc: 26.2500\n",
            "Epoch: 023, Train Acc: 40.0000, Train Loss: 1.6228, Valid Loss: 1.7270, Test Acc: 26.2500\n",
            "Epoch: 024, Train Acc: 40.6818, Train Loss: 1.6069, Valid Loss: 1.7243, Test Acc: 26.2500\n",
            "Epoch: 025, Train Acc: 41.1364, Train Loss: 1.5959, Valid Loss: 1.7204, Test Acc: 27.5000\n",
            "Epoch: 026, Train Acc: 41.1364, Train Loss: 1.5822, Valid Loss: 1.7151, Test Acc: 28.7500\n",
            "Epoch: 027, Train Acc: 41.1364, Train Loss: 1.5675, Valid Loss: 1.7172, Test Acc: 28.7500\n",
            "Epoch: 028, Train Acc: 41.3636, Train Loss: 1.5499, Valid Loss: 1.7118, Test Acc: 28.7500\n",
            "Epoch: 029, Train Acc: 43.1818, Train Loss: 1.5345, Valid Loss: 1.7038, Test Acc: 27.5000\n",
            "Epoch: 030, Train Acc: 42.9545, Train Loss: 1.5123, Valid Loss: 1.6935, Test Acc: 28.7500\n",
            "Epoch: 031, Train Acc: 44.7727, Train Loss: 1.4943, Valid Loss: 1.6832, Test Acc: 31.2500\n",
            "Epoch: 032, Train Acc: 45.4545, Train Loss: 1.4773, Valid Loss: 1.6759, Test Acc: 31.2500\n",
            "Epoch: 033, Train Acc: 45.4545, Train Loss: 1.4651, Valid Loss: 1.6665, Test Acc: 33.7500\n",
            "Epoch: 034, Train Acc: 47.9545, Train Loss: 1.4431, Valid Loss: 1.6663, Test Acc: 35.0000\n",
            "Epoch: 035, Train Acc: 49.0909, Train Loss: 1.4311, Valid Loss: 1.6600, Test Acc: 36.2500\n",
            "Epoch: 036, Train Acc: 51.3636, Train Loss: 1.4156, Valid Loss: 1.6540, Test Acc: 35.0000\n",
            "Epoch: 037, Train Acc: 52.7273, Train Loss: 1.3987, Valid Loss: 1.6547, Test Acc: 33.7500\n",
            "Epoch: 038, Train Acc: 54.7727, Train Loss: 1.3753, Valid Loss: 1.6524, Test Acc: 35.0000\n",
            "Epoch: 039, Train Acc: 55.0000, Train Loss: 1.3507, Valid Loss: 1.6562, Test Acc: 35.0000\n",
            "Epoch: 040, Train Acc: 56.3636, Train Loss: 1.3246, Valid Loss: 1.6502, Test Acc: 35.0000\n",
            "Epoch: 041, Train Acc: 58.1818, Train Loss: 1.3033, Valid Loss: 1.6502, Test Acc: 37.5000\n",
            "Epoch: 042, Train Acc: 58.8636, Train Loss: 1.2804, Valid Loss: 1.6482, Test Acc: 36.2500\n",
            "Epoch: 043, Train Acc: 60.2273, Train Loss: 1.2588, Valid Loss: 1.6549, Test Acc: 35.0000\n",
            "Epoch: 044, Train Acc: 59.0909, Train Loss: 1.2360, Valid Loss: 1.6434, Test Acc: 37.5000\n",
            "Epoch: 045, Train Acc: 60.2273, Train Loss: 1.2135, Valid Loss: 1.6398, Test Acc: 35.0000\n",
            "Epoch: 046, Train Acc: 60.6818, Train Loss: 1.1839, Valid Loss: 1.6486, Test Acc: 36.2500\n",
            "Epoch: 047, Train Acc: 62.9545, Train Loss: 1.1508, Valid Loss: 1.6378, Test Acc: 43.7500\n",
            "Epoch: 048, Train Acc: 66.3636, Train Loss: 1.1165, Valid Loss: 1.6420, Test Acc: 40.0000\n",
            "Epoch: 049, Train Acc: 68.1818, Train Loss: 1.0854, Valid Loss: 1.6387, Test Acc: 40.0000\n",
            "Epoch: 050, Train Acc: 70.4545, Train Loss: 1.0601, Valid Loss: 1.6315, Test Acc: 40.0000\n",
            "Epoch: 051, Train Acc: 70.2273, Train Loss: 1.0379, Valid Loss: 1.6412, Test Acc: 40.0000\n",
            "Epoch: 052, Train Acc: 72.0455, Train Loss: 1.0181, Valid Loss: 1.6274, Test Acc: 38.7500\n",
            "Epoch: 053, Train Acc: 72.9545, Train Loss: 0.9872, Valid Loss: 1.6072, Test Acc: 42.5000\n",
            "Epoch: 054, Train Acc: 72.5000, Train Loss: 0.9623, Valid Loss: 1.5658, Test Acc: 37.5000\n",
            "Epoch: 055, Train Acc: 72.9545, Train Loss: 0.9254, Valid Loss: 1.5609, Test Acc: 40.0000\n",
            "Epoch: 056, Train Acc: 72.9545, Train Loss: 0.9035, Valid Loss: 1.5988, Test Acc: 38.7500\n",
            "Epoch: 057, Train Acc: 73.8636, Train Loss: 0.8851, Valid Loss: 1.6635, Test Acc: 37.5000\n",
            "Epoch: 058, Train Acc: 75.6818, Train Loss: 0.8701, Valid Loss: 1.6798, Test Acc: 38.7500\n",
            "Epoch: 059, Train Acc: 77.7273, Train Loss: 0.8436, Valid Loss: 1.6346, Test Acc: 38.7500\n",
            "Epoch: 060, Train Acc: 80.0000, Train Loss: 0.8166, Valid Loss: 1.5731, Test Acc: 40.0000\n",
            "Epoch: 061, Train Acc: 81.8182, Train Loss: 0.7808, Valid Loss: 1.5610, Test Acc: 41.2500\n",
            "Epoch: 062, Train Acc: 80.9091, Train Loss: 0.7511, Valid Loss: 1.5571, Test Acc: 41.2500\n",
            "Epoch: 063, Train Acc: 82.5000, Train Loss: 0.7226, Valid Loss: 1.5437, Test Acc: 41.2500\n",
            "Epoch: 064, Train Acc: 84.3182, Train Loss: 0.6966, Valid Loss: 1.5629, Test Acc: 41.2500\n",
            "Epoch: 065, Train Acc: 86.8182, Train Loss: 0.6597, Valid Loss: 1.5676, Test Acc: 41.2500\n",
            "Epoch: 066, Train Acc: 86.5909, Train Loss: 0.6291, Valid Loss: 1.6079, Test Acc: 42.5000\n",
            "Epoch: 067, Train Acc: 87.5000, Train Loss: 0.6139, Valid Loss: 1.6226, Test Acc: 40.0000\n",
            "Epoch: 068, Train Acc: 86.5909, Train Loss: 0.6017, Valid Loss: 1.5918, Test Acc: 41.2500\n",
            "Epoch: 069, Train Acc: 86.8182, Train Loss: 0.5924, Valid Loss: 1.5736, Test Acc: 41.2500\n",
            "Epoch: 070, Train Acc: 87.2727, Train Loss: 0.5557, Valid Loss: 1.5664, Test Acc: 40.0000\n",
            "Epoch: 071, Train Acc: 89.3182, Train Loss: 0.5252, Valid Loss: 1.5334, Test Acc: 42.5000\n",
            "Epoch: 072, Train Acc: 90.6818, Train Loss: 0.5041, Valid Loss: 1.5522, Test Acc: 42.5000\n",
            "Epoch: 073, Train Acc: 90.0000, Train Loss: 0.4935, Valid Loss: 1.5661, Test Acc: 41.2500\n",
            "Epoch: 074, Train Acc: 90.4545, Train Loss: 0.4794, Valid Loss: 1.5906, Test Acc: 41.2500\n",
            "Epoch: 075, Train Acc: 88.8636, Train Loss: 0.4729, Valid Loss: 1.6068, Test Acc: 41.2500\n",
            "Epoch: 076, Train Acc: 88.8636, Train Loss: 0.4563, Valid Loss: 1.6122, Test Acc: 42.5000\n",
            "Epoch: 077, Train Acc: 91.3636, Train Loss: 0.4270, Valid Loss: 1.6589, Test Acc: 42.5000\n",
            "Epoch: 078, Train Acc: 92.5000, Train Loss: 0.4053, Valid Loss: 1.6629, Test Acc: 45.0000\n",
            "Epoch: 079, Train Acc: 92.9545, Train Loss: 0.3917, Valid Loss: 1.6624, Test Acc: 48.7500\n",
            "Epoch: 080, Train Acc: 93.4091, Train Loss: 0.3741, Valid Loss: 1.6602, Test Acc: 48.7500\n",
            "Epoch: 081, Train Acc: 92.5000, Train Loss: 0.3567, Valid Loss: 1.5922, Test Acc: 50.0000\n",
            "Epoch: 082, Train Acc: 93.1818, Train Loss: 0.3401, Valid Loss: 1.5591, Test Acc: 50.0000\n",
            "Epoch: 083, Train Acc: 93.4091, Train Loss: 0.3250, Valid Loss: 1.5585, Test Acc: 48.7500\n",
            "Epoch: 084, Train Acc: 93.4091, Train Loss: 0.3201, Valid Loss: 1.5974, Test Acc: 46.2500\n",
            "Epoch: 085, Train Acc: 94.0909, Train Loss: 0.3190, Valid Loss: 1.6443, Test Acc: 46.2500\n",
            "\n",
            "\n",
            "Best test accuracy:  50.0\n",
            "Best epoch:  81\n"
          ]
        }
      ],
      "source": [
        "#Model-specific arguments\n",
        "\n",
        "# Optimisation params\n",
        "args.lr=0.001\n",
        "args.weight_decay=0.0005\n",
        "\n",
        "# Model configuration\n",
        "args.d=3\n",
        "args.layers=4\n",
        "args.hidden_channels= 15\n",
        "args.input_dropout=0.0\n",
        "args.dropout=0.1\n",
        "#args.orth = \"householder\"  #choices=['matrix_exp', 'cayley', 'householder', 'euler'], parametrization for the orthogonal group\n",
        "args.edges_feat = \"none\"    #choices=['none', 'concat', 'linear', 'bilinear']\n",
        "args.readout = \"mlp\"        #choices=['mean', 'sum', 'max', 'sag', 'mlp', 'concat']\n",
        "\n",
        "\n",
        "config=args.__dict__\n",
        "print(config)\n",
        "\n",
        "model = DiscreteDiagSheafDiffusion(config)\n",
        "#model = DiscreteDiagPoolSheafDiffusion(config)\n",
        "#model = DiscreteBundleSheafDiffusion(config)\n",
        "model = model.to(args.device)\n",
        "\n",
        "sheaf_learner_params, other_params = model.grouped_parameters()\n",
        "\n",
        "# Optimizers \n",
        "optimizer = torch.optim.Adam([\n",
        "    {'params': sheaf_learner_params, 'weight_decay': config['sheaf_decay']},\n",
        "    {'params': other_params, 'weight_decay': config['weight_decay']}\n",
        "], lr=config['lr'])\n",
        "\n",
        "\n",
        "# Training and test criterions\n",
        "criterion_train = torch.nn.CrossEntropyLoss()\n",
        "# In order not to make test loss change with batch_size \n",
        "criterion_test = torch.nn.CrossEntropyLoss(reduction = 'sum')\n",
        "\n",
        "best_test_acc = 0\n",
        "best_epoch = 0\n",
        "# Initial best loss value, used for the early stopping technique\n",
        "best_loss = 100.00\n",
        "patience = args.patience\n",
        "\n",
        "for epoch in range(1, args.epochs):\n",
        "    # Training epoch\n",
        "    train(model, optimizer, criterion_train, train_loader, args.device)\n",
        "\n",
        "\n",
        "    # Compute training, validation and test accuracy\n",
        "    train_acc, train_loss = test(model, criterion_test, train_loader, args.device)\n",
        "    valid_acc, valid_loss = test(model, criterion_test, valid_loader, args.device)\n",
        "    test_acc, test_loss = test(model, criterion_test, test_loader, args.device)\n",
        "\n",
        "    if best_test_acc < test_acc:\n",
        "        best_test_acc = test_acc\n",
        "        best_epoch = epoch\n",
        "\n",
        "    if valid_loss < best_loss:\n",
        "        best_loss = valid_loss\n",
        "        patience = args.patience\n",
        "    else:\n",
        "        patience -= 1\n",
        "        \n",
        "    if patience <= 0: \n",
        "        # Early stopping with patience\n",
        "        break \n",
        "\n",
        "    print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Train Loss: {train_loss:.4f}, Valid Loss: {valid_loss:.4f}, Test Acc: {test_acc:.4f}')\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"Best test accuracy: \", best_test_acc)\n",
        "print(\"Best epoch: \", best_epoch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4n6zFWO7d8aQ"
      },
      "source": [
        "#### Diagonal sheaf diffusion, **global SAG** readout "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IcJn8SeJd8aS",
        "outputId": "b1058685-d07f-46f0-8c10-8afa78c5133d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epochs': 200, 'lr': 0.01, 'weight_decay': 0.0005, 'sheaf_decay': 0.0005, 'patience': 15, 'second_linear': False, 'd': 3, 'layers': 4, 'normalised': True, 'deg_normalised': False, 'linear': False, 'hidden_channels': 15, 'input_dropout': 0.0, 'dropout': 0.1, 'left_weights': True, 'right_weights': True, 'add_lp': False, 'add_hp': False, 'use_act': True, 'sheaf_act': 'tanh', 'edge_weights': True, 'orth': 'householder', 'edges_feat': 'none', 'readout': 'sag', 'dense_intermediate_dim': 256, 'dense_output_graph_dim': 128, 'output_nn_intermediate_dim': 64, 'set_transformer_k': 8, 'input_dim': 3, 'output_dim': 6, 'input_dim_edge': 0, 'max_num_nodes_in_graph': 126, 'device': device(type='cuda', index=0)}\n",
            "Epoch: 001, Train Acc: 14.5455, Train Loss: 1.8949, Valid Loss: 1.9199, Test Acc: 16.2500\n",
            "Epoch: 002, Train Acc: 16.5909, Train Loss: 1.8280, Valid Loss: 1.8388, Test Acc: 18.7500\n",
            "Epoch: 003, Train Acc: 20.4545, Train Loss: 1.7940, Valid Loss: 1.8154, Test Acc: 17.5000\n",
            "Epoch: 004, Train Acc: 21.5909, Train Loss: 1.7916, Valid Loss: 1.8238, Test Acc: 18.7500\n",
            "Epoch: 005, Train Acc: 23.1818, Train Loss: 1.7808, Valid Loss: 1.8034, Test Acc: 21.2500\n",
            "Epoch: 006, Train Acc: 21.8182, Train Loss: 1.7651, Valid Loss: 1.7740, Test Acc: 15.0000\n",
            "Epoch: 007, Train Acc: 22.9545, Train Loss: 1.7572, Valid Loss: 1.7661, Test Acc: 15.0000\n",
            "Epoch: 008, Train Acc: 22.5000, Train Loss: 1.7533, Valid Loss: 1.7708, Test Acc: 15.0000\n",
            "Epoch: 009, Train Acc: 22.2727, Train Loss: 1.7516, Valid Loss: 1.7746, Test Acc: 16.2500\n",
            "Epoch: 010, Train Acc: 21.1364, Train Loss: 1.7572, Valid Loss: 1.7922, Test Acc: 18.7500\n",
            "Epoch: 011, Train Acc: 24.0909, Train Loss: 1.7279, Valid Loss: 1.7442, Test Acc: 17.5000\n",
            "Epoch: 012, Train Acc: 22.9545, Train Loss: 1.7284, Valid Loss: 1.7438, Test Acc: 13.7500\n",
            "Epoch: 013, Train Acc: 21.5909, Train Loss: 1.7390, Valid Loss: 1.7766, Test Acc: 17.5000\n",
            "Epoch: 014, Train Acc: 27.2727, Train Loss: 1.6942, Valid Loss: 1.7032, Test Acc: 18.7500\n",
            "Epoch: 015, Train Acc: 23.8636, Train Loss: 1.7142, Valid Loss: 1.7397, Test Acc: 18.7500\n",
            "Epoch: 016, Train Acc: 25.0000, Train Loss: 1.6916, Valid Loss: 1.7155, Test Acc: 15.0000\n",
            "Epoch: 017, Train Acc: 27.5000, Train Loss: 1.6792, Valid Loss: 1.6760, Test Acc: 22.5000\n",
            "Epoch: 018, Train Acc: 26.8182, Train Loss: 1.6883, Valid Loss: 1.6933, Test Acc: 21.2500\n",
            "Epoch: 019, Train Acc: 26.5909, Train Loss: 1.6739, Valid Loss: 1.6746, Test Acc: 21.2500\n",
            "Epoch: 020, Train Acc: 24.7727, Train Loss: 1.6871, Valid Loss: 1.6973, Test Acc: 26.2500\n",
            "Epoch: 021, Train Acc: 31.1364, Train Loss: 1.6626, Valid Loss: 1.6602, Test Acc: 25.0000\n",
            "Epoch: 022, Train Acc: 31.3636, Train Loss: 1.6633, Valid Loss: 1.6887, Test Acc: 28.7500\n",
            "Epoch: 023, Train Acc: 29.0909, Train Loss: 1.6513, Valid Loss: 1.7080, Test Acc: 20.0000\n",
            "Epoch: 024, Train Acc: 28.4091, Train Loss: 1.6430, Valid Loss: 1.6773, Test Acc: 25.0000\n",
            "Epoch: 025, Train Acc: 28.1818, Train Loss: 1.6579, Valid Loss: 1.6969, Test Acc: 23.7500\n",
            "Epoch: 026, Train Acc: 30.6818, Train Loss: 1.6413, Valid Loss: 1.6927, Test Acc: 22.5000\n",
            "Epoch: 027, Train Acc: 32.0455, Train Loss: 1.6535, Valid Loss: 1.7199, Test Acc: 25.0000\n",
            "Epoch: 028, Train Acc: 30.9091, Train Loss: 1.6308, Valid Loss: 1.7108, Test Acc: 27.5000\n",
            "Epoch: 029, Train Acc: 31.1364, Train Loss: 1.6191, Valid Loss: 1.6784, Test Acc: 25.0000\n",
            "Epoch: 030, Train Acc: 32.0455, Train Loss: 1.6317, Valid Loss: 1.6650, Test Acc: 23.7500\n",
            "Epoch: 031, Train Acc: 30.4545, Train Loss: 1.6507, Valid Loss: 1.7723, Test Acc: 21.2500\n",
            "Epoch: 032, Train Acc: 27.7273, Train Loss: 1.6546, Valid Loss: 1.7419, Test Acc: 20.0000\n",
            "Epoch: 033, Train Acc: 31.8182, Train Loss: 1.6026, Valid Loss: 1.6554, Test Acc: 26.2500\n",
            "Epoch: 034, Train Acc: 29.0909, Train Loss: 1.6089, Valid Loss: 1.6911, Test Acc: 22.5000\n",
            "Epoch: 035, Train Acc: 31.1364, Train Loss: 1.6056, Valid Loss: 1.6704, Test Acc: 20.0000\n",
            "Epoch: 036, Train Acc: 32.2727, Train Loss: 1.6122, Valid Loss: 1.6465, Test Acc: 27.5000\n",
            "Epoch: 037, Train Acc: 32.9545, Train Loss: 1.5841, Valid Loss: 1.6346, Test Acc: 21.2500\n",
            "Epoch: 038, Train Acc: 28.6364, Train Loss: 1.6269, Valid Loss: 1.7212, Test Acc: 23.7500\n",
            "Epoch: 039, Train Acc: 33.4091, Train Loss: 1.5703, Valid Loss: 1.6443, Test Acc: 22.5000\n",
            "Epoch: 040, Train Acc: 29.5455, Train Loss: 1.6466, Valid Loss: 1.6836, Test Acc: 22.5000\n",
            "Epoch: 041, Train Acc: 35.0000, Train Loss: 1.5695, Valid Loss: 1.6438, Test Acc: 22.5000\n",
            "Epoch: 042, Train Acc: 31.3636, Train Loss: 1.5925, Valid Loss: 1.6744, Test Acc: 27.5000\n",
            "Epoch: 043, Train Acc: 33.6364, Train Loss: 1.5722, Valid Loss: 1.6672, Test Acc: 25.0000\n",
            "Epoch: 044, Train Acc: 33.4091, Train Loss: 1.5668, Valid Loss: 1.6309, Test Acc: 23.7500\n",
            "Epoch: 045, Train Acc: 36.3636, Train Loss: 1.5620, Valid Loss: 1.6271, Test Acc: 32.5000\n",
            "Epoch: 046, Train Acc: 32.0455, Train Loss: 1.5848, Valid Loss: 1.6679, Test Acc: 35.0000\n",
            "Epoch: 047, Train Acc: 36.3636, Train Loss: 1.5463, Valid Loss: 1.6144, Test Acc: 22.5000\n",
            "Epoch: 048, Train Acc: 37.2727, Train Loss: 1.5458, Valid Loss: 1.5873, Test Acc: 23.7500\n",
            "Epoch: 049, Train Acc: 36.3636, Train Loss: 1.5373, Valid Loss: 1.5675, Test Acc: 26.2500\n",
            "Epoch: 050, Train Acc: 40.0000, Train Loss: 1.5116, Valid Loss: 1.5534, Test Acc: 31.2500\n",
            "Epoch: 051, Train Acc: 36.8182, Train Loss: 1.5098, Valid Loss: 1.5724, Test Acc: 26.2500\n",
            "Epoch: 052, Train Acc: 36.1364, Train Loss: 1.5159, Valid Loss: 1.5836, Test Acc: 30.0000\n",
            "Epoch: 053, Train Acc: 39.5455, Train Loss: 1.5000, Valid Loss: 1.5456, Test Acc: 33.7500\n",
            "Epoch: 054, Train Acc: 37.5000, Train Loss: 1.4949, Valid Loss: 1.5907, Test Acc: 35.0000\n",
            "Epoch: 055, Train Acc: 40.2273, Train Loss: 1.4594, Valid Loss: 1.5902, Test Acc: 33.7500\n",
            "Epoch: 056, Train Acc: 40.2273, Train Loss: 1.4532, Valid Loss: 1.6103, Test Acc: 31.2500\n",
            "Epoch: 057, Train Acc: 41.1364, Train Loss: 1.4768, Valid Loss: 1.6424, Test Acc: 30.0000\n",
            "Epoch: 058, Train Acc: 43.1818, Train Loss: 1.4324, Valid Loss: 1.5750, Test Acc: 36.2500\n",
            "Epoch: 059, Train Acc: 42.2727, Train Loss: 1.4191, Valid Loss: 1.6276, Test Acc: 38.7500\n",
            "Epoch: 060, Train Acc: 42.5000, Train Loss: 1.4243, Valid Loss: 1.6166, Test Acc: 31.2500\n",
            "Epoch: 061, Train Acc: 45.2273, Train Loss: 1.4037, Valid Loss: 1.5706, Test Acc: 30.0000\n",
            "Epoch: 062, Train Acc: 43.4091, Train Loss: 1.4227, Valid Loss: 1.5192, Test Acc: 35.0000\n",
            "Epoch: 063, Train Acc: 41.8182, Train Loss: 1.4199, Valid Loss: 1.5069, Test Acc: 38.7500\n",
            "Epoch: 064, Train Acc: 42.9545, Train Loss: 1.4409, Valid Loss: 1.5151, Test Acc: 37.5000\n",
            "Epoch: 065, Train Acc: 43.1818, Train Loss: 1.4506, Valid Loss: 1.5140, Test Acc: 33.7500\n",
            "Epoch: 066, Train Acc: 40.9091, Train Loss: 1.4625, Valid Loss: 1.7142, Test Acc: 28.7500\n",
            "Epoch: 067, Train Acc: 44.3182, Train Loss: 1.3946, Valid Loss: 1.5622, Test Acc: 32.5000\n",
            "Epoch: 068, Train Acc: 44.0909, Train Loss: 1.4100, Valid Loss: 1.5487, Test Acc: 38.7500\n",
            "Epoch: 069, Train Acc: 42.5000, Train Loss: 1.4142, Valid Loss: 1.6141, Test Acc: 37.5000\n",
            "Epoch: 070, Train Acc: 44.0909, Train Loss: 1.3695, Valid Loss: 1.5851, Test Acc: 38.7500\n",
            "Epoch: 071, Train Acc: 39.5455, Train Loss: 1.4251, Valid Loss: 1.6787, Test Acc: 38.7500\n",
            "Epoch: 072, Train Acc: 46.5909, Train Loss: 1.3722, Valid Loss: 1.5015, Test Acc: 31.2500\n",
            "Epoch: 073, Train Acc: 41.1364, Train Loss: 1.4305, Valid Loss: 1.6169, Test Acc: 32.5000\n",
            "Epoch: 074, Train Acc: 43.8636, Train Loss: 1.4061, Valid Loss: 1.5771, Test Acc: 32.5000\n",
            "Epoch: 075, Train Acc: 42.0455, Train Loss: 1.4149, Valid Loss: 1.4887, Test Acc: 35.0000\n",
            "Epoch: 076, Train Acc: 45.4545, Train Loss: 1.3791, Valid Loss: 1.5391, Test Acc: 45.0000\n",
            "Epoch: 077, Train Acc: 47.0455, Train Loss: 1.3517, Valid Loss: 1.5699, Test Acc: 37.5000\n",
            "Epoch: 078, Train Acc: 44.5455, Train Loss: 1.4110, Valid Loss: 1.5896, Test Acc: 30.0000\n",
            "Epoch: 079, Train Acc: 43.1818, Train Loss: 1.4162, Valid Loss: 1.6232, Test Acc: 37.5000\n",
            "Epoch: 080, Train Acc: 42.5000, Train Loss: 1.4161, Valid Loss: 1.6669, Test Acc: 37.5000\n",
            "Epoch: 081, Train Acc: 43.4091, Train Loss: 1.3706, Valid Loss: 1.6116, Test Acc: 32.5000\n",
            "Epoch: 082, Train Acc: 46.8182, Train Loss: 1.3312, Valid Loss: 1.5435, Test Acc: 42.5000\n",
            "Epoch: 083, Train Acc: 45.9091, Train Loss: 1.3989, Valid Loss: 1.6000, Test Acc: 41.2500\n",
            "Epoch: 084, Train Acc: 48.6364, Train Loss: 1.3415, Valid Loss: 1.5480, Test Acc: 38.7500\n",
            "Epoch: 085, Train Acc: 47.9545, Train Loss: 1.3170, Valid Loss: 1.5813, Test Acc: 42.5000\n",
            "Epoch: 086, Train Acc: 48.6364, Train Loss: 1.3038, Valid Loss: 1.4903, Test Acc: 32.5000\n",
            "Epoch: 087, Train Acc: 51.1364, Train Loss: 1.2562, Valid Loss: 1.4895, Test Acc: 42.5000\n",
            "Epoch: 088, Train Acc: 47.7273, Train Loss: 1.3261, Valid Loss: 1.5595, Test Acc: 42.5000\n",
            "Epoch: 089, Train Acc: 50.4545, Train Loss: 1.2941, Valid Loss: 1.4989, Test Acc: 33.7500\n",
            "\n",
            "\n",
            "Best test accuracy:  45.0\n",
            "Best epoch:  76\n"
          ]
        }
      ],
      "source": [
        "#Model-specific arguments\n",
        "\n",
        "# Optimisation params\n",
        "args.lr=0.01\n",
        "args.weight_decay=0.0005\n",
        "\n",
        "# Model configuration\n",
        "args.d=3\n",
        "args.layers=4\n",
        "args.hidden_channels= 15\n",
        "args.input_dropout=0.0\n",
        "args.dropout=0.1\n",
        "#args.orth = \"householder\"  #choices=['matrix_exp', 'cayley', 'householder', 'euler'], parametrization for the orthogonal group\n",
        "args.edges_feat = \"none\"    #choices=['none', 'concat', 'linear', 'bilinear']\n",
        "args.readout = \"sag\"        #choices=['mean', 'sum', 'max', 'sag', 'mlp', 'concat']\n",
        "\n",
        "\n",
        "config=args.__dict__\n",
        "print(config)\n",
        "\n",
        "model = DiscreteDiagSheafDiffusion(config)\n",
        "#model = DiscreteDiagPoolSheafDiffusion(config)\n",
        "#model = DiscreteBundleSheafDiffusion(config)\n",
        "model = model.to(args.device)\n",
        "\n",
        "sheaf_learner_params, other_params = model.grouped_parameters()\n",
        "\n",
        "# Optimizers \n",
        "optimizer = torch.optim.Adam([\n",
        "    {'params': sheaf_learner_params, 'weight_decay': config['sheaf_decay']},\n",
        "    {'params': other_params, 'weight_decay': config['weight_decay']}\n",
        "], lr=config['lr'])\n",
        "\n",
        "\n",
        "# Training and test criterions\n",
        "criterion_train = torch.nn.CrossEntropyLoss()\n",
        "# In order not to make test loss change with batch_size \n",
        "criterion_test = torch.nn.CrossEntropyLoss(reduction = 'sum')\n",
        "\n",
        "best_test_acc = 0\n",
        "best_epoch = 0\n",
        "# Initial best loss value, used for the early stopping technique\n",
        "best_loss = 100.00\n",
        "patience = args.patience\n",
        "\n",
        "for epoch in range(1, args.epochs):\n",
        "    # Training epoch\n",
        "    train(model, optimizer, criterion_train, train_loader, args.device)\n",
        "\n",
        "\n",
        "    # Compute training, validation and test accuracy\n",
        "    train_acc, train_loss = test(model, criterion_test, train_loader, args.device)\n",
        "    valid_acc, valid_loss = test(model, criterion_test, valid_loader, args.device)\n",
        "    test_acc, test_loss = test(model, criterion_test, test_loader, args.device)\n",
        "\n",
        "    if best_test_acc < test_acc:\n",
        "        best_test_acc = test_acc\n",
        "        best_epoch = epoch\n",
        "\n",
        "    if valid_loss < best_loss:\n",
        "        best_loss = valid_loss\n",
        "        patience = args.patience\n",
        "    else:\n",
        "        patience -= 1\n",
        "        \n",
        "    if patience <= 0: \n",
        "        # Early stopping with patience\n",
        "        break \n",
        "\n",
        "    print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Train Loss: {train_loss:.4f}, Valid Loss: {valid_loss:.4f}, Test Acc: {test_acc:.4f}')\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"Best test accuracy: \", best_test_acc)\n",
        "print(\"Best epoch: \", best_epoch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcKMQwtWel_C"
      },
      "source": [
        "#### Diagonal sheaf diffusion, **hierarchical SAG** readout "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iqHrGU0vel_D",
        "outputId": "6180aca7-89d7-4e08-e04a-c85e068d9e45"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epochs': 200, 'lr': 0.001, 'weight_decay': 0.0005, 'sheaf_decay': 0.0005, 'patience': 15, 'second_linear': False, 'd': 3, 'layers': 2, 'normalised': True, 'deg_normalised': False, 'linear': False, 'hidden_channels': 15, 'input_dropout': 0.0, 'dropout': 0.0, 'left_weights': True, 'right_weights': True, 'add_lp': False, 'add_hp': False, 'use_act': True, 'sheaf_act': 'tanh', 'edge_weights': True, 'orth': 'householder', 'edges_feat': 'none', 'readout': 'sag', 'dense_intermediate_dim': 256, 'dense_output_graph_dim': 128, 'output_nn_intermediate_dim': 64, 'set_transformer_k': 8, 'input_dim': 3, 'output_dim': 6, 'input_dim_edge': 0, 'max_num_nodes_in_graph': 126, 'device': device(type='cuda', index=0)}\n",
            "Epoch: 001, Train Acc: 17.0455, Train Loss: 1.7916, Valid Loss: 1.7909, Test Acc: 11.2500\n",
            "Epoch: 002, Train Acc: 17.0455, Train Loss: 1.7889, Valid Loss: 1.7869, Test Acc: 11.2500\n",
            "Epoch: 003, Train Acc: 17.0455, Train Loss: 1.7856, Valid Loss: 1.7809, Test Acc: 11.2500\n",
            "Epoch: 004, Train Acc: 17.0455, Train Loss: 1.7817, Valid Loss: 1.7732, Test Acc: 11.2500\n",
            "Epoch: 005, Train Acc: 17.0455, Train Loss: 1.7771, Valid Loss: 1.7705, Test Acc: 11.2500\n",
            "Epoch: 006, Train Acc: 17.0455, Train Loss: 1.7770, Valid Loss: 1.7740, Test Acc: 11.2500\n",
            "Epoch: 007, Train Acc: 17.0455, Train Loss: 1.7750, Valid Loss: 1.7762, Test Acc: 11.2500\n",
            "Epoch: 008, Train Acc: 16.8182, Train Loss: 1.7731, Valid Loss: 1.7777, Test Acc: 11.2500\n",
            "Epoch: 009, Train Acc: 16.3636, Train Loss: 1.7690, Valid Loss: 1.7721, Test Acc: 11.2500\n",
            "Epoch: 010, Train Acc: 17.2727, Train Loss: 1.7655, Valid Loss: 1.7669, Test Acc: 11.2500\n",
            "Epoch: 011, Train Acc: 17.9545, Train Loss: 1.7614, Valid Loss: 1.7508, Test Acc: 13.7500\n",
            "Epoch: 012, Train Acc: 19.3182, Train Loss: 1.7584, Valid Loss: 1.7459, Test Acc: 12.5000\n",
            "Epoch: 013, Train Acc: 20.0000, Train Loss: 1.7537, Valid Loss: 1.7326, Test Acc: 13.7500\n",
            "Epoch: 014, Train Acc: 21.1364, Train Loss: 1.7477, Valid Loss: 1.7232, Test Acc: 13.7500\n",
            "Epoch: 015, Train Acc: 21.5909, Train Loss: 1.7439, Valid Loss: 1.7203, Test Acc: 15.0000\n",
            "Epoch: 016, Train Acc: 22.7273, Train Loss: 1.7395, Valid Loss: 1.7205, Test Acc: 16.2500\n",
            "Epoch: 017, Train Acc: 24.5455, Train Loss: 1.7345, Valid Loss: 1.7213, Test Acc: 16.2500\n",
            "Epoch: 018, Train Acc: 22.9545, Train Loss: 1.7296, Valid Loss: 1.7197, Test Acc: 18.7500\n",
            "Epoch: 019, Train Acc: 24.5455, Train Loss: 1.7181, Valid Loss: 1.7131, Test Acc: 18.7500\n",
            "Epoch: 020, Train Acc: 24.7727, Train Loss: 1.7092, Valid Loss: 1.7074, Test Acc: 18.7500\n",
            "Epoch: 021, Train Acc: 26.8182, Train Loss: 1.7056, Valid Loss: 1.7001, Test Acc: 18.7500\n",
            "Epoch: 022, Train Acc: 27.0455, Train Loss: 1.6954, Valid Loss: 1.6850, Test Acc: 18.7500\n",
            "Epoch: 023, Train Acc: 27.9545, Train Loss: 1.6895, Valid Loss: 1.6943, Test Acc: 18.7500\n",
            "Epoch: 024, Train Acc: 28.6364, Train Loss: 1.6888, Valid Loss: 1.6993, Test Acc: 21.2500\n",
            "Epoch: 025, Train Acc: 30.2273, Train Loss: 1.6853, Valid Loss: 1.7021, Test Acc: 20.0000\n",
            "Epoch: 026, Train Acc: 29.0909, Train Loss: 1.6785, Valid Loss: 1.6888, Test Acc: 21.2500\n",
            "Epoch: 027, Train Acc: 28.1818, Train Loss: 1.6768, Valid Loss: 1.6823, Test Acc: 25.0000\n",
            "Epoch: 028, Train Acc: 29.3182, Train Loss: 1.6747, Valid Loss: 1.6847, Test Acc: 20.0000\n",
            "Epoch: 029, Train Acc: 31.3636, Train Loss: 1.6721, Valid Loss: 1.6969, Test Acc: 20.0000\n",
            "Epoch: 030, Train Acc: 30.9091, Train Loss: 1.6722, Valid Loss: 1.6914, Test Acc: 18.7500\n",
            "Epoch: 031, Train Acc: 30.0000, Train Loss: 1.6730, Valid Loss: 1.6818, Test Acc: 20.0000\n",
            "Epoch: 032, Train Acc: 30.4545, Train Loss: 1.6735, Valid Loss: 1.6853, Test Acc: 22.5000\n",
            "Epoch: 033, Train Acc: 31.5909, Train Loss: 1.6707, Valid Loss: 1.6957, Test Acc: 22.5000\n",
            "Epoch: 034, Train Acc: 31.3636, Train Loss: 1.6670, Valid Loss: 1.6968, Test Acc: 21.2500\n",
            "Epoch: 035, Train Acc: 31.5909, Train Loss: 1.6654, Valid Loss: 1.6803, Test Acc: 21.2500\n",
            "Epoch: 036, Train Acc: 30.9091, Train Loss: 1.6581, Valid Loss: 1.6768, Test Acc: 25.0000\n",
            "Epoch: 037, Train Acc: 32.0455, Train Loss: 1.6576, Valid Loss: 1.6900, Test Acc: 25.0000\n",
            "Epoch: 038, Train Acc: 32.9545, Train Loss: 1.6521, Valid Loss: 1.6873, Test Acc: 25.0000\n",
            "Epoch: 039, Train Acc: 32.9545, Train Loss: 1.6544, Valid Loss: 1.6785, Test Acc: 30.0000\n",
            "Epoch: 040, Train Acc: 35.0000, Train Loss: 1.6484, Valid Loss: 1.6880, Test Acc: 28.7500\n",
            "Epoch: 041, Train Acc: 32.0455, Train Loss: 1.6502, Valid Loss: 1.6849, Test Acc: 23.7500\n",
            "Epoch: 042, Train Acc: 33.6364, Train Loss: 1.6455, Valid Loss: 1.6801, Test Acc: 22.5000\n",
            "Epoch: 043, Train Acc: 35.0000, Train Loss: 1.6405, Valid Loss: 1.6704, Test Acc: 26.2500\n",
            "Epoch: 044, Train Acc: 34.3182, Train Loss: 1.6444, Valid Loss: 1.6619, Test Acc: 27.5000\n",
            "Epoch: 045, Train Acc: 35.0000, Train Loss: 1.6440, Valid Loss: 1.6622, Test Acc: 28.7500\n",
            "Epoch: 046, Train Acc: 34.5455, Train Loss: 1.6394, Valid Loss: 1.6729, Test Acc: 25.0000\n",
            "Epoch: 047, Train Acc: 33.4091, Train Loss: 1.6383, Valid Loss: 1.6779, Test Acc: 21.2500\n",
            "Epoch: 048, Train Acc: 36.5909, Train Loss: 1.6376, Valid Loss: 1.6535, Test Acc: 28.7500\n",
            "Epoch: 049, Train Acc: 35.4545, Train Loss: 1.6359, Valid Loss: 1.6420, Test Acc: 28.7500\n",
            "Epoch: 050, Train Acc: 36.1364, Train Loss: 1.6362, Valid Loss: 1.6578, Test Acc: 26.2500\n",
            "Epoch: 051, Train Acc: 35.4545, Train Loss: 1.6329, Valid Loss: 1.6597, Test Acc: 26.2500\n",
            "Epoch: 052, Train Acc: 36.5909, Train Loss: 1.6329, Valid Loss: 1.6413, Test Acc: 23.7500\n",
            "Epoch: 053, Train Acc: 35.4545, Train Loss: 1.6314, Valid Loss: 1.6595, Test Acc: 23.7500\n",
            "Epoch: 054, Train Acc: 34.7727, Train Loss: 1.6301, Valid Loss: 1.6498, Test Acc: 23.7500\n",
            "Epoch: 055, Train Acc: 34.7727, Train Loss: 1.6295, Valid Loss: 1.6652, Test Acc: 26.2500\n",
            "Epoch: 056, Train Acc: 37.0455, Train Loss: 1.6249, Valid Loss: 1.6438, Test Acc: 26.2500\n",
            "Epoch: 057, Train Acc: 36.1364, Train Loss: 1.6329, Valid Loss: 1.6362, Test Acc: 26.2500\n",
            "Epoch: 058, Train Acc: 35.9091, Train Loss: 1.6290, Valid Loss: 1.6430, Test Acc: 26.2500\n",
            "Epoch: 059, Train Acc: 34.3182, Train Loss: 1.6367, Valid Loss: 1.6722, Test Acc: 30.0000\n",
            "Epoch: 060, Train Acc: 35.0000, Train Loss: 1.6290, Valid Loss: 1.6607, Test Acc: 27.5000\n",
            "Epoch: 061, Train Acc: 34.0909, Train Loss: 1.6257, Valid Loss: 1.6482, Test Acc: 25.0000\n",
            "Epoch: 062, Train Acc: 34.3182, Train Loss: 1.6399, Valid Loss: 1.6661, Test Acc: 22.5000\n",
            "Epoch: 063, Train Acc: 35.6818, Train Loss: 1.6298, Valid Loss: 1.6522, Test Acc: 28.7500\n",
            "Epoch: 064, Train Acc: 35.9091, Train Loss: 1.6286, Valid Loss: 1.6619, Test Acc: 31.2500\n",
            "Epoch: 065, Train Acc: 34.5455, Train Loss: 1.6344, Valid Loss: 1.6847, Test Acc: 26.2500\n",
            "Epoch: 066, Train Acc: 37.0455, Train Loss: 1.6140, Valid Loss: 1.6565, Test Acc: 31.2500\n",
            "Epoch: 067, Train Acc: 37.2727, Train Loss: 1.6153, Valid Loss: 1.6088, Test Acc: 26.2500\n",
            "Epoch: 068, Train Acc: 37.9545, Train Loss: 1.6123, Valid Loss: 1.6396, Test Acc: 28.7500\n",
            "Epoch: 069, Train Acc: 35.9091, Train Loss: 1.6254, Valid Loss: 1.6619, Test Acc: 25.0000\n",
            "Epoch: 070, Train Acc: 37.7273, Train Loss: 1.6179, Valid Loss: 1.6565, Test Acc: 30.0000\n",
            "Epoch: 071, Train Acc: 35.9091, Train Loss: 1.6112, Valid Loss: 1.6342, Test Acc: 26.2500\n",
            "Epoch: 072, Train Acc: 38.6364, Train Loss: 1.6054, Valid Loss: 1.6177, Test Acc: 28.7500\n",
            "Epoch: 073, Train Acc: 36.1364, Train Loss: 1.6171, Valid Loss: 1.6322, Test Acc: 27.5000\n",
            "Epoch: 074, Train Acc: 36.1364, Train Loss: 1.6228, Valid Loss: 1.6664, Test Acc: 30.0000\n",
            "Epoch: 075, Train Acc: 37.0455, Train Loss: 1.6115, Valid Loss: 1.6552, Test Acc: 30.0000\n",
            "Epoch: 076, Train Acc: 37.0455, Train Loss: 1.6164, Valid Loss: 1.6299, Test Acc: 20.0000\n",
            "Epoch: 077, Train Acc: 35.6818, Train Loss: 1.6084, Valid Loss: 1.6327, Test Acc: 35.0000\n",
            "Epoch: 078, Train Acc: 35.0000, Train Loss: 1.6109, Valid Loss: 1.6380, Test Acc: 28.7500\n",
            "Epoch: 079, Train Acc: 36.8182, Train Loss: 1.6089, Valid Loss: 1.6214, Test Acc: 28.7500\n",
            "Epoch: 080, Train Acc: 37.5000, Train Loss: 1.6089, Valid Loss: 1.6203, Test Acc: 30.0000\n",
            "Epoch: 081, Train Acc: 36.5909, Train Loss: 1.6074, Valid Loss: 1.6113, Test Acc: 32.5000\n",
            "\n",
            "\n",
            "Best test accuracy:  35.0\n",
            "Best epoch:  77\n"
          ]
        }
      ],
      "source": [
        "#Model-specific arguments\n",
        "\n",
        "# Optimisation params\n",
        "args.lr=0.001\n",
        "args.weight_decay=0.0005\n",
        "\n",
        "# Model configuration\n",
        "args.d=3\n",
        "args.layers=2\n",
        "args.hidden_channels= 15\n",
        "args.input_dropout=0.0\n",
        "args.dropout=0.0\n",
        "#args.orth = \"householder\"  #choices=['matrix_exp', 'cayley', 'householder', 'euler'], parametrization for the orthogonal group\n",
        "args.edges_feat = \"none\"    #choices=['none', 'concat', 'linear', 'bilinear']\n",
        "#args.readout = \"sag\"       #choices=['mean', 'sum', 'max', 'sag', 'mlp', 'concat']\n",
        "\n",
        "\n",
        "config=args.__dict__\n",
        "print(config)\n",
        "\n",
        "#model = DiscreteDiagSheafDiffusion(config)\n",
        "model = DiscreteDiagPoolSheafDiffusion(config)\n",
        "#model = DiscreteBundleSheafDiffusion(config)\n",
        "model = model.to(args.device)\n",
        "\n",
        "sheaf_learner_params, other_params = model.grouped_parameters()\n",
        "\n",
        "# Optimizers \n",
        "optimizer = torch.optim.Adam([\n",
        "    {'params': sheaf_learner_params, 'weight_decay': config['sheaf_decay']},\n",
        "    {'params': other_params, 'weight_decay': config['weight_decay']}\n",
        "], lr=config['lr'])\n",
        "\n",
        "\n",
        "# Training and test criterions\n",
        "criterion_train = torch.nn.CrossEntropyLoss()\n",
        "# In order not to make test loss change with batch_size \n",
        "criterion_test = torch.nn.CrossEntropyLoss(reduction = 'sum')\n",
        "\n",
        "best_test_acc = 0\n",
        "best_epoch = 0\n",
        "# Initial best loss value, used for the early stopping technique\n",
        "best_loss = 100.00\n",
        "patience = args.patience\n",
        "\n",
        "for epoch in range(1, args.epochs):\n",
        "    # Training epoch\n",
        "    train(model, optimizer, criterion_train, train_loader, args.device)\n",
        "\n",
        "\n",
        "    # Compute training, validation and test accuracy\n",
        "    train_acc, train_loss = test(model, criterion_test, train_loader, args.device)\n",
        "    valid_acc, valid_loss = test(model, criterion_test, valid_loader, args.device)\n",
        "    test_acc, test_loss = test(model, criterion_test, test_loader, args.device)\n",
        "\n",
        "    if best_test_acc < test_acc:\n",
        "        best_test_acc = test_acc\n",
        "        best_epoch = epoch\n",
        "\n",
        "    if valid_loss < best_loss:\n",
        "        best_loss = valid_loss\n",
        "        patience = args.patience\n",
        "    else:\n",
        "        patience -= 1\n",
        "        \n",
        "    if patience <= 0: \n",
        "        # Early stopping with patience\n",
        "        break \n",
        "\n",
        "    print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Train Loss: {train_loss:.4f}, Valid Loss: {valid_loss:.4f}, Test Acc: {test_acc:.4f}')\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"Best test accuracy: \", best_test_acc)\n",
        "print(\"Best epoch: \", best_epoch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eG6tmQzolcdY"
      },
      "source": [
        "## Mutagenicity"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Za0ZdINim7Oe"
      },
      "source": [
        "### Download of the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V7aMkuZQCTvj",
        "outputId": "76a926c6-eb3b-441c-8aac-153adfa9a435"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading https://www.chrsmrrs.com/graphkerneldatasets/Mutagenicity.zip\n",
            "Extracting data/TUDataset/Mutagenicity/Mutagenicity.zip\n",
            "Processing...\n",
            "Done!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset: Mutagenicity(4337):\n",
            "====================\n",
            "Number of graphs: 4337\n",
            "Number of node features: 14\n",
            "Number of edge features: 3\n",
            "Number of classes: 2\n",
            "====================\n",
            "Data(edge_index=[2, 32], x=[16, 14], edge_attr=[32, 3], y=[1])\n",
            "\n",
            "Contains isolated nodes: False\n",
            "Contains self-loops: False\n",
            "Is undirected: True\n"
          ]
        }
      ],
      "source": [
        "import torch_geometric.transforms as T\n",
        "from torch_geometric.utils.undirected import to_undirected\n",
        "from torch_geometric.utils import remove_self_loops\n",
        "\n",
        "dataset = TUDataset(root='data/TUDataset', name='Mutagenicity', transform=T.NormalizeFeatures())\n",
        "\n",
        "print(f'Dataset: {dataset}:')\n",
        "print('====================')\n",
        "print(f'Number of graphs: {len(dataset)}')\n",
        "print(f'Number of node features: {dataset.num_node_features}')\n",
        "print(f'Number of edge features: {dataset.num_edge_features}')\n",
        "print(f'Number of classes: {dataset.num_classes}')\n",
        "\n",
        "# In order not to mess with laplacian: must be undirected and not contain self loops\n",
        "# Check that does not contain self loops\n",
        "\n",
        "max_num_nodes_in_graph = 0\n",
        "for data in dataset:\n",
        "\n",
        "    # Remove self-loops\n",
        "    data.edge_index, _ = remove_self_loops(data.edge_index)\n",
        "\n",
        "    # Make the graph undirected\n",
        "    data.edge_index = to_undirected(data.edge_index)\n",
        "    num_nodes = data.x.size(dim=0)\n",
        "    \n",
        "    if num_nodes > max_num_nodes_in_graph:\n",
        "      max_num_nodes_in_graph = num_nodes\n",
        "\n",
        "print('====================')\n",
        "graph1 = dataset[0]  # Get the first graph object.\n",
        "print(graph1)\n",
        "print()\n",
        "\n",
        "print(f'Contains isolated nodes: {graph1.has_isolated_nodes()}')\n",
        "print(f'Contains self-loops: {graph1.has_self_loops()}')\n",
        "print(f'Is undirected: {graph1.is_undirected()}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9PwnbNJ4irI"
      },
      "source": [
        "### Split into training, validation and test set "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zWjwf97k4JwX",
        "outputId": "0fa09d99-8a11-4df8-d163-c0ff31343e5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of training graphs: 3700\n",
            "Number of validation graphs: 300\n",
            "Number of test graphs: 337\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(12345)\n",
        "dataset = dataset.shuffle()\n",
        "\n",
        "train_dataset = dataset[:3700]\n",
        "valid_dataset = dataset[3700:4000]\n",
        "test_dataset = dataset[4000:]\n",
        "\n",
        "print(f'Number of training graphs: {len(train_dataset)}')\n",
        "print(f'Number of validation graphs: {len(valid_dataset)}')\n",
        "print(f'Number of test graphs: {len(test_dataset)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qq5CCNuY4mia"
      },
      "source": [
        "### Creation of Dataloader \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mrtj8TEe4o9R",
        "outputId": "2351d461-b464-49f4-84b4-b8e3099128d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 1:\n",
            "=======\n",
            "Number of graphs in the current batch: 256\n",
            "DataBatch(edge_index=[2, 15054], x=[7536, 14], edge_attr=[15054, 3], y=[256], batch=[7536], ptr=[257])\n",
            "\n",
            "Step 2:\n",
            "=======\n",
            "Number of graphs in the current batch: 256\n",
            "DataBatch(edge_index=[2, 15748], x=[7689, 14], edge_attr=[15748, 3], y=[256], batch=[7689], ptr=[257])\n",
            "\n",
            "Step 3:\n",
            "=======\n",
            "Number of graphs in the current batch: 256\n",
            "DataBatch(edge_index=[2, 15150], x=[7470, 14], edge_attr=[15150, 3], y=[256], batch=[7470], ptr=[257])\n",
            "\n",
            "Step 4:\n",
            "=======\n",
            "Number of graphs in the current batch: 256\n",
            "DataBatch(edge_index=[2, 16882], x=[8364, 14], edge_attr=[16882, 3], y=[256], batch=[8364], ptr=[257])\n",
            "\n",
            "Step 5:\n",
            "=======\n",
            "Number of graphs in the current batch: 256\n",
            "DataBatch(edge_index=[2, 15272], x=[7455, 14], edge_attr=[15272, 3], y=[256], batch=[7455], ptr=[257])\n",
            "\n",
            "Step 6:\n",
            "=======\n",
            "Number of graphs in the current batch: 256\n",
            "DataBatch(edge_index=[2, 15138], x=[7358, 14], edge_attr=[15138, 3], y=[256], batch=[7358], ptr=[257])\n",
            "\n",
            "Step 7:\n",
            "=======\n",
            "Number of graphs in the current batch: 256\n",
            "DataBatch(edge_index=[2, 15570], x=[7741, 14], edge_attr=[15570, 3], y=[256], batch=[7741], ptr=[257])\n",
            "\n",
            "Step 8:\n",
            "=======\n",
            "Number of graphs in the current batch: 256\n",
            "DataBatch(edge_index=[2, 16582], x=[8334, 14], edge_attr=[16582, 3], y=[256], batch=[8334], ptr=[257])\n",
            "\n",
            "Step 9:\n",
            "=======\n",
            "Number of graphs in the current batch: 256\n",
            "DataBatch(edge_index=[2, 16098], x=[7870, 14], edge_attr=[16098, 3], y=[256], batch=[7870], ptr=[257])\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch_geometric/deprecation.py:12: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
            "  warnings.warn(out)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 10:\n",
            "=======\n",
            "Number of graphs in the current batch: 256\n",
            "DataBatch(edge_index=[2, 16368], x=[7943, 14], edge_attr=[16368, 3], y=[256], batch=[7943], ptr=[257])\n",
            "\n",
            "Step 11:\n",
            "=======\n",
            "Number of graphs in the current batch: 256\n",
            "DataBatch(edge_index=[2, 15678], x=[7678, 14], edge_attr=[15678, 3], y=[256], batch=[7678], ptr=[257])\n",
            "\n",
            "Step 12:\n",
            "=======\n",
            "Number of graphs in the current batch: 256\n",
            "DataBatch(edge_index=[2, 15672], x=[7643, 14], edge_attr=[15672, 3], y=[256], batch=[7643], ptr=[257])\n",
            "\n",
            "Step 13:\n",
            "=======\n",
            "Number of graphs in the current batch: 256\n",
            "DataBatch(edge_index=[2, 15094], x=[7363, 14], edge_attr=[15094, 3], y=[256], batch=[7363], ptr=[257])\n",
            "\n",
            "Step 14:\n",
            "=======\n",
            "Number of graphs in the current batch: 256\n",
            "DataBatch(edge_index=[2, 16054], x=[8023, 14], edge_attr=[16054, 3], y=[256], batch=[8023], ptr=[257])\n",
            "\n",
            "Step 15:\n",
            "=======\n",
            "Number of graphs in the current batch: 116\n",
            "DataBatch(edge_index=[2, 6874], x=[3346, 14], edge_attr=[6874, 3], y=[116], batch=[3346], ptr=[117])\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from torch_geometric.data import DataLoader\n",
        "\n",
        "batch_size = 256\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size = batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size = batch_size, shuffle=False)\n",
        "\n",
        "for step, data in enumerate(train_loader):\n",
        "    print(f'Step {step + 1}:')\n",
        "    print('=======')\n",
        "    print(f'Number of graphs in the current batch: {data.num_graphs}')\n",
        "    print(data)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZ_ArOYHpdjj"
      },
      "source": [
        "### Running of experiments \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "MhuM33iopdjn"
      },
      "outputs": [],
      "source": [
        "# Add extra arguments\n",
        "args.input_dim = dataset.num_features\n",
        "args.output_dim = dataset.num_classes\n",
        "args.input_dim_edge = dataset.num_edge_features\n",
        "args.max_num_nodes_in_graph = max_num_nodes_in_graph\n",
        "args.device = torch.device(f'cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "assert args.normalised or args.deg_normalised\n",
        "if args.sheaf_decay is None:\n",
        "    args.sheaf_decay = args.weight_decay"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3piLlA1olB7"
      },
      "source": [
        "#### Baseline scalar model (**d=1**), [mean, sum, max] readout "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "esvAuWTLp8gN",
        "outputId": "28a7061b-0e6a-4da3-f64a-9de2f8e28b1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epochs': 200, 'lr': 0.01, 'weight_decay': 0.0005, 'sheaf_decay': 0.0005, 'patience': 15, 'second_linear': False, 'd': 1, 'layers': 4, 'normalised': True, 'deg_normalised': False, 'linear': False, 'hidden_channels': 15, 'input_dropout': 0.0, 'dropout': 0.1, 'left_weights': True, 'right_weights': True, 'add_lp': False, 'add_hp': False, 'use_act': True, 'sheaf_act': 'tanh', 'edge_weights': True, 'orth': 'householder', 'edges_feat': 'none', 'readout': 'concat', 'dense_intermediate_dim': 256, 'dense_output_graph_dim': 128, 'output_nn_intermediate_dim': 64, 'set_transformer_k': 8, 'input_dim': 14, 'output_dim': 2, 'input_dim_edge': 3, 'max_num_nodes_in_graph': 417, 'device': device(type='cuda', index=0)}\n",
            "Epoch: 001, Train Acc: 65.7297, Train Loss: 0.6309, Valid Loss: 0.6401, Test Acc: 64.9852\n",
            "Epoch: 002, Train Acc: 65.4054, Train Loss: 0.6041, Valid Loss: 0.5984, Test Acc: 65.2819\n",
            "Epoch: 003, Train Acc: 69.3514, Train Loss: 0.5834, Valid Loss: 0.5977, Test Acc: 67.0623\n",
            "Epoch: 004, Train Acc: 70.4324, Train Loss: 0.5693, Valid Loss: 0.5931, Test Acc: 68.2493\n",
            "Epoch: 005, Train Acc: 70.5676, Train Loss: 0.5635, Valid Loss: 0.5758, Test Acc: 67.6558\n",
            "Epoch: 006, Train Acc: 70.0000, Train Loss: 0.5710, Valid Loss: 0.6046, Test Acc: 69.1395\n",
            "Epoch: 007, Train Acc: 70.8649, Train Loss: 0.5594, Valid Loss: 0.5777, Test Acc: 68.2493\n",
            "Epoch: 008, Train Acc: 70.5135, Train Loss: 0.5561, Valid Loss: 0.5772, Test Acc: 67.3591\n",
            "Epoch: 009, Train Acc: 70.7838, Train Loss: 0.5509, Valid Loss: 0.5813, Test Acc: 69.7329\n",
            "Epoch: 010, Train Acc: 70.8108, Train Loss: 0.5465, Valid Loss: 0.5671, Test Acc: 69.7329\n",
            "Epoch: 011, Train Acc: 71.7297, Train Loss: 0.5402, Valid Loss: 0.5661, Test Acc: 69.4362\n",
            "Epoch: 012, Train Acc: 72.1081, Train Loss: 0.5419, Valid Loss: 0.5739, Test Acc: 72.7003\n",
            "Epoch: 013, Train Acc: 68.9459, Train Loss: 0.5601, Valid Loss: 0.5774, Test Acc: 66.7656\n",
            "Epoch: 014, Train Acc: 71.2703, Train Loss: 0.5499, Valid Loss: 0.5925, Test Acc: 70.3264\n",
            "Epoch: 015, Train Acc: 73.1622, Train Loss: 0.5284, Valid Loss: 0.5589, Test Acc: 73.2938\n",
            "Epoch: 016, Train Acc: 73.2432, Train Loss: 0.5247, Valid Loss: 0.5570, Test Acc: 71.2166\n",
            "Epoch: 017, Train Acc: 72.5946, Train Loss: 0.5256, Valid Loss: 0.5530, Test Acc: 70.9199\n",
            "Epoch: 018, Train Acc: 74.6216, Train Loss: 0.5215, Valid Loss: 0.5455, Test Acc: 74.4807\n",
            "Epoch: 019, Train Acc: 74.3784, Train Loss: 0.5100, Valid Loss: 0.5352, Test Acc: 72.9970\n",
            "Epoch: 020, Train Acc: 73.6486, Train Loss: 0.5291, Valid Loss: 0.6033, Test Acc: 72.1068\n",
            "Epoch: 021, Train Acc: 73.6486, Train Loss: 0.5213, Valid Loss: 0.5494, Test Acc: 70.3264\n",
            "Epoch: 022, Train Acc: 74.7297, Train Loss: 0.5066, Valid Loss: 0.5407, Test Acc: 74.4807\n",
            "Epoch: 023, Train Acc: 74.8108, Train Loss: 0.5089, Valid Loss: 0.5646, Test Acc: 74.4807\n",
            "Epoch: 024, Train Acc: 75.4054, Train Loss: 0.4999, Valid Loss: 0.5361, Test Acc: 76.2611\n",
            "Epoch: 025, Train Acc: 75.9459, Train Loss: 0.5100, Valid Loss: 0.5485, Test Acc: 75.6677\n",
            "Epoch: 026, Train Acc: 76.2973, Train Loss: 0.4992, Valid Loss: 0.5532, Test Acc: 75.9644\n",
            "Epoch: 027, Train Acc: 74.7027, Train Loss: 0.5173, Valid Loss: 0.5631, Test Acc: 74.4807\n",
            "Epoch: 028, Train Acc: 74.2162, Train Loss: 0.5140, Valid Loss: 0.5417, Test Acc: 72.1068\n",
            "Epoch: 029, Train Acc: 75.2973, Train Loss: 0.5063, Valid Loss: 0.5481, Test Acc: 74.4807\n",
            "Epoch: 030, Train Acc: 75.3784, Train Loss: 0.5125, Valid Loss: 0.5330, Test Acc: 72.7003\n",
            "Epoch: 031, Train Acc: 75.7568, Train Loss: 0.5053, Valid Loss: 0.5529, Test Acc: 75.0742\n",
            "Epoch: 032, Train Acc: 75.1081, Train Loss: 0.4949, Valid Loss: 0.5260, Test Acc: 73.5905\n",
            "Epoch: 033, Train Acc: 75.7568, Train Loss: 0.5049, Valid Loss: 0.5482, Test Acc: 75.0742\n",
            "Epoch: 034, Train Acc: 75.7568, Train Loss: 0.5116, Valid Loss: 0.5335, Test Acc: 75.3709\n",
            "Epoch: 035, Train Acc: 75.4595, Train Loss: 0.4952, Valid Loss: 0.5243, Test Acc: 74.4807\n",
            "Epoch: 036, Train Acc: 77.0000, Train Loss: 0.4856, Valid Loss: 0.5231, Test Acc: 76.5579\n",
            "Epoch: 037, Train Acc: 75.8108, Train Loss: 0.4952, Valid Loss: 0.5283, Test Acc: 74.1840\n",
            "Epoch: 038, Train Acc: 76.2162, Train Loss: 0.4945, Valid Loss: 0.5293, Test Acc: 72.9970\n",
            "Epoch: 039, Train Acc: 76.2432, Train Loss: 0.4899, Valid Loss: 0.5268, Test Acc: 76.2611\n",
            "Epoch: 040, Train Acc: 77.0270, Train Loss: 0.4861, Valid Loss: 0.5156, Test Acc: 75.0742\n",
            "Epoch: 041, Train Acc: 76.5676, Train Loss: 0.4869, Valid Loss: 0.5416, Test Acc: 78.0415\n",
            "Epoch: 042, Train Acc: 77.1622, Train Loss: 0.4851, Valid Loss: 0.5387, Test Acc: 75.6677\n",
            "Epoch: 043, Train Acc: 76.8919, Train Loss: 0.4853, Valid Loss: 0.5384, Test Acc: 76.2611\n",
            "Epoch: 044, Train Acc: 75.7568, Train Loss: 0.5061, Valid Loss: 0.5288, Test Acc: 71.8101\n",
            "Epoch: 045, Train Acc: 76.3243, Train Loss: 0.4935, Valid Loss: 0.5203, Test Acc: 71.5134\n",
            "Epoch: 046, Train Acc: 75.5946, Train Loss: 0.5011, Valid Loss: 0.5570, Test Acc: 73.8872\n",
            "Epoch: 047, Train Acc: 77.6486, Train Loss: 0.4793, Valid Loss: 0.5148, Test Acc: 75.3709\n",
            "Epoch: 048, Train Acc: 76.3243, Train Loss: 0.4968, Valid Loss: 0.5196, Test Acc: 71.8101\n",
            "Epoch: 049, Train Acc: 77.0270, Train Loss: 0.4833, Valid Loss: 0.5176, Test Acc: 74.4807\n",
            "Epoch: 050, Train Acc: 76.6757, Train Loss: 0.4830, Valid Loss: 0.5578, Test Acc: 72.9970\n",
            "Epoch: 051, Train Acc: 76.0541, Train Loss: 0.4900, Valid Loss: 0.5358, Test Acc: 72.4036\n",
            "Epoch: 052, Train Acc: 77.1351, Train Loss: 0.4751, Valid Loss: 0.5100, Test Acc: 73.5905\n",
            "Epoch: 053, Train Acc: 77.1081, Train Loss: 0.4840, Valid Loss: 0.5140, Test Acc: 74.1840\n",
            "Epoch: 054, Train Acc: 75.9730, Train Loss: 0.4843, Valid Loss: 0.5223, Test Acc: 74.4807\n",
            "Epoch: 055, Train Acc: 77.4054, Train Loss: 0.4740, Valid Loss: 0.5077, Test Acc: 74.1840\n",
            "Epoch: 056, Train Acc: 77.6486, Train Loss: 0.4727, Valid Loss: 0.5066, Test Acc: 74.1840\n",
            "Epoch: 057, Train Acc: 75.2703, Train Loss: 0.4966, Valid Loss: 0.5533, Test Acc: 74.1840\n",
            "Epoch: 058, Train Acc: 77.1081, Train Loss: 0.4757, Valid Loss: 0.5032, Test Acc: 74.7774\n",
            "Epoch: 059, Train Acc: 77.7027, Train Loss: 0.4654, Valid Loss: 0.5162, Test Acc: 74.7774\n",
            "Epoch: 060, Train Acc: 78.0541, Train Loss: 0.4701, Valid Loss: 0.5106, Test Acc: 75.6677\n",
            "Epoch: 061, Train Acc: 76.5676, Train Loss: 0.4781, Valid Loss: 0.5341, Test Acc: 74.7774\n",
            "Epoch: 062, Train Acc: 77.6486, Train Loss: 0.4767, Valid Loss: 0.5336, Test Acc: 74.4807\n",
            "Epoch: 063, Train Acc: 78.1622, Train Loss: 0.4629, Valid Loss: 0.5048, Test Acc: 76.2611\n",
            "Epoch: 064, Train Acc: 77.7297, Train Loss: 0.4700, Valid Loss: 0.5102, Test Acc: 74.7774\n",
            "Epoch: 065, Train Acc: 79.1622, Train Loss: 0.4672, Valid Loss: 0.5133, Test Acc: 75.0742\n",
            "Epoch: 066, Train Acc: 78.7027, Train Loss: 0.4605, Valid Loss: 0.5041, Test Acc: 74.1840\n",
            "Epoch: 067, Train Acc: 78.1351, Train Loss: 0.4643, Valid Loss: 0.5097, Test Acc: 75.6677\n",
            "Epoch: 068, Train Acc: 76.6216, Train Loss: 0.4813, Valid Loss: 0.5728, Test Acc: 75.0742\n",
            "Epoch: 069, Train Acc: 77.3243, Train Loss: 0.4659, Valid Loss: 0.5103, Test Acc: 75.0742\n",
            "Epoch: 070, Train Acc: 77.9730, Train Loss: 0.4567, Valid Loss: 0.5110, Test Acc: 75.6677\n",
            "Epoch: 071, Train Acc: 77.5676, Train Loss: 0.4729, Valid Loss: 0.5138, Test Acc: 73.8872\n",
            "Epoch: 072, Train Acc: 77.3243, Train Loss: 0.4721, Valid Loss: 0.5303, Test Acc: 73.8872\n",
            "\n",
            "\n",
            "Best test accuracy:  78.04154302670622\n",
            "Best epoch:  41\n"
          ]
        }
      ],
      "source": [
        "#Model-specific arguments\n",
        "\n",
        "# Optimisation params\n",
        "args.lr=0.01\n",
        "args.weight_decay=0.0005\n",
        "\n",
        "# Model configuration\n",
        "args.d=1\n",
        "args.layers=4\n",
        "args.hidden_channels= 15\n",
        "args.input_dropout=0.0\n",
        "args.dropout=0.1\n",
        "args.orth = \"householder\"   #choices=['matrix_exp', 'cayley', 'householder', 'euler'], parametrization for the orthogonal group\n",
        "args.edges_feat = \"none\"    #choices=['none', 'concat', 'linear', 'bilinear']\n",
        "args.readout = \"concat\"     #choices=['mean', 'sum', 'max', 'sag', 'mlp', 'concat']\n",
        "\n",
        "\n",
        "config=args.__dict__\n",
        "print(config)\n",
        "\n",
        "model = DiscreteDiagSheafDiffusion(config)\n",
        "#model = DiscreteDiagPoolSheafDiffusion(config)\n",
        "#model = DiscreteBundleSheafDiffusion(config)\n",
        "model = model.to(args.device)\n",
        "\n",
        "sheaf_learner_params, other_params = model.grouped_parameters()\n",
        "\n",
        "# Optimizers \n",
        "optimizer = torch.optim.Adam([\n",
        "    {'params': sheaf_learner_params, 'weight_decay': config['sheaf_decay']},\n",
        "    {'params': other_params, 'weight_decay': config['weight_decay']}\n",
        "], lr=config['lr'])\n",
        "\n",
        "\n",
        "# Training and test criterions\n",
        "criterion_train = torch.nn.CrossEntropyLoss()\n",
        "# In order not to make test loss change with batch_size \n",
        "criterion_test = torch.nn.CrossEntropyLoss(reduction = 'sum')\n",
        "\n",
        "best_test_acc = 0\n",
        "best_epoch = 0\n",
        "# Initial best loss value, used for the early stopping technique\n",
        "best_loss = 100.00\n",
        "patience = args.patience\n",
        "\n",
        "for epoch in range(1, args.epochs):\n",
        "    # Training epoch\n",
        "    train(model, optimizer, criterion_train, train_loader, args.device)\n",
        "\n",
        "\n",
        "    # Compute training, validation and test accuracy\n",
        "    train_acc, train_loss = test(model, criterion_test, train_loader, args.device)\n",
        "    valid_acc, valid_loss = test(model, criterion_test, valid_loader, args.device)\n",
        "    test_acc, test_loss = test(model, criterion_test, test_loader, args.device)\n",
        "\n",
        "    if best_test_acc < test_acc:\n",
        "        best_test_acc = test_acc\n",
        "        best_epoch = epoch\n",
        "\n",
        "    if valid_loss < best_loss:\n",
        "        best_loss = valid_loss\n",
        "        patience = args.patience\n",
        "    else:\n",
        "        patience -= 1\n",
        "        \n",
        "    if patience <= 0: \n",
        "        # Early stopping with patience\n",
        "        break \n",
        "\n",
        "    print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Train Loss: {train_loss:.4f}, Valid Loss: {valid_loss:.4f}, Test Acc: {test_acc:.4f}')\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"Best test accuracy: \", best_test_acc)\n",
        "print(\"Best epoch: \", best_epoch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXwNcDOFCZPv"
      },
      "source": [
        "#### Diagonal sheaf diffusion, **[mean, sum, max]** readout "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ZfVmN4JLBbP",
        "outputId": "f514646a-30df-456b-b95c-3e0a6a3f71dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epochs': 200, 'lr': 0.01, 'weight_decay': 0.0005, 'sheaf_decay': 0.0005, 'patience': 15, 'second_linear': False, 'd': 3, 'layers': 4, 'normalised': True, 'deg_normalised': False, 'linear': False, 'hidden_channels': 15, 'input_dropout': 0.0, 'dropout': 0.1, 'left_weights': True, 'right_weights': True, 'add_lp': False, 'add_hp': False, 'use_act': True, 'sheaf_act': 'tanh', 'edge_weights': True, 'orth': 'householder', 'edges_feat': 'none', 'readout': 'concat', 'dense_intermediate_dim': 256, 'dense_output_graph_dim': 128, 'output_nn_intermediate_dim': 64, 'set_transformer_k': 8, 'input_dim': 14, 'output_dim': 2, 'input_dim_edge': 3, 'max_num_nodes_in_graph': 417, 'device': device(type='cuda', index=0)}\n",
            "Epoch: 001, Train Acc: 55.1081, Train Loss: 0.6889, Valid Loss: 0.6746, Test Acc: 56.3798\n",
            "Epoch: 002, Train Acc: 65.3243, Train Loss: 0.6118, Valid Loss: 0.6181, Test Acc: 63.7982\n",
            "Epoch: 003, Train Acc: 68.3514, Train Loss: 0.5799, Valid Loss: 0.5827, Test Acc: 67.0623\n",
            "Epoch: 004, Train Acc: 67.5676, Train Loss: 0.5750, Valid Loss: 0.5835, Test Acc: 68.2493\n",
            "Epoch: 005, Train Acc: 67.8649, Train Loss: 0.5729, Valid Loss: 0.5862, Test Acc: 65.2819\n",
            "Epoch: 006, Train Acc: 70.7568, Train Loss: 0.5646, Valid Loss: 0.5960, Test Acc: 69.7329\n",
            "Epoch: 007, Train Acc: 70.8649, Train Loss: 0.5563, Valid Loss: 0.5778, Test Acc: 67.9525\n",
            "Epoch: 008, Train Acc: 69.1351, Train Loss: 0.5601, Valid Loss: 0.5702, Test Acc: 67.0623\n",
            "Epoch: 009, Train Acc: 71.8919, Train Loss: 0.5383, Valid Loss: 0.5721, Test Acc: 70.9199\n",
            "Epoch: 010, Train Acc: 73.5135, Train Loss: 0.5349, Valid Loss: 0.5541, Test Acc: 70.6231\n",
            "Epoch: 011, Train Acc: 74.2703, Train Loss: 0.5215, Valid Loss: 0.5404, Test Acc: 70.6231\n",
            "Epoch: 012, Train Acc: 73.3784, Train Loss: 0.5338, Valid Loss: 0.5556, Test Acc: 69.1395\n",
            "Epoch: 013, Train Acc: 74.0541, Train Loss: 0.5204, Valid Loss: 0.5654, Test Acc: 71.8101\n",
            "Epoch: 014, Train Acc: 75.4595, Train Loss: 0.5086, Valid Loss: 0.5454, Test Acc: 74.1840\n",
            "Epoch: 015, Train Acc: 76.5676, Train Loss: 0.5001, Valid Loss: 0.5304, Test Acc: 72.4036\n",
            "Epoch: 016, Train Acc: 76.4324, Train Loss: 0.4925, Valid Loss: 0.5297, Test Acc: 73.5905\n",
            "Epoch: 017, Train Acc: 75.9730, Train Loss: 0.5024, Valid Loss: 0.5337, Test Acc: 72.4036\n",
            "Epoch: 018, Train Acc: 75.8649, Train Loss: 0.5027, Valid Loss: 0.5374, Test Acc: 70.0297\n",
            "Epoch: 019, Train Acc: 77.0541, Train Loss: 0.4893, Valid Loss: 0.5485, Test Acc: 74.4807\n",
            "Epoch: 020, Train Acc: 77.2973, Train Loss: 0.4786, Valid Loss: 0.5341, Test Acc: 75.0742\n",
            "Epoch: 021, Train Acc: 75.7027, Train Loss: 0.5005, Valid Loss: 0.5489, Test Acc: 74.1840\n",
            "Epoch: 022, Train Acc: 77.2703, Train Loss: 0.4918, Valid Loss: 0.5362, Test Acc: 75.9644\n",
            "Epoch: 023, Train Acc: 77.2703, Train Loss: 0.4898, Valid Loss: 0.5258, Test Acc: 75.3709\n",
            "Epoch: 024, Train Acc: 77.9730, Train Loss: 0.4780, Valid Loss: 0.5033, Test Acc: 73.2938\n",
            "Epoch: 025, Train Acc: 77.0541, Train Loss: 0.4850, Valid Loss: 0.5028, Test Acc: 72.4036\n",
            "Epoch: 026, Train Acc: 79.0270, Train Loss: 0.4640, Valid Loss: 0.5205, Test Acc: 76.5579\n",
            "Epoch: 027, Train Acc: 78.7838, Train Loss: 0.4637, Valid Loss: 0.4928, Test Acc: 74.7774\n",
            "Epoch: 028, Train Acc: 78.8919, Train Loss: 0.4616, Valid Loss: 0.5171, Test Acc: 76.8546\n",
            "Epoch: 029, Train Acc: 77.7568, Train Loss: 0.4840, Valid Loss: 0.5638, Test Acc: 75.0742\n",
            "Epoch: 030, Train Acc: 78.0000, Train Loss: 0.4718, Valid Loss: 0.5201, Test Acc: 73.8872\n",
            "Epoch: 031, Train Acc: 78.7838, Train Loss: 0.4570, Valid Loss: 0.4991, Test Acc: 75.0742\n",
            "Epoch: 032, Train Acc: 78.9730, Train Loss: 0.4566, Valid Loss: 0.5014, Test Acc: 75.6677\n",
            "Epoch: 033, Train Acc: 78.6486, Train Loss: 0.4658, Valid Loss: 0.5068, Test Acc: 74.7774\n",
            "Epoch: 034, Train Acc: 79.6757, Train Loss: 0.4533, Valid Loss: 0.5097, Test Acc: 75.9644\n",
            "Epoch: 035, Train Acc: 78.5135, Train Loss: 0.4592, Valid Loss: 0.4930, Test Acc: 72.9970\n",
            "Epoch: 036, Train Acc: 78.3243, Train Loss: 0.4576, Valid Loss: 0.5236, Test Acc: 76.5579\n",
            "Epoch: 037, Train Acc: 79.7838, Train Loss: 0.4468, Valid Loss: 0.4886, Test Acc: 75.6677\n",
            "Epoch: 038, Train Acc: 79.0811, Train Loss: 0.4607, Valid Loss: 0.5307, Test Acc: 74.1840\n",
            "Epoch: 039, Train Acc: 79.7568, Train Loss: 0.4475, Valid Loss: 0.4951, Test Acc: 77.7448\n",
            "Epoch: 040, Train Acc: 80.0541, Train Loss: 0.4414, Valid Loss: 0.5057, Test Acc: 74.4807\n",
            "Epoch: 041, Train Acc: 78.8649, Train Loss: 0.4683, Valid Loss: 0.5020, Test Acc: 75.3709\n",
            "Epoch: 042, Train Acc: 77.8378, Train Loss: 0.4629, Valid Loss: 0.5103, Test Acc: 74.1840\n",
            "Epoch: 043, Train Acc: 79.1622, Train Loss: 0.4526, Valid Loss: 0.4936, Test Acc: 74.7774\n",
            "Epoch: 044, Train Acc: 80.0000, Train Loss: 0.4425, Valid Loss: 0.4994, Test Acc: 76.2611\n",
            "Epoch: 045, Train Acc: 79.6216, Train Loss: 0.4598, Valid Loss: 0.4995, Test Acc: 74.4807\n",
            "Epoch: 046, Train Acc: 77.9730, Train Loss: 0.4661, Valid Loss: 0.5370, Test Acc: 72.7003\n",
            "Epoch: 047, Train Acc: 79.9459, Train Loss: 0.4343, Valid Loss: 0.4918, Test Acc: 74.4807\n",
            "Epoch: 048, Train Acc: 80.6216, Train Loss: 0.4354, Valid Loss: 0.4825, Test Acc: 75.6677\n",
            "Epoch: 049, Train Acc: 79.9189, Train Loss: 0.4431, Valid Loss: 0.5155, Test Acc: 75.6677\n",
            "Epoch: 050, Train Acc: 80.0541, Train Loss: 0.4338, Valid Loss: 0.4858, Test Acc: 75.0742\n",
            "Epoch: 051, Train Acc: 79.3514, Train Loss: 0.4556, Valid Loss: 0.4940, Test Acc: 73.5905\n",
            "Epoch: 052, Train Acc: 79.0811, Train Loss: 0.4671, Valid Loss: 0.5195, Test Acc: 74.7774\n",
            "Epoch: 053, Train Acc: 79.3514, Train Loss: 0.4500, Valid Loss: 0.5380, Test Acc: 74.4807\n",
            "Epoch: 054, Train Acc: 80.6216, Train Loss: 0.4303, Valid Loss: 0.4827, Test Acc: 74.7774\n",
            "Epoch: 055, Train Acc: 79.4865, Train Loss: 0.4429, Valid Loss: 0.5043, Test Acc: 73.2938\n",
            "Epoch: 056, Train Acc: 80.3243, Train Loss: 0.4390, Valid Loss: 0.4958, Test Acc: 73.2938\n",
            "Epoch: 057, Train Acc: 80.6757, Train Loss: 0.4258, Valid Loss: 0.4736, Test Acc: 75.6677\n",
            "Epoch: 058, Train Acc: 80.4865, Train Loss: 0.4317, Valid Loss: 0.4749, Test Acc: 75.3709\n",
            "Epoch: 059, Train Acc: 80.3784, Train Loss: 0.4340, Valid Loss: 0.4876, Test Acc: 75.6677\n",
            "Epoch: 060, Train Acc: 80.2973, Train Loss: 0.4349, Valid Loss: 0.4764, Test Acc: 75.9644\n",
            "Epoch: 061, Train Acc: 79.2432, Train Loss: 0.4476, Valid Loss: 0.5266, Test Acc: 75.0742\n",
            "Epoch: 062, Train Acc: 80.2703, Train Loss: 0.4326, Valid Loss: 0.4915, Test Acc: 76.2611\n",
            "Epoch: 063, Train Acc: 79.1351, Train Loss: 0.4543, Valid Loss: 0.4911, Test Acc: 73.8872\n",
            "Epoch: 064, Train Acc: 80.3514, Train Loss: 0.4385, Valid Loss: 0.4830, Test Acc: 75.0742\n",
            "Epoch: 065, Train Acc: 80.9459, Train Loss: 0.4247, Valid Loss: 0.4980, Test Acc: 78.0415\n",
            "Epoch: 066, Train Acc: 78.6216, Train Loss: 0.4617, Valid Loss: 0.4962, Test Acc: 73.8872\n",
            "Epoch: 067, Train Acc: 79.5676, Train Loss: 0.4429, Valid Loss: 0.4773, Test Acc: 73.5905\n",
            "Epoch: 068, Train Acc: 81.0000, Train Loss: 0.4312, Valid Loss: 0.4857, Test Acc: 75.3709\n",
            "Epoch: 069, Train Acc: 80.6757, Train Loss: 0.4248, Valid Loss: 0.4740, Test Acc: 75.0742\n",
            "Epoch: 070, Train Acc: 80.5946, Train Loss: 0.4350, Valid Loss: 0.5213, Test Acc: 75.0742\n",
            "Epoch: 071, Train Acc: 81.2162, Train Loss: 0.4258, Valid Loss: 0.4779, Test Acc: 75.3709\n",
            "\n",
            "\n",
            "Best test accuracy:  78.04154302670622\n",
            "Best epoch:  65\n"
          ]
        }
      ],
      "source": [
        "#Model-specific arguments\n",
        "\n",
        "# Optimisation params\n",
        "args.lr=0.01\n",
        "args.weight_decay=0.0005\n",
        "\n",
        "# Model configuration\n",
        "args.d=3\n",
        "args.layers=4\n",
        "args.hidden_channels= 15\n",
        "args.input_dropout=0.0\n",
        "args.dropout=0.1\n",
        "args.orth = \"householder\"   #choices=['matrix_exp', 'cayley', 'householder', 'euler'], parametrization for the orthogonal group\n",
        "args.edges_feat = \"none\"    #choices=['none', 'concat', 'linear', 'bilinear']\n",
        "args.readout = \"concat\"     #choices=['mean', 'sum', 'max', 'sag', 'mlp', 'concat']\n",
        "\n",
        "\n",
        "config=args.__dict__\n",
        "print(config)\n",
        "\n",
        "model = DiscreteDiagSheafDiffusion(config)\n",
        "#model = DiscreteDiagPoolSheafDiffusion(config)\n",
        "#model = DiscreteBundleSheafDiffusion(config)\n",
        "model = model.to(args.device)\n",
        "\n",
        "sheaf_learner_params, other_params = model.grouped_parameters()\n",
        "\n",
        "# Optimizers \n",
        "optimizer = torch.optim.Adam([\n",
        "    {'params': sheaf_learner_params, 'weight_decay': config['sheaf_decay']},\n",
        "    {'params': other_params, 'weight_decay': config['weight_decay']}\n",
        "], lr=config['lr'])\n",
        "\n",
        "\n",
        "# Training and test criterions\n",
        "criterion_train = torch.nn.CrossEntropyLoss()\n",
        "# In order not to make test loss change with batch_size \n",
        "criterion_test = torch.nn.CrossEntropyLoss(reduction = 'sum')\n",
        "\n",
        "best_test_acc = 0\n",
        "best_epoch = 0\n",
        "# Initial best loss value, used for the early stopping technique\n",
        "best_loss = 100.00\n",
        "patience = args.patience\n",
        "\n",
        "for epoch in range(1, args.epochs):\n",
        "    # Training epoch\n",
        "    train(model, optimizer, criterion_train, train_loader, args.device)\n",
        "\n",
        "\n",
        "    # Compute training, validation and test accuracy\n",
        "    train_acc, train_loss = test(model, criterion_test, train_loader, args.device)\n",
        "    valid_acc, valid_loss = test(model, criterion_test, valid_loader, args.device)\n",
        "    test_acc, test_loss = test(model, criterion_test, test_loader, args.device)\n",
        "\n",
        "    if best_test_acc < test_acc:\n",
        "        best_test_acc = test_acc\n",
        "        best_epoch = epoch\n",
        "\n",
        "    if valid_loss < best_loss:\n",
        "        best_loss = valid_loss\n",
        "        patience = args.patience\n",
        "    else:\n",
        "        patience -= 1\n",
        "        \n",
        "    if patience <= 0: \n",
        "        # Early stopping with patience\n",
        "        break \n",
        "\n",
        "    print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Train Loss: {train_loss:.4f}, Valid Loss: {valid_loss:.4f}, Test Acc: {test_acc:.4f}')\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"Best test accuracy: \", best_test_acc)\n",
        "print(\"Best epoch: \", best_epoch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dM4eOqpHCZPy"
      },
      "source": [
        "#### **Bundle** sheaf diffusion, **[mean, sum, max]** readout "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u6SJsJ91CZP0",
        "outputId": "a6a9eaf4-0075-4318-a563-3b2c1cff3eea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epochs': 200, 'lr': 0.01, 'weight_decay': 0.0005, 'sheaf_decay': 0.0005, 'patience': 15, 'second_linear': False, 'd': 2, 'layers': 4, 'normalised': True, 'deg_normalised': False, 'linear': False, 'hidden_channels': 15, 'input_dropout': 0.0, 'dropout': 0.1, 'left_weights': True, 'right_weights': True, 'add_lp': False, 'add_hp': False, 'use_act': True, 'sheaf_act': 'tanh', 'edge_weights': True, 'orth': 'householder', 'edges_feat': 'none', 'readout': 'concat', 'dense_intermediate_dim': 256, 'dense_output_graph_dim': 128, 'output_nn_intermediate_dim': 64, 'set_transformer_k': 8, 'input_dim': 14, 'output_dim': 2, 'input_dim_edge': 3, 'max_num_nodes_in_graph': 417, 'device': device(type='cuda', index=0)}\n",
            "Epoch: 001, Train Acc: 62.8378, Train Loss: 0.6536, Valid Loss: 0.6572, Test Acc: 59.9407\n",
            "Epoch: 002, Train Acc: 64.6486, Train Loss: 0.6216, Valid Loss: 0.6187, Test Acc: 63.2047\n",
            "Epoch: 003, Train Acc: 65.8649, Train Loss: 0.5960, Valid Loss: 0.5994, Test Acc: 64.0950\n",
            "Epoch: 004, Train Acc: 66.8649, Train Loss: 0.5887, Valid Loss: 0.5856, Test Acc: 65.5786\n",
            "Epoch: 005, Train Acc: 68.0811, Train Loss: 0.5865, Valid Loss: 0.5847, Test Acc: 66.4688\n",
            "Epoch: 006, Train Acc: 68.0270, Train Loss: 0.6019, Valid Loss: 0.6121, Test Acc: 66.7656\n",
            "Epoch: 007, Train Acc: 66.2162, Train Loss: 0.5929, Valid Loss: 0.5866, Test Acc: 64.0950\n",
            "Epoch: 008, Train Acc: 68.1351, Train Loss: 0.5764, Valid Loss: 0.5784, Test Acc: 64.3917\n",
            "Epoch: 009, Train Acc: 69.0811, Train Loss: 0.5681, Valid Loss: 0.5677, Test Acc: 66.7656\n",
            "Epoch: 010, Train Acc: 69.3243, Train Loss: 0.5543, Valid Loss: 0.5529, Test Acc: 65.5786\n",
            "Epoch: 011, Train Acc: 72.4865, Train Loss: 0.5336, Valid Loss: 0.5496, Test Acc: 67.9525\n",
            "Epoch: 012, Train Acc: 72.8108, Train Loss: 0.5347, Valid Loss: 0.5542, Test Acc: 69.4362\n",
            "Epoch: 013, Train Acc: 72.2973, Train Loss: 0.5462, Valid Loss: 0.5724, Test Acc: 70.0297\n",
            "Epoch: 014, Train Acc: 75.2703, Train Loss: 0.5208, Valid Loss: 0.5348, Test Acc: 71.2166\n",
            "Epoch: 015, Train Acc: 75.8378, Train Loss: 0.5096, Valid Loss: 0.5286, Test Acc: 73.5905\n",
            "Epoch: 016, Train Acc: 75.0270, Train Loss: 0.5173, Valid Loss: 0.5285, Test Acc: 74.1840\n",
            "Epoch: 017, Train Acc: 75.5405, Train Loss: 0.5120, Valid Loss: 0.5486, Test Acc: 74.4807\n",
            "Epoch: 018, Train Acc: 75.2703, Train Loss: 0.5105, Valid Loss: 0.5443, Test Acc: 73.8872\n",
            "Epoch: 019, Train Acc: 76.2703, Train Loss: 0.5042, Valid Loss: 0.5439, Test Acc: 75.6677\n",
            "Epoch: 020, Train Acc: 75.6757, Train Loss: 0.5057, Valid Loss: 0.5400, Test Acc: 75.6677\n",
            "Epoch: 021, Train Acc: 76.1622, Train Loss: 0.5039, Valid Loss: 0.5318, Test Acc: 75.3709\n",
            "Epoch: 022, Train Acc: 76.3784, Train Loss: 0.4957, Valid Loss: 0.5335, Test Acc: 74.4807\n",
            "Epoch: 023, Train Acc: 74.8378, Train Loss: 0.5118, Valid Loss: 0.5262, Test Acc: 73.2938\n",
            "Epoch: 024, Train Acc: 76.5405, Train Loss: 0.4910, Valid Loss: 0.5243, Test Acc: 75.6677\n",
            "Epoch: 025, Train Acc: 76.4865, Train Loss: 0.4919, Valid Loss: 0.5226, Test Acc: 73.8872\n",
            "Epoch: 026, Train Acc: 76.2703, Train Loss: 0.4903, Valid Loss: 0.5238, Test Acc: 73.2938\n",
            "Epoch: 027, Train Acc: 76.1892, Train Loss: 0.4924, Valid Loss: 0.5133, Test Acc: 72.4036\n",
            "Epoch: 028, Train Acc: 77.3784, Train Loss: 0.4861, Valid Loss: 0.5229, Test Acc: 76.8546\n",
            "Epoch: 029, Train Acc: 77.3243, Train Loss: 0.4779, Valid Loss: 0.5148, Test Acc: 74.7774\n",
            "Epoch: 030, Train Acc: 75.1622, Train Loss: 0.5055, Valid Loss: 0.5137, Test Acc: 70.6231\n",
            "Epoch: 031, Train Acc: 77.1892, Train Loss: 0.4820, Valid Loss: 0.5054, Test Acc: 75.0742\n",
            "Epoch: 032, Train Acc: 76.1351, Train Loss: 0.5013, Valid Loss: 0.5279, Test Acc: 72.4036\n",
            "Epoch: 033, Train Acc: 76.1622, Train Loss: 0.4909, Valid Loss: 0.5381, Test Acc: 75.3709\n",
            "Epoch: 034, Train Acc: 77.5135, Train Loss: 0.4812, Valid Loss: 0.5312, Test Acc: 74.7774\n",
            "Epoch: 035, Train Acc: 77.7027, Train Loss: 0.4709, Valid Loss: 0.5086, Test Acc: 75.0742\n",
            "Epoch: 036, Train Acc: 77.5946, Train Loss: 0.4815, Valid Loss: 0.5192, Test Acc: 74.4807\n",
            "Epoch: 037, Train Acc: 76.9459, Train Loss: 0.4806, Valid Loss: 0.5107, Test Acc: 73.5905\n",
            "Epoch: 038, Train Acc: 77.3514, Train Loss: 0.4887, Valid Loss: 0.5262, Test Acc: 73.2938\n",
            "Epoch: 039, Train Acc: 76.5135, Train Loss: 0.4880, Valid Loss: 0.5023, Test Acc: 72.9970\n",
            "Epoch: 040, Train Acc: 78.0811, Train Loss: 0.4692, Valid Loss: 0.5150, Test Acc: 74.7774\n",
            "Epoch: 041, Train Acc: 77.8919, Train Loss: 0.4801, Valid Loss: 0.5231, Test Acc: 73.2938\n",
            "Epoch: 042, Train Acc: 78.5135, Train Loss: 0.4688, Valid Loss: 0.5222, Test Acc: 75.6677\n",
            "Epoch: 043, Train Acc: 78.7027, Train Loss: 0.4652, Valid Loss: 0.5134, Test Acc: 75.3709\n",
            "Epoch: 044, Train Acc: 77.4054, Train Loss: 0.4797, Valid Loss: 0.5234, Test Acc: 75.9644\n",
            "Epoch: 045, Train Acc: 78.0000, Train Loss: 0.4653, Valid Loss: 0.5163, Test Acc: 75.3709\n",
            "Epoch: 046, Train Acc: 78.2973, Train Loss: 0.4668, Valid Loss: 0.5187, Test Acc: 75.3709\n",
            "Epoch: 047, Train Acc: 78.0541, Train Loss: 0.4759, Valid Loss: 0.5112, Test Acc: 73.5905\n",
            "Epoch: 048, Train Acc: 77.0000, Train Loss: 0.4744, Valid Loss: 0.5320, Test Acc: 75.0742\n",
            "Epoch: 049, Train Acc: 78.6216, Train Loss: 0.4732, Valid Loss: 0.5110, Test Acc: 75.3709\n",
            "Epoch: 050, Train Acc: 77.7838, Train Loss: 0.4722, Valid Loss: 0.5043, Test Acc: 73.8872\n",
            "Epoch: 051, Train Acc: 78.8108, Train Loss: 0.4664, Valid Loss: 0.5121, Test Acc: 75.3709\n",
            "Epoch: 052, Train Acc: 77.5405, Train Loss: 0.4770, Valid Loss: 0.5313, Test Acc: 76.8546\n",
            "Epoch: 053, Train Acc: 76.8919, Train Loss: 0.4770, Valid Loss: 0.5139, Test Acc: 72.9970\n",
            "\n",
            "\n",
            "Best test accuracy:  76.8545994065282\n",
            "Best epoch:  28\n"
          ]
        }
      ],
      "source": [
        "#Model-specific arguments\n",
        "\n",
        "# Optimisation params\n",
        "args.lr=0.01\n",
        "args.weight_decay=0.0005\n",
        "\n",
        "# Model configuration\n",
        "args.d=3\n",
        "args.layers=4\n",
        "#args.hidden_channels= 15\n",
        "#args.input_dropout=0.0\n",
        "#args.dropout=0.1\n",
        "#args.orth = \"householder\"   #choices=['matrix_exp', 'cayley', 'householder', 'euler'], parametrization for the orthogonal group\n",
        "args.edges_feat = \"none\"    #choices=['none', 'concat', 'linear', 'bilinear']\n",
        "args.readout = \"concat\"       #choices=['mean', 'sum', 'max', 'sag', 'mlp', 'concat']\n",
        "\n",
        "\n",
        "config=args.__dict__\n",
        "print(config)\n",
        "\n",
        "#model = DiscreteDiagSheafDiffusion(config)\n",
        "#model = DiscreteDiagPoolSheafDiffusion(config)\n",
        "model = DiscreteBundleSheafDiffusion(config)\n",
        "model = model.to(args.device)\n",
        "\n",
        "sheaf_learner_params, other_params = model.grouped_parameters()\n",
        "\n",
        "# Optimizers \n",
        "optimizer = torch.optim.Adam([\n",
        "    {'params': sheaf_learner_params, 'weight_decay': config['sheaf_decay']},\n",
        "    {'params': other_params, 'weight_decay': config['weight_decay']}\n",
        "], lr=config['lr'])\n",
        "\n",
        "\n",
        "# Training and test criterions\n",
        "criterion_train = torch.nn.CrossEntropyLoss()\n",
        "# In order not to make test loss change with batch_size \n",
        "criterion_test = torch.nn.CrossEntropyLoss(reduction = 'sum')\n",
        "\n",
        "best_test_acc = 0\n",
        "best_epoch = 0\n",
        "# Initial best loss value, used for the early stopping technique\n",
        "best_loss = 100.00\n",
        "patience = args.patience\n",
        "\n",
        "for epoch in range(1, args.epochs):\n",
        "    # Training epoch\n",
        "    train(model, optimizer, criterion_train, train_loader, args.device)\n",
        "\n",
        "\n",
        "    # Compute training, validation and test accuracy\n",
        "    train_acc, train_loss = test(model, criterion_test, train_loader, args.device)\n",
        "    valid_acc, valid_loss = test(model, criterion_test, valid_loader, args.device)\n",
        "    test_acc, test_loss = test(model, criterion_test, test_loader, args.device)\n",
        "\n",
        "    if best_test_acc < test_acc:\n",
        "        best_test_acc = test_acc\n",
        "        best_epoch = epoch\n",
        "\n",
        "    if valid_loss < best_loss:\n",
        "        best_loss = valid_loss\n",
        "        patience = args.patience\n",
        "    else:\n",
        "        patience -= 1\n",
        "        \n",
        "    if patience <= 0: \n",
        "        # Early stopping with patience\n",
        "        break \n",
        "\n",
        "    print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Train Loss: {train_loss:.4f}, Valid Loss: {valid_loss:.4f}, Test Acc: {test_acc:.4f}')\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"Best test accuracy: \", best_test_acc)\n",
        "print(\"Best epoch: \", best_epoch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HekNT_rCU6Dh"
      },
      "source": [
        "#### Diagonal sheaf diffusion, **[mean, sum, max]** readout, edge features handling through **concatenation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i8KO_OQsU6Vx",
        "outputId": "a2f4acec-54ef-41a4-cf47-5ea0e0a9aa80"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epochs': 200, 'lr': 0.01, 'weight_decay': 0.0005, 'sheaf_decay': 0.0005, 'patience': 15, 'second_linear': False, 'd': 3, 'layers': 4, 'normalised': True, 'deg_normalised': False, 'linear': False, 'hidden_channels': 15, 'input_dropout': 0.0, 'dropout': 0.1, 'left_weights': True, 'right_weights': True, 'add_lp': False, 'add_hp': False, 'use_act': True, 'sheaf_act': 'tanh', 'edge_weights': True, 'orth': 'householder', 'edges_feat': 'concat', 'readout': 'concat', 'dense_intermediate_dim': 256, 'dense_output_graph_dim': 128, 'output_nn_intermediate_dim': 64, 'set_transformer_k': 8, 'input_dim': 14, 'output_dim': 2, 'input_dim_edge': 3, 'max_num_nodes_in_graph': 417, 'device': device(type='cuda', index=0)}\n",
            "Epoch: 001, Train Acc: 55.4595, Train Loss: 0.6728, Valid Loss: 0.6687, Test Acc: 56.3798\n",
            "Epoch: 002, Train Acc: 64.5405, Train Loss: 0.6191, Valid Loss: 0.6154, Test Acc: 62.3145\n",
            "Epoch: 003, Train Acc: 66.3514, Train Loss: 0.5968, Valid Loss: 0.6011, Test Acc: 64.9852\n",
            "Epoch: 004, Train Acc: 65.9730, Train Loss: 0.5964, Valid Loss: 0.5971, Test Acc: 62.6113\n",
            "Epoch: 005, Train Acc: 67.2162, Train Loss: 0.5783, Valid Loss: 0.5837, Test Acc: 64.9852\n",
            "Epoch: 006, Train Acc: 69.5135, Train Loss: 0.5701, Valid Loss: 0.5868, Test Acc: 66.7656\n",
            "Epoch: 007, Train Acc: 70.8649, Train Loss: 0.5474, Valid Loss: 0.5592, Test Acc: 67.0623\n",
            "Epoch: 008, Train Acc: 72.0811, Train Loss: 0.5324, Valid Loss: 0.5527, Test Acc: 69.4362\n",
            "Epoch: 009, Train Acc: 73.7027, Train Loss: 0.5191, Valid Loss: 0.5382, Test Acc: 69.7329\n",
            "Epoch: 010, Train Acc: 76.1081, Train Loss: 0.4997, Valid Loss: 0.5461, Test Acc: 73.2938\n",
            "Epoch: 011, Train Acc: 75.4865, Train Loss: 0.5094, Valid Loss: 0.5707, Test Acc: 74.4807\n",
            "Epoch: 012, Train Acc: 75.9459, Train Loss: 0.5061, Valid Loss: 0.5554, Test Acc: 75.9644\n",
            "Epoch: 013, Train Acc: 75.7568, Train Loss: 0.5000, Valid Loss: 0.5292, Test Acc: 70.3264\n",
            "Epoch: 014, Train Acc: 77.4865, Train Loss: 0.4880, Valid Loss: 0.5200, Test Acc: 73.5905\n",
            "Epoch: 015, Train Acc: 77.1622, Train Loss: 0.4859, Valid Loss: 0.5446, Test Acc: 73.8872\n",
            "Epoch: 016, Train Acc: 77.5135, Train Loss: 0.4747, Valid Loss: 0.5352, Test Acc: 75.0742\n",
            "Epoch: 017, Train Acc: 78.0541, Train Loss: 0.4741, Valid Loss: 0.5271, Test Acc: 73.5905\n",
            "Epoch: 018, Train Acc: 78.1351, Train Loss: 0.4710, Valid Loss: 0.5472, Test Acc: 75.9644\n",
            "Epoch: 019, Train Acc: 77.8378, Train Loss: 0.4682, Valid Loss: 0.5392, Test Acc: 76.2611\n",
            "Epoch: 020, Train Acc: 78.0541, Train Loss: 0.4749, Valid Loss: 0.5364, Test Acc: 74.4807\n",
            "Epoch: 021, Train Acc: 78.1351, Train Loss: 0.4769, Valid Loss: 0.5220, Test Acc: 74.1840\n",
            "Epoch: 022, Train Acc: 77.1892, Train Loss: 0.4775, Valid Loss: 0.5233, Test Acc: 73.8872\n",
            "Epoch: 023, Train Acc: 78.4865, Train Loss: 0.4589, Valid Loss: 0.5117, Test Acc: 76.5579\n",
            "Epoch: 024, Train Acc: 77.8108, Train Loss: 0.4659, Valid Loss: 0.5263, Test Acc: 73.5905\n",
            "Epoch: 025, Train Acc: 77.8649, Train Loss: 0.4720, Valid Loss: 0.5327, Test Acc: 72.4036\n",
            "Epoch: 026, Train Acc: 78.4595, Train Loss: 0.4631, Valid Loss: 0.5197, Test Acc: 75.0742\n",
            "Epoch: 027, Train Acc: 78.6216, Train Loss: 0.4646, Valid Loss: 0.5138, Test Acc: 74.1840\n",
            "Epoch: 028, Train Acc: 79.2162, Train Loss: 0.4568, Valid Loss: 0.5131, Test Acc: 76.5579\n",
            "Epoch: 029, Train Acc: 79.3243, Train Loss: 0.4554, Valid Loss: 0.5084, Test Acc: 77.1513\n",
            "Epoch: 030, Train Acc: 78.2973, Train Loss: 0.4704, Valid Loss: 0.5134, Test Acc: 74.7774\n",
            "Epoch: 031, Train Acc: 77.4054, Train Loss: 0.4893, Valid Loss: 0.5165, Test Acc: 72.1068\n",
            "Epoch: 032, Train Acc: 78.5676, Train Loss: 0.4643, Valid Loss: 0.5393, Test Acc: 76.2611\n",
            "Epoch: 033, Train Acc: 79.1622, Train Loss: 0.4563, Valid Loss: 0.5153, Test Acc: 74.4807\n",
            "Epoch: 034, Train Acc: 78.4595, Train Loss: 0.4650, Valid Loss: 0.5189, Test Acc: 76.2611\n",
            "Epoch: 035, Train Acc: 78.6216, Train Loss: 0.4706, Valid Loss: 0.5255, Test Acc: 75.0742\n",
            "Epoch: 036, Train Acc: 79.8378, Train Loss: 0.4447, Valid Loss: 0.5030, Test Acc: 73.5905\n",
            "Epoch: 037, Train Acc: 78.9730, Train Loss: 0.4549, Valid Loss: 0.5017, Test Acc: 75.9644\n",
            "Epoch: 038, Train Acc: 80.0000, Train Loss: 0.4425, Valid Loss: 0.5069, Test Acc: 75.9644\n",
            "Epoch: 039, Train Acc: 80.2432, Train Loss: 0.4486, Valid Loss: 0.4993, Test Acc: 75.0742\n",
            "Epoch: 040, Train Acc: 78.7838, Train Loss: 0.4630, Valid Loss: 0.5297, Test Acc: 73.2938\n",
            "Epoch: 041, Train Acc: 79.2703, Train Loss: 0.4566, Valid Loss: 0.4930, Test Acc: 73.2938\n",
            "Epoch: 042, Train Acc: 81.1622, Train Loss: 0.4331, Valid Loss: 0.4840, Test Acc: 76.2611\n",
            "Epoch: 043, Train Acc: 79.2703, Train Loss: 0.4487, Valid Loss: 0.5007, Test Acc: 75.0742\n",
            "Epoch: 044, Train Acc: 80.4865, Train Loss: 0.4410, Valid Loss: 0.4955, Test Acc: 75.0742\n",
            "Epoch: 045, Train Acc: 80.1622, Train Loss: 0.4453, Valid Loss: 0.4991, Test Acc: 74.7774\n",
            "Epoch: 046, Train Acc: 78.3784, Train Loss: 0.4643, Valid Loss: 0.5080, Test Acc: 74.4807\n",
            "Epoch: 047, Train Acc: 80.2162, Train Loss: 0.4425, Valid Loss: 0.5058, Test Acc: 74.1840\n",
            "Epoch: 048, Train Acc: 78.7568, Train Loss: 0.4646, Valid Loss: 0.5642, Test Acc: 72.9970\n",
            "Epoch: 049, Train Acc: 80.1622, Train Loss: 0.4388, Valid Loss: 0.4883, Test Acc: 76.2611\n",
            "Epoch: 050, Train Acc: 79.9189, Train Loss: 0.4422, Valid Loss: 0.4823, Test Acc: 75.3709\n",
            "Epoch: 051, Train Acc: 81.1351, Train Loss: 0.4325, Valid Loss: 0.4848, Test Acc: 76.2611\n",
            "Epoch: 052, Train Acc: 79.5946, Train Loss: 0.4450, Valid Loss: 0.5086, Test Acc: 75.6677\n",
            "Epoch: 053, Train Acc: 80.5946, Train Loss: 0.4352, Valid Loss: 0.4955, Test Acc: 75.0742\n",
            "Epoch: 054, Train Acc: 80.8378, Train Loss: 0.4318, Valid Loss: 0.5067, Test Acc: 73.2938\n",
            "Epoch: 055, Train Acc: 81.1351, Train Loss: 0.4231, Valid Loss: 0.4681, Test Acc: 75.3709\n",
            "Epoch: 056, Train Acc: 81.6757, Train Loss: 0.4177, Valid Loss: 0.4774, Test Acc: 74.7774\n",
            "Epoch: 057, Train Acc: 80.7838, Train Loss: 0.4321, Valid Loss: 0.5149, Test Acc: 76.5579\n",
            "Epoch: 058, Train Acc: 81.0811, Train Loss: 0.4264, Valid Loss: 0.4855, Test Acc: 75.0742\n",
            "Epoch: 059, Train Acc: 80.1892, Train Loss: 0.4342, Valid Loss: 0.4672, Test Acc: 74.7774\n",
            "Epoch: 060, Train Acc: 81.1622, Train Loss: 0.4226, Valid Loss: 0.4790, Test Acc: 75.0742\n",
            "Epoch: 061, Train Acc: 80.6486, Train Loss: 0.4274, Valid Loss: 0.4737, Test Acc: 75.3709\n",
            "Epoch: 062, Train Acc: 81.2703, Train Loss: 0.4295, Valid Loss: 0.5256, Test Acc: 76.8546\n",
            "Epoch: 063, Train Acc: 81.5676, Train Loss: 0.4298, Valid Loss: 0.5030, Test Acc: 75.6677\n",
            "Epoch: 064, Train Acc: 81.6216, Train Loss: 0.4203, Valid Loss: 0.4900, Test Acc: 74.1840\n",
            "Epoch: 065, Train Acc: 80.7297, Train Loss: 0.4290, Valid Loss: 0.4810, Test Acc: 77.1513\n",
            "Epoch: 066, Train Acc: 80.3784, Train Loss: 0.4441, Valid Loss: 0.5066, Test Acc: 75.3709\n",
            "Epoch: 067, Train Acc: 80.2703, Train Loss: 0.4379, Valid Loss: 0.5083, Test Acc: 74.7774\n",
            "Epoch: 068, Train Acc: 80.8108, Train Loss: 0.4304, Valid Loss: 0.4914, Test Acc: 77.7448\n",
            "Epoch: 069, Train Acc: 80.5135, Train Loss: 0.4332, Valid Loss: 0.5057, Test Acc: 75.3709\n",
            "Epoch: 070, Train Acc: 81.1892, Train Loss: 0.4306, Valid Loss: 0.5174, Test Acc: 75.9644\n",
            "Epoch: 071, Train Acc: 81.6757, Train Loss: 0.4209, Valid Loss: 0.4886, Test Acc: 75.9644\n",
            "Epoch: 072, Train Acc: 81.5405, Train Loss: 0.4247, Valid Loss: 0.4750, Test Acc: 75.6677\n",
            "Epoch: 073, Train Acc: 81.6757, Train Loss: 0.4222, Valid Loss: 0.4977, Test Acc: 76.8546\n",
            "\n",
            "\n",
            "Best test accuracy:  77.74480712166172\n",
            "Best epoch:  68\n"
          ]
        }
      ],
      "source": [
        "#Model-specific arguments\n",
        "\n",
        "# Optimisation params\n",
        "args.lr=0.01\n",
        "args.weight_decay=0.0005\n",
        "\n",
        "# Model configuration\n",
        "args.d=3\n",
        "args.layers=4\n",
        "args.hidden_channels= 15\n",
        "args.input_dropout=0.0\n",
        "args.dropout=0.1\n",
        "args.orth = \"householder\"   #choices=['matrix_exp', 'cayley', 'householder', 'euler'], parametrization for the orthogonal group\n",
        "args.edges_feat = \"concat\"    #choices=['none', 'concat', 'linear', 'bilinear']\n",
        "args.readout = \"concat\"     #choices=['mean', 'sum', 'max', 'sag', 'mlp', 'concat']\n",
        "\n",
        "\n",
        "config=args.__dict__\n",
        "print(config)\n",
        "\n",
        "model = DiscreteDiagSheafDiffusion(config)\n",
        "#model = DiscreteDiagPoolSheafDiffusion(config)\n",
        "#model = DiscreteBundleSheafDiffusion(config)\n",
        "model = model.to(args.device)\n",
        "\n",
        "sheaf_learner_params, other_params = model.grouped_parameters()\n",
        "\n",
        "# Optimizers \n",
        "optimizer = torch.optim.Adam([\n",
        "    {'params': sheaf_learner_params, 'weight_decay': config['sheaf_decay']},\n",
        "    {'params': other_params, 'weight_decay': config['weight_decay']}\n",
        "], lr=config['lr'])\n",
        "\n",
        "\n",
        "# Training and test criterions\n",
        "criterion_train = torch.nn.CrossEntropyLoss()\n",
        "# In order not to make test loss change with batch_size \n",
        "criterion_test = torch.nn.CrossEntropyLoss(reduction = 'sum')\n",
        "\n",
        "best_test_acc = 0\n",
        "best_epoch = 0\n",
        "# Initial best loss value, used for the early stopping technique\n",
        "best_loss = 100.00\n",
        "patience = args.patience\n",
        "\n",
        "for epoch in range(1, args.epochs):\n",
        "    # Training epoch\n",
        "    train(model, optimizer, criterion_train, train_loader, args.device)\n",
        "\n",
        "\n",
        "    # Compute training, validation and test accuracy\n",
        "    train_acc, train_loss = test(model, criterion_test, train_loader, args.device)\n",
        "    valid_acc, valid_loss = test(model, criterion_test, valid_loader, args.device)\n",
        "    test_acc, test_loss = test(model, criterion_test, test_loader, args.device)\n",
        "\n",
        "    if best_test_acc < test_acc:\n",
        "        best_test_acc = test_acc\n",
        "        best_epoch = epoch\n",
        "\n",
        "    if valid_loss < best_loss:\n",
        "        best_loss = valid_loss\n",
        "        patience = args.patience\n",
        "    else:\n",
        "        patience -= 1\n",
        "        \n",
        "    if patience <= 0: \n",
        "        # Early stopping with patience\n",
        "        break \n",
        "\n",
        "    print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Train Loss: {train_loss:.4f}, Valid Loss: {valid_loss:.4f}, Test Acc: {test_acc:.4f}')\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"Best test accuracy: \", best_test_acc)\n",
        "print(\"Best epoch: \", best_epoch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05vESKWppM2M"
      },
      "source": [
        "#### Diagonal sheaf diffusion, **[mean, sum, max]** readout, edge features handling through **linear** transform\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R0cSVxk4pM2S",
        "outputId": "7870f450-cc79-4f64-d719-ffcea2ac5f5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epochs': 200, 'lr': 0.01, 'weight_decay': 0.0005, 'sheaf_decay': 0.0005, 'patience': 15, 'second_linear': False, 'd': 3, 'layers': 4, 'normalised': True, 'deg_normalised': False, 'linear': False, 'hidden_channels': 15, 'input_dropout': 0.0, 'dropout': 0.1, 'left_weights': True, 'right_weights': True, 'add_lp': False, 'add_hp': False, 'use_act': True, 'sheaf_act': 'tanh', 'edge_weights': True, 'orth': 'householder', 'edges_feat': 'linear', 'readout': 'concat', 'dense_intermediate_dim': 256, 'dense_output_graph_dim': 128, 'output_nn_intermediate_dim': 64, 'set_transformer_k': 8, 'input_dim': 14, 'output_dim': 2, 'input_dim_edge': 3, 'max_num_nodes_in_graph': 417, 'device': device(type='cuda', index=0)}\n",
            "Epoch: 001, Train Acc: 59.2703, Train Loss: 0.6804, Valid Loss: 0.6930, Test Acc: 55.7864\n",
            "Epoch: 002, Train Acc: 61.6757, Train Loss: 0.6428, Valid Loss: 0.6377, Test Acc: 61.4243\n",
            "Epoch: 003, Train Acc: 66.4054, Train Loss: 0.6033, Valid Loss: 0.6137, Test Acc: 62.9080\n",
            "Epoch: 004, Train Acc: 69.4595, Train Loss: 0.5812, Valid Loss: 0.5946, Test Acc: 64.9852\n",
            "Epoch: 005, Train Acc: 70.5135, Train Loss: 0.5603, Valid Loss: 0.5849, Test Acc: 66.4688\n",
            "Epoch: 006, Train Acc: 70.8378, Train Loss: 0.5481, Valid Loss: 0.5769, Test Acc: 67.0623\n",
            "Epoch: 007, Train Acc: 72.3243, Train Loss: 0.5349, Valid Loss: 0.5621, Test Acc: 69.4362\n",
            "Epoch: 008, Train Acc: 73.9459, Train Loss: 0.5158, Valid Loss: 0.5391, Test Acc: 69.4362\n",
            "Epoch: 009, Train Acc: 76.1622, Train Loss: 0.5036, Valid Loss: 0.5380, Test Acc: 71.2166\n",
            "Epoch: 010, Train Acc: 75.2703, Train Loss: 0.5089, Valid Loss: 0.5400, Test Acc: 72.4036\n",
            "Epoch: 011, Train Acc: 78.1892, Train Loss: 0.4953, Valid Loss: 0.5288, Test Acc: 74.4807\n",
            "Epoch: 012, Train Acc: 77.6757, Train Loss: 0.4870, Valid Loss: 0.5264, Test Acc: 72.9970\n",
            "Epoch: 013, Train Acc: 77.0000, Train Loss: 0.4916, Valid Loss: 0.5394, Test Acc: 74.1840\n",
            "Epoch: 014, Train Acc: 77.4324, Train Loss: 0.4847, Valid Loss: 0.5581, Test Acc: 75.0742\n",
            "Epoch: 015, Train Acc: 75.3514, Train Loss: 0.5077, Valid Loss: 0.5686, Test Acc: 72.9970\n",
            "Epoch: 016, Train Acc: 77.5405, Train Loss: 0.4900, Valid Loss: 0.5241, Test Acc: 72.4036\n",
            "Epoch: 017, Train Acc: 76.8649, Train Loss: 0.4842, Valid Loss: 0.5089, Test Acc: 70.6231\n",
            "Epoch: 018, Train Acc: 78.7568, Train Loss: 0.4660, Valid Loss: 0.5251, Test Acc: 75.9644\n",
            "Epoch: 019, Train Acc: 78.7838, Train Loss: 0.4716, Valid Loss: 0.5403, Test Acc: 76.8546\n",
            "Epoch: 020, Train Acc: 74.6486, Train Loss: 0.5109, Valid Loss: 0.5715, Test Acc: 72.7003\n",
            "Epoch: 021, Train Acc: 77.1892, Train Loss: 0.4877, Valid Loss: 0.5542, Test Acc: 74.7774\n",
            "Epoch: 022, Train Acc: 79.3243, Train Loss: 0.4630, Valid Loss: 0.5114, Test Acc: 74.7774\n",
            "Epoch: 023, Train Acc: 78.9730, Train Loss: 0.4598, Valid Loss: 0.5104, Test Acc: 76.8546\n",
            "Epoch: 024, Train Acc: 77.9459, Train Loss: 0.4747, Valid Loss: 0.5226, Test Acc: 73.8872\n",
            "Epoch: 025, Train Acc: 77.4324, Train Loss: 0.4882, Valid Loss: 0.5269, Test Acc: 74.4807\n",
            "Epoch: 026, Train Acc: 77.5946, Train Loss: 0.4782, Valid Loss: 0.5269, Test Acc: 75.0742\n",
            "Epoch: 027, Train Acc: 79.1081, Train Loss: 0.4654, Valid Loss: 0.5128, Test Acc: 76.5579\n",
            "Epoch: 028, Train Acc: 79.2973, Train Loss: 0.4641, Valid Loss: 0.5270, Test Acc: 75.0742\n",
            "Epoch: 029, Train Acc: 78.5676, Train Loss: 0.4703, Valid Loss: 0.5320, Test Acc: 73.2938\n",
            "Epoch: 030, Train Acc: 79.3784, Train Loss: 0.4571, Valid Loss: 0.5160, Test Acc: 75.9644\n",
            "Epoch: 031, Train Acc: 79.7838, Train Loss: 0.4498, Valid Loss: 0.5093, Test Acc: 75.9644\n",
            "\n",
            "\n",
            "Best test accuracy:  76.8545994065282\n",
            "Best epoch:  19\n"
          ]
        }
      ],
      "source": [
        "#Model-specific arguments\n",
        "\n",
        "# Optimisation params\n",
        "args.lr=0.01\n",
        "args.weight_decay=0.0005\n",
        "\n",
        "# Model configuration\n",
        "args.d=3\n",
        "args.layers=4\n",
        "args.hidden_channels= 15\n",
        "args.input_dropout=0.0\n",
        "args.dropout=0.1\n",
        "args.orth = \"householder\"   #choices=['matrix_exp', 'cayley', 'householder', 'euler'], parametrization for the orthogonal group\n",
        "args.edges_feat = \"linear\"    #choices=['none', 'concat', 'linear', 'bilinear']\n",
        "args.readout = \"concat\"     #choices=['mean', 'sum', 'max', 'sag', 'mlp', 'concat']\n",
        "\n",
        "\n",
        "config=args.__dict__\n",
        "print(config)\n",
        "\n",
        "model = DiscreteDiagSheafDiffusion(config)\n",
        "#model = DiscreteDiagPoolSheafDiffusion(config)\n",
        "#model = DiscreteBundleSheafDiffusion(config)\n",
        "model = model.to(args.device)\n",
        "\n",
        "sheaf_learner_params, other_params = model.grouped_parameters()\n",
        "\n",
        "# Optimizers \n",
        "optimizer = torch.optim.Adam([\n",
        "    {'params': sheaf_learner_params, 'weight_decay': config['sheaf_decay']},\n",
        "    {'params': other_params, 'weight_decay': config['weight_decay']}\n",
        "], lr=config['lr'])\n",
        "\n",
        "\n",
        "# Training and test criterions\n",
        "criterion_train = torch.nn.CrossEntropyLoss()\n",
        "# In order not to make test loss change with batch_size \n",
        "criterion_test = torch.nn.CrossEntropyLoss(reduction = 'sum')\n",
        "\n",
        "best_test_acc = 0\n",
        "best_epoch = 0\n",
        "# Initial best loss value, used for the early stopping technique\n",
        "best_loss = 100.00\n",
        "patience = args.patience\n",
        "\n",
        "for epoch in range(1, args.epochs):\n",
        "    # Training epoch\n",
        "    train(model, optimizer, criterion_train, train_loader, args.device)\n",
        "\n",
        "\n",
        "    # Compute training, validation and test accuracy\n",
        "    train_acc, train_loss = test(model, criterion_test, train_loader, args.device)\n",
        "    valid_acc, valid_loss = test(model, criterion_test, valid_loader, args.device)\n",
        "    test_acc, test_loss = test(model, criterion_test, test_loader, args.device)\n",
        "\n",
        "    if best_test_acc < test_acc:\n",
        "        best_test_acc = test_acc\n",
        "        best_epoch = epoch\n",
        "\n",
        "    if valid_loss < best_loss:\n",
        "        best_loss = valid_loss\n",
        "        patience = args.patience\n",
        "    else:\n",
        "        patience -= 1\n",
        "        \n",
        "    if patience <= 0: \n",
        "        # Early stopping with patience\n",
        "        break \n",
        "\n",
        "    print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Train Loss: {train_loss:.4f}, Valid Loss: {valid_loss:.4f}, Test Acc: {test_acc:.4f}')\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"Best test accuracy: \", best_test_acc)\n",
        "print(\"Best epoch: \", best_epoch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvQlpsB-pdjs"
      },
      "source": [
        "#### Diagonal sheaf diffusion, **[mean, sum, max]** readout, edge features handling through **bilinear** transform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kl-JNsXG3JVh",
        "outputId": "0ab422ce-6321-4126-96bc-4f9c8c74f674"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epochs': 200, 'lr': 0.01, 'weight_decay': 0.0005, 'sheaf_decay': 0.0005, 'patience': 15, 'second_linear': False, 'd': 3, 'layers': 4, 'normalised': True, 'deg_normalised': False, 'linear': False, 'hidden_channels': 15, 'input_dropout': 0.0, 'dropout': 0.0, 'left_weights': True, 'right_weights': True, 'add_lp': False, 'add_hp': False, 'use_act': True, 'sheaf_act': 'tanh', 'edge_weights': True, 'orth': 'householder', 'edges_feat': 'bilinear', 'readout': 'concat', 'dense_intermediate_dim': 256, 'dense_output_graph_dim': 128, 'output_nn_intermediate_dim': 64, 'set_transformer_k': 8, 'input_dim': 14, 'output_dim': 2, 'input_dim_edge': 3, 'max_num_nodes_in_graph': 417, 'device': device(type='cuda', index=0)}\n",
            "Epoch: 001, Train Acc: 61.5676, Train Loss: 0.6784, Valid Loss: 0.6784, Test Acc: 58.4570\n",
            "Epoch: 002, Train Acc: 65.3784, Train Loss: 0.6627, Valid Loss: 0.6660, Test Acc: 64.6884\n",
            "Epoch: 003, Train Acc: 66.0270, Train Loss: 0.6223, Valid Loss: 0.6554, Test Acc: 64.0950\n",
            "Epoch: 004, Train Acc: 67.1351, Train Loss: 0.5803, Valid Loss: 0.5916, Test Acc: 66.4688\n",
            "Epoch: 005, Train Acc: 67.8649, Train Loss: 0.5810, Valid Loss: 0.5931, Test Acc: 64.0950\n",
            "Epoch: 006, Train Acc: 70.9459, Train Loss: 0.5580, Valid Loss: 0.5793, Test Acc: 67.9525\n",
            "Epoch: 007, Train Acc: 72.8378, Train Loss: 0.5433, Valid Loss: 0.5791, Test Acc: 68.5460\n",
            "Epoch: 008, Train Acc: 73.4865, Train Loss: 0.5370, Valid Loss: 0.5706, Test Acc: 70.9199\n",
            "Epoch: 009, Train Acc: 76.6216, Train Loss: 0.4952, Valid Loss: 0.5383, Test Acc: 72.7003\n",
            "Epoch: 010, Train Acc: 75.1081, Train Loss: 0.5170, Valid Loss: 0.5681, Test Acc: 71.2166\n",
            "Epoch: 011, Train Acc: 75.8649, Train Loss: 0.4945, Valid Loss: 0.5474, Test Acc: 71.8101\n",
            "Epoch: 012, Train Acc: 76.0270, Train Loss: 0.5009, Valid Loss: 0.5583, Test Acc: 73.5905\n",
            "Epoch: 013, Train Acc: 78.0270, Train Loss: 0.4851, Valid Loss: 0.5276, Test Acc: 73.8872\n",
            "Epoch: 014, Train Acc: 78.9189, Train Loss: 0.4578, Valid Loss: 0.5261, Test Acc: 75.0742\n",
            "Epoch: 015, Train Acc: 79.3514, Train Loss: 0.4544, Valid Loss: 0.5170, Test Acc: 77.1513\n",
            "Epoch: 016, Train Acc: 79.3514, Train Loss: 0.4594, Valid Loss: 0.5122, Test Acc: 75.6677\n",
            "Epoch: 017, Train Acc: 75.4865, Train Loss: 0.5047, Valid Loss: 0.5607, Test Acc: 71.8101\n",
            "Epoch: 018, Train Acc: 79.5135, Train Loss: 0.4472, Valid Loss: 0.5172, Test Acc: 76.2611\n",
            "Epoch: 019, Train Acc: 78.6486, Train Loss: 0.4590, Valid Loss: 0.5097, Test Acc: 73.8872\n",
            "Epoch: 020, Train Acc: 80.2973, Train Loss: 0.4325, Valid Loss: 0.5033, Test Acc: 76.2611\n",
            "Epoch: 021, Train Acc: 80.4324, Train Loss: 0.4425, Valid Loss: 0.5038, Test Acc: 75.3709\n",
            "Epoch: 022, Train Acc: 80.2162, Train Loss: 0.4411, Valid Loss: 0.4987, Test Acc: 75.0742\n",
            "Epoch: 023, Train Acc: 80.8378, Train Loss: 0.4301, Valid Loss: 0.4937, Test Acc: 74.1840\n",
            "Epoch: 024, Train Acc: 81.5676, Train Loss: 0.4166, Valid Loss: 0.5021, Test Acc: 75.6677\n",
            "Epoch: 025, Train Acc: 81.2703, Train Loss: 0.4196, Valid Loss: 0.4940, Test Acc: 75.6677\n",
            "Epoch: 026, Train Acc: 80.9459, Train Loss: 0.4268, Valid Loss: 0.5141, Test Acc: 75.0742\n",
            "Epoch: 027, Train Acc: 81.9189, Train Loss: 0.4264, Valid Loss: 0.5126, Test Acc: 78.0415\n",
            "Epoch: 028, Train Acc: 81.5946, Train Loss: 0.4112, Valid Loss: 0.5012, Test Acc: 75.9644\n",
            "Epoch: 029, Train Acc: 80.7027, Train Loss: 0.4392, Valid Loss: 0.4783, Test Acc: 75.6677\n",
            "Epoch: 030, Train Acc: 82.6216, Train Loss: 0.4073, Valid Loss: 0.5103, Test Acc: 76.5579\n",
            "Epoch: 031, Train Acc: 82.3514, Train Loss: 0.4052, Valid Loss: 0.4847, Test Acc: 75.9644\n",
            "Epoch: 032, Train Acc: 83.1892, Train Loss: 0.3899, Valid Loss: 0.4771, Test Acc: 75.9644\n",
            "Epoch: 033, Train Acc: 82.1081, Train Loss: 0.4268, Valid Loss: 0.4847, Test Acc: 76.2611\n",
            "Epoch: 034, Train Acc: 82.8378, Train Loss: 0.3944, Valid Loss: 0.4890, Test Acc: 77.4481\n",
            "Epoch: 035, Train Acc: 81.0270, Train Loss: 0.4216, Valid Loss: 0.5165, Test Acc: 75.9644\n",
            "Epoch: 036, Train Acc: 81.4595, Train Loss: 0.4102, Valid Loss: 0.5039, Test Acc: 74.1840\n",
            "Epoch: 037, Train Acc: 82.0811, Train Loss: 0.4085, Valid Loss: 0.4779, Test Acc: 73.2938\n",
            "Epoch: 038, Train Acc: 82.7027, Train Loss: 0.3979, Valid Loss: 0.4807, Test Acc: 74.7774\n",
            "Epoch: 039, Train Acc: 79.4054, Train Loss: 0.4360, Valid Loss: 0.4932, Test Acc: 73.2938\n",
            "Epoch: 040, Train Acc: 81.1351, Train Loss: 0.4327, Valid Loss: 0.5246, Test Acc: 73.5905\n",
            "Epoch: 041, Train Acc: 82.1351, Train Loss: 0.4104, Valid Loss: 0.4847, Test Acc: 76.5579\n",
            "Epoch: 042, Train Acc: 83.7027, Train Loss: 0.3749, Valid Loss: 0.4555, Test Acc: 74.7774\n",
            "Epoch: 043, Train Acc: 83.5946, Train Loss: 0.3786, Valid Loss: 0.4653, Test Acc: 75.9644\n",
            "Epoch: 044, Train Acc: 83.3514, Train Loss: 0.3870, Valid Loss: 0.4857, Test Acc: 78.0415\n",
            "Epoch: 045, Train Acc: 82.6757, Train Loss: 0.4007, Valid Loss: 0.5007, Test Acc: 75.3709\n",
            "Epoch: 046, Train Acc: 83.3243, Train Loss: 0.3818, Valid Loss: 0.4469, Test Acc: 73.8872\n",
            "Epoch: 047, Train Acc: 78.7297, Train Loss: 0.4469, Valid Loss: 0.5386, Test Acc: 72.9970\n",
            "Epoch: 048, Train Acc: 83.8649, Train Loss: 0.3797, Valid Loss: 0.4458, Test Acc: 76.2611\n",
            "Epoch: 049, Train Acc: 83.9459, Train Loss: 0.3807, Valid Loss: 0.4664, Test Acc: 75.9644\n",
            "Epoch: 050, Train Acc: 83.8108, Train Loss: 0.3780, Valid Loss: 0.4773, Test Acc: 77.7448\n",
            "Epoch: 051, Train Acc: 83.2703, Train Loss: 0.3825, Valid Loss: 0.4625, Test Acc: 75.0742\n",
            "Epoch: 052, Train Acc: 84.5135, Train Loss: 0.3614, Valid Loss: 0.4885, Test Acc: 76.5579\n",
            "Epoch: 053, Train Acc: 83.5946, Train Loss: 0.3767, Valid Loss: 0.4646, Test Acc: 76.8546\n",
            "Epoch: 054, Train Acc: 84.6486, Train Loss: 0.3647, Valid Loss: 0.4611, Test Acc: 74.4807\n",
            "Epoch: 055, Train Acc: 84.8919, Train Loss: 0.3630, Valid Loss: 0.4565, Test Acc: 76.8546\n",
            "Epoch: 056, Train Acc: 85.3243, Train Loss: 0.3558, Valid Loss: 0.4688, Test Acc: 76.8546\n",
            "Epoch: 057, Train Acc: 83.3784, Train Loss: 0.3916, Valid Loss: 0.4594, Test Acc: 74.1840\n",
            "Epoch: 058, Train Acc: 83.4595, Train Loss: 0.3807, Valid Loss: 0.5053, Test Acc: 76.5579\n",
            "Epoch: 059, Train Acc: 84.8108, Train Loss: 0.3543, Valid Loss: 0.4565, Test Acc: 75.6677\n",
            "Epoch: 060, Train Acc: 82.8919, Train Loss: 0.3976, Valid Loss: 0.5063, Test Acc: 73.5905\n",
            "Epoch: 061, Train Acc: 83.8919, Train Loss: 0.3804, Valid Loss: 0.4966, Test Acc: 77.1513\n",
            "Epoch: 062, Train Acc: 84.1081, Train Loss: 0.3746, Valid Loss: 0.4516, Test Acc: 74.4807\n",
            "\n",
            "\n",
            "Best test accuracy:  78.63501483679525\n",
            "Best epoch:  63\n"
          ]
        }
      ],
      "source": [
        "#Model-specific arguments\n",
        "\n",
        "# Optimisation params\n",
        "args.lr=0.01\n",
        "args.weight_decay=0.0005\n",
        "\n",
        "# Model configuration\n",
        "args.d=3\n",
        "args.layers=4\n",
        "args.hidden_channels= 15\n",
        "args.input_dropout=0.0\n",
        "args.dropout=0.0\n",
        "args.orth = \"householder\"   #choices=['matrix_exp', 'cayley', 'householder', 'euler'], parametrization for the orthogonal group\n",
        "args.edges_feat = \"bilinear\"    #choices=['none', 'concat', 'linear', 'bilinear']\n",
        "args.readout = \"concat\"       #choices=['mean', 'sum', 'max', 'sag', 'mlp', 'concat']\n",
        "\n",
        "\n",
        "config=args.__dict__\n",
        "print(config)\n",
        "\n",
        "model = DiscreteDiagSheafDiffusion(config)\n",
        "#model = DiscreteDiagPoolSheafDiffusion(config)\n",
        "#model = DiscreteBundleSheafDiffusion(config)\n",
        "model = model.to(args.device)\n",
        "\n",
        "sheaf_learner_params, other_params = model.grouped_parameters()\n",
        "\n",
        "# Optimizers \n",
        "optimizer = torch.optim.Adam([\n",
        "    {'params': sheaf_learner_params, 'weight_decay': config['sheaf_decay']},\n",
        "    {'params': other_params, 'weight_decay': config['weight_decay']}\n",
        "], lr=config['lr'])\n",
        "\n",
        "\n",
        "# Training and test criterions\n",
        "criterion_train = torch.nn.CrossEntropyLoss()\n",
        "# In order not to make test loss change with batch_size \n",
        "criterion_test = torch.nn.CrossEntropyLoss(reduction = 'sum')\n",
        "\n",
        "best_test_acc = 0\n",
        "best_epoch = 0\n",
        "# Initial best loss value, used for the early stopping technique\n",
        "best_loss = 100.00\n",
        "patience = args.patience\n",
        "\n",
        "for epoch in range(1, args.epochs):\n",
        "    # Training epoch\n",
        "    train(model, optimizer, criterion_train, train_loader, args.device)\n",
        "\n",
        "\n",
        "    # Compute training, validation and test accuracy\n",
        "    train_acc, train_loss = test(model, criterion_test, train_loader, args.device)\n",
        "    valid_acc, valid_loss = test(model, criterion_test, valid_loader, args.device)\n",
        "    test_acc, test_loss = test(model, criterion_test, test_loader, args.device)\n",
        "\n",
        "    if best_test_acc < test_acc:\n",
        "        best_test_acc = test_acc\n",
        "        best_epoch = epoch\n",
        "\n",
        "    if valid_loss < best_loss:\n",
        "        best_loss = valid_loss\n",
        "        patience = args.patience\n",
        "    else:\n",
        "        patience -= 1\n",
        "        \n",
        "    if patience <= 0: \n",
        "        # Early stopping with patience\n",
        "        break \n",
        "\n",
        "    print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Train Loss: {train_loss:.4f}, Valid Loss: {valid_loss:.4f}, Test Acc: {test_acc:.4f}')\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"Best test accuracy: \", best_test_acc)\n",
        "print(\"Best epoch: \", best_epoch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OK8R0pc4pdj4"
      },
      "source": [
        "#### Diagonal sheaf diffusion, edge-features handling through bilinear transform, **mean** readout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rvq8oinipdj5",
        "outputId": "15e80211-58e0-4706-d2a1-a5fc8799f524"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epochs': 200, 'lr': 0.02, 'weight_decay': 0.0005, 'sheaf_decay': 0.0005, 'patience': 15, 'second_linear': False, 'd': 3, 'layers': 4, 'normalised': True, 'deg_normalised': False, 'linear': False, 'hidden_channels': 15, 'input_dropout': 0.0, 'dropout': 0.0, 'left_weights': True, 'right_weights': True, 'add_lp': False, 'add_hp': False, 'use_act': True, 'sheaf_act': 'tanh', 'edge_weights': True, 'orth': 'householder', 'edges_feat': 'bilinear', 'readout': 'mean', 'dense_intermediate_dim': 256, 'dense_output_graph_dim': 128, 'output_nn_intermediate_dim': 64, 'set_transformer_k': 8, 'input_dim': 14, 'output_dim': 2, 'input_dim_edge': 3, 'max_num_nodes_in_graph': 417, 'device': device(type='cuda', index=0)}\n",
            "Epoch: 001, Train Acc: 67.6757, Train Loss: 0.6268, Valid Loss: 0.6335, Test Acc: 64.9852\n",
            "Epoch: 002, Train Acc: 68.5405, Train Loss: 0.5949, Valid Loss: 0.6064, Test Acc: 67.0623\n",
            "Epoch: 003, Train Acc: 68.7027, Train Loss: 0.5823, Valid Loss: 0.5995, Test Acc: 66.4688\n",
            "Epoch: 004, Train Acc: 72.0000, Train Loss: 0.5564, Valid Loss: 0.5856, Test Acc: 68.2493\n",
            "Epoch: 005, Train Acc: 67.2162, Train Loss: 0.6187, Valid Loss: 0.6648, Test Acc: 67.6558\n",
            "Epoch: 006, Train Acc: 72.7838, Train Loss: 0.5512, Valid Loss: 0.5688, Test Acc: 70.3264\n",
            "Epoch: 007, Train Acc: 73.8378, Train Loss: 0.5260, Valid Loss: 0.5552, Test Acc: 69.4362\n",
            "Epoch: 008, Train Acc: 73.3784, Train Loss: 0.5347, Valid Loss: 0.5697, Test Acc: 70.0297\n",
            "Epoch: 009, Train Acc: 76.7027, Train Loss: 0.4986, Valid Loss: 0.5242, Test Acc: 72.7003\n",
            "Epoch: 010, Train Acc: 76.3243, Train Loss: 0.5076, Valid Loss: 0.5327, Test Acc: 69.7329\n",
            "Epoch: 011, Train Acc: 75.9459, Train Loss: 0.4947, Valid Loss: 0.5260, Test Acc: 70.9199\n",
            "Epoch: 012, Train Acc: 77.7027, Train Loss: 0.4934, Valid Loss: 0.5463, Test Acc: 72.9970\n",
            "Epoch: 013, Train Acc: 75.3243, Train Loss: 0.4989, Valid Loss: 0.5386, Test Acc: 71.5134\n",
            "Epoch: 014, Train Acc: 76.2703, Train Loss: 0.4925, Valid Loss: 0.5473, Test Acc: 71.5134\n",
            "Epoch: 015, Train Acc: 75.8108, Train Loss: 0.5007, Valid Loss: 0.5499, Test Acc: 72.4036\n",
            "Epoch: 016, Train Acc: 76.8378, Train Loss: 0.4839, Valid Loss: 0.5059, Test Acc: 73.8872\n",
            "Epoch: 017, Train Acc: 75.8108, Train Loss: 0.4946, Valid Loss: 0.5575, Test Acc: 68.5460\n",
            "Epoch: 018, Train Acc: 77.9730, Train Loss: 0.4762, Valid Loss: 0.5145, Test Acc: 71.8101\n",
            "Epoch: 019, Train Acc: 78.1622, Train Loss: 0.4842, Valid Loss: 0.5210, Test Acc: 70.3264\n",
            "Epoch: 020, Train Acc: 78.8378, Train Loss: 0.4697, Valid Loss: 0.5507, Test Acc: 73.8872\n",
            "Epoch: 021, Train Acc: 76.8919, Train Loss: 0.4838, Valid Loss: 0.5639, Test Acc: 74.1840\n",
            "Epoch: 022, Train Acc: 78.2703, Train Loss: 0.4679, Valid Loss: 0.5398, Test Acc: 73.2938\n",
            "Epoch: 023, Train Acc: 78.1081, Train Loss: 0.4675, Valid Loss: 0.5234, Test Acc: 74.4807\n",
            "Epoch: 024, Train Acc: 79.4595, Train Loss: 0.4594, Valid Loss: 0.5239, Test Acc: 75.3709\n",
            "Epoch: 025, Train Acc: 79.4865, Train Loss: 0.4512, Valid Loss: 0.5416, Test Acc: 71.5134\n",
            "Epoch: 026, Train Acc: 77.4595, Train Loss: 0.4745, Valid Loss: 0.5312, Test Acc: 70.3264\n",
            "Epoch: 027, Train Acc: 78.4324, Train Loss: 0.4628, Valid Loss: 0.5435, Test Acc: 73.2938\n",
            "Epoch: 028, Train Acc: 78.9459, Train Loss: 0.4602, Valid Loss: 0.5468, Test Acc: 72.4036\n",
            "Epoch: 029, Train Acc: 77.5946, Train Loss: 0.4754, Valid Loss: 0.5346, Test Acc: 70.3264\n",
            "Epoch: 030, Train Acc: 77.3243, Train Loss: 0.4783, Valid Loss: 0.5340, Test Acc: 71.8101\n",
            "\n",
            "\n",
            "Best test accuracy:  75.37091988130564\n",
            "Best epoch:  24\n"
          ]
        }
      ],
      "source": [
        "#Model-specific arguments\n",
        "\n",
        "# Optimisation params\n",
        "args.lr=0.02\n",
        "args.weight_decay=0.0005\n",
        "\n",
        "# Model configuration\n",
        "args.d=3\n",
        "args.layers=4\n",
        "args.hidden_channels= 15\n",
        "args.input_dropout=0.0\n",
        "args.dropout=0.0\n",
        "args.orth = \"householder\"   #choices=['matrix_exp', 'cayley', 'householder', 'euler'], parametrization for the orthogonal group\n",
        "args.edges_feat = \"bilinear\"    #choices=['none', 'concat', 'linear', 'bilinear']\n",
        "args.readout = \"mean\"       #choices=['mean', 'sum', 'max', 'sag', 'mlp', 'concat']\n",
        "\n",
        "\n",
        "config=args.__dict__\n",
        "print(config)\n",
        "\n",
        "model = DiscreteDiagSheafDiffusion(config)\n",
        "#model = DiscreteDiagPoolSheafDiffusion(config)\n",
        "#model = DiscreteBundleSheafDiffusion(config)\n",
        "model = model.to(args.device)\n",
        "\n",
        "sheaf_learner_params, other_params = model.grouped_parameters()\n",
        "\n",
        "# Optimizers \n",
        "optimizer = torch.optim.Adam([\n",
        "    {'params': sheaf_learner_params, 'weight_decay': config['sheaf_decay']},\n",
        "    {'params': other_params, 'weight_decay': config['weight_decay']}\n",
        "], lr=config['lr'])\n",
        "\n",
        "\n",
        "# Training and test criterions\n",
        "criterion_train = torch.nn.CrossEntropyLoss()\n",
        "# In order not to make test loss change with batch_size \n",
        "criterion_test = torch.nn.CrossEntropyLoss(reduction = 'sum')\n",
        "\n",
        "best_test_acc = 0\n",
        "best_epoch = 0\n",
        "# Initial best loss value, used for the early stopping technique\n",
        "best_loss = 100.00\n",
        "patience = args.patience\n",
        "\n",
        "for epoch in range(1, args.epochs):\n",
        "    # Training epoch\n",
        "    train(model, optimizer, criterion_train, train_loader, args.device)\n",
        "\n",
        "\n",
        "    # Compute training, validation and test accuracy\n",
        "    train_acc, train_loss = test(model, criterion_test, train_loader, args.device)\n",
        "    valid_acc, valid_loss = test(model, criterion_test, valid_loader, args.device)\n",
        "    test_acc, test_loss = test(model, criterion_test, test_loader, args.device)\n",
        "\n",
        "    if best_test_acc < test_acc:\n",
        "        best_test_acc = test_acc\n",
        "        best_epoch = epoch\n",
        "\n",
        "    if valid_loss < best_loss:\n",
        "        best_loss = valid_loss\n",
        "        patience = args.patience\n",
        "    else:\n",
        "        patience -= 1\n",
        "        \n",
        "    if patience <= 0: \n",
        "        # Early stopping with patience\n",
        "        break \n",
        "\n",
        "    print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Train Loss: {train_loss:.4f}, Valid Loss: {valid_loss:.4f}, Test Acc: {test_acc:.4f}')\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"Best test accuracy: \", best_test_acc)\n",
        "print(\"Best epoch: \", best_epoch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XL-x5jgRlXW3"
      },
      "source": [
        "#### Diagonal sheaf diffusion, edge-features handling through bilinear transform, **sum** readout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4WatTUDiUBdb",
        "outputId": "558530b9-d46a-4acc-8225-068909f01022"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epochs': 200, 'lr': 0.03, 'weight_decay': 0.0005, 'sheaf_decay': 0.0005, 'patience': 15, 'second_linear': False, 'd': 3, 'layers': 4, 'normalised': True, 'deg_normalised': False, 'linear': False, 'hidden_channels': 15, 'input_dropout': 0.0, 'dropout': 0.0, 'left_weights': True, 'right_weights': True, 'add_lp': False, 'add_hp': False, 'use_act': True, 'sheaf_act': 'tanh', 'edge_weights': True, 'orth': 'householder', 'edges_feat': 'bilinear', 'readout': 'sum', 'dense_intermediate_dim': 256, 'dense_output_graph_dim': 128, 'output_nn_intermediate_dim': 64, 'set_transformer_k': 8, 'input_dim': 14, 'output_dim': 2, 'input_dim_edge': 3, 'max_num_nodes_in_graph': 417, 'device': device(type='cuda', index=0)}\n",
            "Epoch: 001, Train Acc: 64.7027, Train Loss: 0.6477, Valid Loss: 0.6525, Test Acc: 63.2047\n",
            "Epoch: 002, Train Acc: 59.0541, Train Loss: 0.6494, Valid Loss: 0.6410, Test Acc: 60.2374\n",
            "Epoch: 003, Train Acc: 65.9730, Train Loss: 0.6074, Valid Loss: 0.6151, Test Acc: 64.9852\n",
            "Epoch: 004, Train Acc: 68.4054, Train Loss: 0.5813, Valid Loss: 0.6154, Test Acc: 65.5786\n",
            "Epoch: 005, Train Acc: 68.1351, Train Loss: 0.5901, Valid Loss: 0.6192, Test Acc: 64.9852\n",
            "Epoch: 006, Train Acc: 67.7838, Train Loss: 0.5796, Valid Loss: 0.5991, Test Acc: 65.8754\n",
            "Epoch: 007, Train Acc: 66.7297, Train Loss: 0.5842, Valid Loss: 0.5884, Test Acc: 66.4688\n",
            "Epoch: 008, Train Acc: 70.8378, Train Loss: 0.5753, Valid Loss: 0.6202, Test Acc: 69.7329\n",
            "Epoch: 009, Train Acc: 71.0000, Train Loss: 0.5530, Valid Loss: 0.5744, Test Acc: 67.6558\n",
            "Epoch: 010, Train Acc: 71.9730, Train Loss: 0.5442, Valid Loss: 0.5716, Test Acc: 71.5134\n",
            "Epoch: 011, Train Acc: 70.7838, Train Loss: 0.5473, Valid Loss: 0.5657, Test Acc: 68.8427\n",
            "Epoch: 012, Train Acc: 71.8919, Train Loss: 0.5388, Valid Loss: 0.5521, Test Acc: 68.8427\n",
            "Epoch: 013, Train Acc: 74.0811, Train Loss: 0.5335, Valid Loss: 0.5751, Test Acc: 72.1068\n",
            "Epoch: 014, Train Acc: 73.0541, Train Loss: 0.5304, Valid Loss: 0.5388, Test Acc: 69.4362\n",
            "Epoch: 015, Train Acc: 74.2162, Train Loss: 0.5291, Valid Loss: 0.5640, Test Acc: 71.5134\n",
            "Epoch: 016, Train Acc: 73.4595, Train Loss: 0.5457, Valid Loss: 0.5809, Test Acc: 71.2166\n",
            "Epoch: 017, Train Acc: 71.4865, Train Loss: 0.5505, Valid Loss: 0.5691, Test Acc: 70.9199\n",
            "Epoch: 018, Train Acc: 73.6486, Train Loss: 0.5334, Valid Loss: 0.5620, Test Acc: 70.9199\n",
            "Epoch: 019, Train Acc: 73.4595, Train Loss: 0.5281, Valid Loss: 0.5436, Test Acc: 71.2166\n",
            "Epoch: 020, Train Acc: 74.0811, Train Loss: 0.5168, Valid Loss: 0.5360, Test Acc: 72.9970\n",
            "Epoch: 021, Train Acc: 76.5946, Train Loss: 0.5122, Valid Loss: 0.5195, Test Acc: 70.9199\n",
            "Epoch: 022, Train Acc: 76.2162, Train Loss: 0.5059, Valid Loss: 0.5289, Test Acc: 73.8872\n",
            "Epoch: 023, Train Acc: 73.5135, Train Loss: 0.5448, Valid Loss: 0.5685, Test Acc: 69.1395\n",
            "Epoch: 024, Train Acc: 73.5405, Train Loss: 0.5519, Valid Loss: 0.5667, Test Acc: 71.2166\n",
            "Epoch: 025, Train Acc: 75.1351, Train Loss: 0.5126, Valid Loss: 0.5299, Test Acc: 75.0742\n",
            "Epoch: 026, Train Acc: 73.4595, Train Loss: 0.5413, Valid Loss: 0.5727, Test Acc: 68.8427\n",
            "Epoch: 027, Train Acc: 75.2432, Train Loss: 0.5229, Valid Loss: 0.5622, Test Acc: 72.1068\n",
            "Epoch: 028, Train Acc: 74.9730, Train Loss: 0.5121, Valid Loss: 0.5229, Test Acc: 73.5905\n",
            "Epoch: 029, Train Acc: 76.0811, Train Loss: 0.5099, Valid Loss: 0.5560, Test Acc: 74.1840\n",
            "Epoch: 030, Train Acc: 76.7838, Train Loss: 0.4967, Valid Loss: 0.5295, Test Acc: 74.4807\n",
            "Epoch: 031, Train Acc: 76.3243, Train Loss: 0.5000, Valid Loss: 0.5192, Test Acc: 70.9199\n",
            "Epoch: 032, Train Acc: 75.8919, Train Loss: 0.5040, Valid Loss: 0.5133, Test Acc: 74.1840\n",
            "Epoch: 033, Train Acc: 76.2162, Train Loss: 0.5098, Valid Loss: 0.5497, Test Acc: 73.2938\n",
            "Epoch: 034, Train Acc: 73.0811, Train Loss: 0.5389, Valid Loss: 0.5541, Test Acc: 67.6558\n",
            "Epoch: 035, Train Acc: 74.0541, Train Loss: 0.5201, Valid Loss: 0.5251, Test Acc: 69.4362\n",
            "Epoch: 036, Train Acc: 75.6216, Train Loss: 0.5055, Valid Loss: 0.5187, Test Acc: 72.9970\n",
            "Epoch: 037, Train Acc: 77.7297, Train Loss: 0.4883, Valid Loss: 0.5072, Test Acc: 73.5905\n",
            "Epoch: 038, Train Acc: 76.2162, Train Loss: 0.4958, Valid Loss: 0.5594, Test Acc: 73.2938\n",
            "Epoch: 039, Train Acc: 76.9189, Train Loss: 0.4956, Valid Loss: 0.5190, Test Acc: 73.2938\n",
            "Epoch: 040, Train Acc: 76.3514, Train Loss: 0.4918, Valid Loss: 0.5364, Test Acc: 74.1840\n",
            "Epoch: 041, Train Acc: 78.1622, Train Loss: 0.4856, Valid Loss: 0.5272, Test Acc: 74.7774\n",
            "Epoch: 042, Train Acc: 77.8919, Train Loss: 0.4845, Valid Loss: 0.4899, Test Acc: 74.1840\n",
            "Epoch: 043, Train Acc: 76.7838, Train Loss: 0.4895, Valid Loss: 0.5106, Test Acc: 72.7003\n",
            "Epoch: 044, Train Acc: 77.5405, Train Loss: 0.4847, Valid Loss: 0.5121, Test Acc: 72.7003\n",
            "Epoch: 045, Train Acc: 78.0270, Train Loss: 0.4829, Valid Loss: 0.4932, Test Acc: 74.1840\n",
            "Epoch: 046, Train Acc: 75.0270, Train Loss: 0.5312, Valid Loss: 0.5934, Test Acc: 72.4036\n",
            "Epoch: 047, Train Acc: 77.2973, Train Loss: 0.4815, Valid Loss: 0.4960, Test Acc: 74.4807\n",
            "Epoch: 048, Train Acc: 75.9730, Train Loss: 0.4842, Valid Loss: 0.5094, Test Acc: 73.2938\n",
            "Epoch: 049, Train Acc: 77.3243, Train Loss: 0.4835, Valid Loss: 0.5007, Test Acc: 74.4807\n",
            "Epoch: 050, Train Acc: 76.5135, Train Loss: 0.4889, Valid Loss: 0.5209, Test Acc: 73.5905\n",
            "Epoch: 051, Train Acc: 78.0811, Train Loss: 0.4760, Valid Loss: 0.5038, Test Acc: 76.2611\n",
            "Epoch: 052, Train Acc: 76.8108, Train Loss: 0.4842, Valid Loss: 0.4967, Test Acc: 73.8872\n",
            "Epoch: 053, Train Acc: 75.8919, Train Loss: 0.5174, Valid Loss: 0.5288, Test Acc: 71.2166\n",
            "Epoch: 054, Train Acc: 76.2162, Train Loss: 0.4947, Valid Loss: 0.5205, Test Acc: 73.2938\n",
            "Epoch: 055, Train Acc: 77.7027, Train Loss: 0.4902, Valid Loss: 0.5038, Test Acc: 72.1068\n",
            "Epoch: 056, Train Acc: 78.3784, Train Loss: 0.4799, Valid Loss: 0.5119, Test Acc: 72.4036\n",
            "\n",
            "\n",
            "Best test accuracy:  76.26112759643917\n",
            "Best epoch:  51\n"
          ]
        }
      ],
      "source": [
        "#Model-specific arguments\n",
        "\n",
        "# Optimisation params\n",
        "args.lr=0.03\n",
        "args.weight_decay=0.0005\n",
        "\n",
        "# Model configuration\n",
        "args.d=3\n",
        "args.layers=4\n",
        "args.hidden_channels= 15\n",
        "args.input_dropout=0.0\n",
        "args.dropout=0.0\n",
        "args.orth = \"householder\"   #choices=['matrix_exp', 'cayley', 'householder', 'euler'], parametrization for the orthogonal group\n",
        "args.edges_feat = \"bilinear\"    #choices=['none', 'concat', 'linear', 'bilinear']\n",
        "args.readout = \"sum\"       #choices=['mean', 'sum', 'max', 'sag', 'mlp', 'concat']\n",
        "\n",
        "\n",
        "config=args.__dict__\n",
        "print(config)\n",
        "\n",
        "model = DiscreteDiagSheafDiffusion(config)\n",
        "#model = DiscreteDiagPoolSheafDiffusion(config)\n",
        "#model = DiscreteBundleSheafDiffusion(config)\n",
        "model = model.to(args.device)\n",
        "\n",
        "sheaf_learner_params, other_params = model.grouped_parameters()\n",
        "\n",
        "# Optimizers \n",
        "optimizer = torch.optim.Adam([\n",
        "    {'params': sheaf_learner_params, 'weight_decay': config['sheaf_decay']},\n",
        "    {'params': other_params, 'weight_decay': config['weight_decay']}\n",
        "], lr=config['lr'])\n",
        "\n",
        "\n",
        "# Training and test criterions\n",
        "criterion_train = torch.nn.CrossEntropyLoss()\n",
        "# In order not to make test loss change with batch_size \n",
        "criterion_test = torch.nn.CrossEntropyLoss(reduction = 'sum')\n",
        "\n",
        "best_test_acc = 0\n",
        "best_epoch = 0\n",
        "# Initial best loss value, used for the early stopping technique\n",
        "best_loss = 100.00\n",
        "patience = args.patience\n",
        "\n",
        "for epoch in range(1, args.epochs):\n",
        "    # Training epoch\n",
        "    train(model, optimizer, criterion_train, train_loader, args.device)\n",
        "\n",
        "\n",
        "    # Compute training, validation and test accuracy\n",
        "    train_acc, train_loss = test(model, criterion_test, train_loader, args.device)\n",
        "    valid_acc, valid_loss = test(model, criterion_test, valid_loader, args.device)\n",
        "    test_acc, test_loss = test(model, criterion_test, test_loader, args.device)\n",
        "\n",
        "    if best_test_acc < test_acc:\n",
        "        best_test_acc = test_acc\n",
        "        best_epoch = epoch\n",
        "\n",
        "    if valid_loss < best_loss:\n",
        "        best_loss = valid_loss\n",
        "        patience = args.patience\n",
        "    else:\n",
        "        patience -= 1\n",
        "        \n",
        "    if patience <= 0: \n",
        "        # Early stopping with patience\n",
        "        break \n",
        "\n",
        "    print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Train Loss: {train_loss:.4f}, Valid Loss: {valid_loss:.4f}, Test Acc: {test_acc:.4f}')\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"Best test accuracy: \", best_test_acc)\n",
        "print(\"Best epoch: \", best_epoch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-jnD_cKlc0u"
      },
      "source": [
        "#### Diagonal sheaf diffusion, edge-features handling through bilinear transform, **max** readout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T8XUxCoron6Q",
        "outputId": "ad1b21ea-3e11-4ea1-8e37-05e42c7eaa5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epochs': 200, 'lr': 0.01, 'weight_decay': 0.0005, 'sheaf_decay': 0.0005, 'patience': 15, 'second_linear': False, 'd': 3, 'layers': 4, 'normalised': True, 'deg_normalised': False, 'linear': False, 'hidden_channels': 15, 'input_dropout': 0.0, 'dropout': 0.0, 'left_weights': True, 'right_weights': True, 'add_lp': False, 'add_hp': False, 'use_act': True, 'sheaf_act': 'tanh', 'edge_weights': True, 'orth': 'householder', 'edges_feat': 'bilinear', 'readout': 'max', 'dense_intermediate_dim': 256, 'dense_output_graph_dim': 128, 'output_nn_intermediate_dim': 64, 'set_transformer_k': 8, 'input_dim': 14, 'output_dim': 2, 'input_dim_edge': 3, 'max_num_nodes_in_graph': 417, 'device': device(type='cuda', index=0)}\n",
            "Epoch: 001, Train Acc: 59.3514, Train Loss: 0.6625, Valid Loss: 0.6609, Test Acc: 58.4570\n",
            "Epoch: 002, Train Acc: 65.8108, Train Loss: 0.6104, Valid Loss: 0.6279, Test Acc: 64.9852\n",
            "Epoch: 003, Train Acc: 67.0270, Train Loss: 0.5980, Valid Loss: 0.6292, Test Acc: 65.5786\n",
            "Epoch: 004, Train Acc: 68.6216, Train Loss: 0.5773, Valid Loss: 0.6077, Test Acc: 68.2493\n",
            "Epoch: 005, Train Acc: 72.6216, Train Loss: 0.5407, Valid Loss: 0.5831, Test Acc: 72.1068\n",
            "Epoch: 006, Train Acc: 73.7297, Train Loss: 0.5201, Valid Loss: 0.5738, Test Acc: 70.6231\n",
            "Epoch: 007, Train Acc: 73.8378, Train Loss: 0.5316, Valid Loss: 0.5951, Test Acc: 74.7774\n",
            "Epoch: 008, Train Acc: 77.5405, Train Loss: 0.4752, Valid Loss: 0.5368, Test Acc: 72.9970\n",
            "Epoch: 009, Train Acc: 77.0270, Train Loss: 0.5003, Valid Loss: 0.5547, Test Acc: 72.7003\n",
            "Epoch: 010, Train Acc: 77.5135, Train Loss: 0.4799, Valid Loss: 0.5331, Test Acc: 72.1068\n",
            "Epoch: 011, Train Acc: 76.9730, Train Loss: 0.4787, Valid Loss: 0.5186, Test Acc: 72.4036\n",
            "Epoch: 012, Train Acc: 78.7568, Train Loss: 0.4493, Valid Loss: 0.5061, Test Acc: 71.5134\n",
            "Epoch: 013, Train Acc: 81.0000, Train Loss: 0.4292, Valid Loss: 0.4942, Test Acc: 74.4807\n",
            "Epoch: 014, Train Acc: 80.5676, Train Loss: 0.4293, Valid Loss: 0.4976, Test Acc: 75.0742\n",
            "Epoch: 015, Train Acc: 80.4595, Train Loss: 0.4329, Valid Loss: 0.5337, Test Acc: 75.3709\n",
            "Epoch: 016, Train Acc: 81.2973, Train Loss: 0.4231, Valid Loss: 0.4973, Test Acc: 75.0742\n",
            "Epoch: 017, Train Acc: 80.0270, Train Loss: 0.4344, Valid Loss: 0.5039, Test Acc: 76.2611\n",
            "Epoch: 018, Train Acc: 81.5135, Train Loss: 0.4264, Valid Loss: 0.5016, Test Acc: 78.6350\n",
            "Epoch: 019, Train Acc: 81.9189, Train Loss: 0.4102, Valid Loss: 0.4789, Test Acc: 75.6677\n",
            "Epoch: 020, Train Acc: 80.9189, Train Loss: 0.4212, Valid Loss: 0.4924, Test Acc: 75.9644\n",
            "Epoch: 021, Train Acc: 81.8108, Train Loss: 0.4181, Valid Loss: 0.4985, Test Acc: 75.9644\n",
            "Epoch: 022, Train Acc: 82.7297, Train Loss: 0.3989, Valid Loss: 0.4581, Test Acc: 78.6350\n",
            "Epoch: 023, Train Acc: 82.9459, Train Loss: 0.4018, Valid Loss: 0.5144, Test Acc: 76.5579\n",
            "Epoch: 024, Train Acc: 81.9459, Train Loss: 0.4056, Valid Loss: 0.5044, Test Acc: 77.1513\n",
            "Epoch: 025, Train Acc: 80.6216, Train Loss: 0.4209, Valid Loss: 0.4991, Test Acc: 74.1840\n",
            "Epoch: 026, Train Acc: 84.0000, Train Loss: 0.3760, Valid Loss: 0.4641, Test Acc: 79.5252\n",
            "Epoch: 027, Train Acc: 82.7297, Train Loss: 0.3935, Valid Loss: 0.5054, Test Acc: 76.8546\n",
            "Epoch: 028, Train Acc: 83.5135, Train Loss: 0.3827, Valid Loss: 0.4537, Test Acc: 77.7448\n",
            "Epoch: 029, Train Acc: 84.1351, Train Loss: 0.3683, Valid Loss: 0.4820, Test Acc: 80.4154\n",
            "Epoch: 030, Train Acc: 83.6757, Train Loss: 0.3662, Valid Loss: 0.4482, Test Acc: 77.7448\n",
            "Epoch: 031, Train Acc: 82.9459, Train Loss: 0.3956, Valid Loss: 0.4994, Test Acc: 77.7448\n",
            "Epoch: 032, Train Acc: 84.8378, Train Loss: 0.3616, Valid Loss: 0.4747, Test Acc: 77.1513\n",
            "Epoch: 033, Train Acc: 83.9730, Train Loss: 0.3730, Valid Loss: 0.4638, Test Acc: 78.9318\n",
            "Epoch: 034, Train Acc: 83.8378, Train Loss: 0.3756, Valid Loss: 0.4911, Test Acc: 77.4481\n",
            "Epoch: 035, Train Acc: 84.2703, Train Loss: 0.3576, Valid Loss: 0.4672, Test Acc: 79.2285\n",
            "Epoch: 036, Train Acc: 82.9459, Train Loss: 0.3739, Valid Loss: 0.4688, Test Acc: 76.8546\n",
            "Epoch: 037, Train Acc: 84.0811, Train Loss: 0.3587, Valid Loss: 0.4334, Test Acc: 76.5579\n",
            "Epoch: 038, Train Acc: 82.7027, Train Loss: 0.3972, Valid Loss: 0.5285, Test Acc: 75.0742\n",
            "Epoch: 039, Train Acc: 83.9459, Train Loss: 0.3724, Valid Loss: 0.4699, Test Acc: 76.2611\n",
            "Epoch: 040, Train Acc: 79.3784, Train Loss: 0.4327, Valid Loss: 0.5041, Test Acc: 73.8872\n",
            "Epoch: 041, Train Acc: 84.5405, Train Loss: 0.3606, Valid Loss: 0.4821, Test Acc: 78.3383\n",
            "Epoch: 042, Train Acc: 84.9730, Train Loss: 0.3582, Valid Loss: 0.4845, Test Acc: 77.1513\n",
            "Epoch: 043, Train Acc: 85.5946, Train Loss: 0.3422, Valid Loss: 0.4555, Test Acc: 78.9318\n",
            "Epoch: 044, Train Acc: 84.2703, Train Loss: 0.3703, Valid Loss: 0.5026, Test Acc: 77.7448\n",
            "Epoch: 045, Train Acc: 83.4324, Train Loss: 0.3795, Valid Loss: 0.5131, Test Acc: 77.1513\n",
            "Epoch: 046, Train Acc: 85.1892, Train Loss: 0.3481, Valid Loss: 0.4596, Test Acc: 78.3383\n",
            "Epoch: 047, Train Acc: 84.4324, Train Loss: 0.3503, Valid Loss: 0.4398, Test Acc: 76.5579\n",
            "Epoch: 048, Train Acc: 85.7297, Train Loss: 0.3321, Valid Loss: 0.4719, Test Acc: 79.8220\n",
            "Epoch: 049, Train Acc: 85.5135, Train Loss: 0.3438, Valid Loss: 0.4564, Test Acc: 76.2611\n",
            "Epoch: 050, Train Acc: 85.9189, Train Loss: 0.3310, Valid Loss: 0.4532, Test Acc: 79.5252\n",
            "Epoch: 051, Train Acc: 85.4324, Train Loss: 0.3402, Valid Loss: 0.4912, Test Acc: 78.9318\n",
            "\n",
            "\n",
            "Best test accuracy:  80.41543026706232\n",
            "Best epoch:  29\n"
          ]
        }
      ],
      "source": [
        "#Model-specific arguments\n",
        "\n",
        "# Optimisation params\n",
        "args.lr=0.01\n",
        "args.weight_decay=0.0005\n",
        "\n",
        "# Model configuration\n",
        "args.d=3\n",
        "args.layers=4\n",
        "args.hidden_channels= 15\n",
        "args.input_dropout=0.0\n",
        "args.dropout=0.0\n",
        "args.orth = \"householder\"   #choices=['matrix_exp', 'cayley', 'householder', 'euler'], parametrization for the orthogonal group\n",
        "args.edges_feat = \"bilinear\"    #choices=['none', 'concat', 'linear', 'bilinear']\n",
        "args.readout = \"max\"       #choices=['mean', 'sum', 'max', 'sag', 'mlp', 'concat']\n",
        "\n",
        "\n",
        "config=args.__dict__\n",
        "print(config)\n",
        "\n",
        "model = DiscreteDiagSheafDiffusion(config)\n",
        "#model = DiscreteDiagPoolSheafDiffusion(config)\n",
        "#model = DiscreteBundleSheafDiffusion(config)\n",
        "model = model.to(args.device)\n",
        "\n",
        "sheaf_learner_params, other_params = model.grouped_parameters()\n",
        "\n",
        "# Optimizers \n",
        "optimizer = torch.optim.Adam([\n",
        "    {'params': sheaf_learner_params, 'weight_decay': config['sheaf_decay']},\n",
        "    {'params': other_params, 'weight_decay': config['weight_decay']}\n",
        "], lr=config['lr'])\n",
        "\n",
        "\n",
        "# Training and test criterions\n",
        "criterion_train = torch.nn.CrossEntropyLoss()\n",
        "# In order not to make test loss change with batch_size \n",
        "criterion_test = torch.nn.CrossEntropyLoss(reduction = 'sum')\n",
        "\n",
        "best_test_acc = 0\n",
        "best_epoch = 0\n",
        "# Initial best loss value, used for the early stopping technique\n",
        "best_loss = 100.00\n",
        "patience = args.patience\n",
        "\n",
        "for epoch in range(1, args.epochs):\n",
        "    # Training epoch\n",
        "    train(model, optimizer, criterion_train, train_loader, args.device)\n",
        "\n",
        "\n",
        "    # Compute training, validation and test accuracy\n",
        "    train_acc, train_loss = test(model, criterion_test, train_loader, args.device)\n",
        "    valid_acc, valid_loss = test(model, criterion_test, valid_loader, args.device)\n",
        "    test_acc, test_loss = test(model, criterion_test, test_loader, args.device)\n",
        "\n",
        "    if best_test_acc < test_acc:\n",
        "        best_test_acc = test_acc\n",
        "        best_epoch = epoch\n",
        "\n",
        "    if valid_loss < best_loss:\n",
        "        best_loss = valid_loss\n",
        "        patience = args.patience\n",
        "    else:\n",
        "        patience -= 1\n",
        "        \n",
        "    if patience <= 0: \n",
        "        # Early stopping with patience\n",
        "        break \n",
        "\n",
        "    print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Train Loss: {train_loss:.4f}, Valid Loss: {valid_loss:.4f}, Test Acc: {test_acc:.4f}')\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"Best test accuracy: \", best_test_acc)\n",
        "print(\"Best epoch: \", best_epoch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZlE1-Hilpdj_"
      },
      "source": [
        "#### Diagonal sheaf diffusion, edge-features handling through bilinear transform, **MLP** readout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "igNwYbiMpdkA",
        "outputId": "8d871ade-2280-4d78-e31e-0ec0c68568b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epochs': 200, 'lr': 0.001, 'weight_decay': 0.0005, 'sheaf_decay': 0.0005, 'patience': 15, 'second_linear': False, 'd': 3, 'layers': 4, 'normalised': True, 'deg_normalised': False, 'linear': False, 'hidden_channels': 15, 'input_dropout': 0.0, 'dropout': 0.1, 'left_weights': True, 'right_weights': True, 'add_lp': False, 'add_hp': False, 'use_act': True, 'sheaf_act': 'tanh', 'edge_weights': True, 'orth': 'householder', 'edges_feat': 'bilinear', 'readout': 'mlp', 'dense_intermediate_dim': 256, 'dense_output_graph_dim': 128, 'output_nn_intermediate_dim': 64, 'set_transformer_k': 8, 'input_dim': 14, 'output_dim': 2, 'input_dim_edge': 3, 'max_num_nodes_in_graph': 417, 'device': device(type='cuda', index=0)}\n",
            "Epoch: 001, Train Acc: 61.6486, Train Loss: 0.6646, Valid Loss: 0.6777, Test Acc: 60.8309\n",
            "Epoch: 002, Train Acc: 63.0811, Train Loss: 0.6328, Valid Loss: 0.6487, Test Acc: 64.3917\n",
            "Epoch: 003, Train Acc: 66.5946, Train Loss: 0.6095, Valid Loss: 0.6301, Test Acc: 64.3917\n",
            "Epoch: 004, Train Acc: 69.4054, Train Loss: 0.5894, Valid Loss: 0.6080, Test Acc: 66.7656\n",
            "Epoch: 005, Train Acc: 70.3784, Train Loss: 0.5685, Valid Loss: 0.5965, Test Acc: 67.9525\n",
            "Epoch: 006, Train Acc: 70.7568, Train Loss: 0.5519, Valid Loss: 0.5892, Test Acc: 70.0297\n",
            "Epoch: 007, Train Acc: 72.1081, Train Loss: 0.5369, Valid Loss: 0.5813, Test Acc: 71.8101\n",
            "Epoch: 008, Train Acc: 73.3784, Train Loss: 0.5238, Valid Loss: 0.5744, Test Acc: 70.3264\n",
            "Epoch: 009, Train Acc: 74.2162, Train Loss: 0.5141, Valid Loss: 0.5762, Test Acc: 71.8101\n",
            "Epoch: 010, Train Acc: 75.5135, Train Loss: 0.5049, Valid Loss: 0.5769, Test Acc: 73.2938\n",
            "Epoch: 011, Train Acc: 76.4865, Train Loss: 0.4892, Valid Loss: 0.5626, Test Acc: 73.5905\n",
            "Epoch: 012, Train Acc: 76.8919, Train Loss: 0.4810, Valid Loss: 0.5695, Test Acc: 73.5905\n",
            "Epoch: 013, Train Acc: 77.7838, Train Loss: 0.4729, Valid Loss: 0.5625, Test Acc: 73.8872\n",
            "Epoch: 014, Train Acc: 78.3514, Train Loss: 0.4582, Valid Loss: 0.5616, Test Acc: 72.9970\n",
            "Epoch: 015, Train Acc: 79.7568, Train Loss: 0.4448, Valid Loss: 0.5525, Test Acc: 71.2166\n",
            "Epoch: 016, Train Acc: 80.0000, Train Loss: 0.4439, Valid Loss: 0.5490, Test Acc: 72.9970\n",
            "Epoch: 017, Train Acc: 80.9730, Train Loss: 0.4313, Valid Loss: 0.5470, Test Acc: 72.4036\n",
            "Epoch: 018, Train Acc: 81.4054, Train Loss: 0.4205, Valid Loss: 0.5430, Test Acc: 71.8101\n",
            "Epoch: 019, Train Acc: 83.0541, Train Loss: 0.4062, Valid Loss: 0.5305, Test Acc: 72.1068\n",
            "Epoch: 020, Train Acc: 83.3514, Train Loss: 0.3906, Valid Loss: 0.5310, Test Acc: 72.7003\n",
            "Epoch: 021, Train Acc: 83.8919, Train Loss: 0.3800, Valid Loss: 0.5301, Test Acc: 72.1068\n",
            "Epoch: 022, Train Acc: 84.0811, Train Loss: 0.3684, Valid Loss: 0.5214, Test Acc: 73.2938\n",
            "Epoch: 023, Train Acc: 85.7297, Train Loss: 0.3577, Valid Loss: 0.5315, Test Acc: 72.1068\n",
            "Epoch: 024, Train Acc: 86.1622, Train Loss: 0.3477, Valid Loss: 0.5271, Test Acc: 72.7003\n",
            "Epoch: 025, Train Acc: 86.7297, Train Loss: 0.3328, Valid Loss: 0.5362, Test Acc: 74.4807\n",
            "Epoch: 026, Train Acc: 86.0811, Train Loss: 0.3289, Valid Loss: 0.5413, Test Acc: 73.2938\n",
            "Epoch: 027, Train Acc: 87.1622, Train Loss: 0.3181, Valid Loss: 0.5345, Test Acc: 74.4807\n",
            "Epoch: 028, Train Acc: 87.4595, Train Loss: 0.3075, Valid Loss: 0.5317, Test Acc: 72.9970\n",
            "Epoch: 029, Train Acc: 87.2703, Train Loss: 0.3045, Valid Loss: 0.5442, Test Acc: 73.5905\n",
            "Epoch: 030, Train Acc: 87.8919, Train Loss: 0.3002, Valid Loss: 0.5370, Test Acc: 74.4807\n",
            "Epoch: 031, Train Acc: 89.0000, Train Loss: 0.2878, Valid Loss: 0.5305, Test Acc: 74.4807\n",
            "Epoch: 032, Train Acc: 89.2162, Train Loss: 0.2802, Valid Loss: 0.5315, Test Acc: 74.4807\n",
            "Epoch: 033, Train Acc: 89.5135, Train Loss: 0.2688, Valid Loss: 0.5277, Test Acc: 73.5905\n",
            "Epoch: 034, Train Acc: 89.2432, Train Loss: 0.2682, Valid Loss: 0.5521, Test Acc: 72.9970\n",
            "Epoch: 035, Train Acc: 90.5676, Train Loss: 0.2609, Valid Loss: 0.5202, Test Acc: 74.4807\n",
            "Epoch: 036, Train Acc: 90.0000, Train Loss: 0.2541, Valid Loss: 0.5384, Test Acc: 75.6677\n",
            "Epoch: 037, Train Acc: 90.5946, Train Loss: 0.2415, Valid Loss: 0.5428, Test Acc: 73.8872\n",
            "Epoch: 038, Train Acc: 91.3514, Train Loss: 0.2363, Valid Loss: 0.5321, Test Acc: 74.7774\n",
            "Epoch: 039, Train Acc: 91.6757, Train Loss: 0.2347, Valid Loss: 0.5254, Test Acc: 76.2611\n",
            "Epoch: 040, Train Acc: 91.4054, Train Loss: 0.2332, Valid Loss: 0.5535, Test Acc: 76.2611\n",
            "Epoch: 041, Train Acc: 91.7568, Train Loss: 0.2268, Valid Loss: 0.5252, Test Acc: 75.6677\n",
            "Epoch: 042, Train Acc: 91.6486, Train Loss: 0.2211, Valid Loss: 0.5267, Test Acc: 75.0742\n",
            "Epoch: 043, Train Acc: 92.0811, Train Loss: 0.2116, Valid Loss: 0.5371, Test Acc: 76.5579\n",
            "Epoch: 044, Train Acc: 92.3514, Train Loss: 0.2164, Valid Loss: 0.5577, Test Acc: 76.8546\n",
            "Epoch: 045, Train Acc: 92.4595, Train Loss: 0.2055, Valid Loss: 0.5414, Test Acc: 76.2611\n",
            "Epoch: 046, Train Acc: 93.3514, Train Loss: 0.1932, Valid Loss: 0.5572, Test Acc: 75.9644\n",
            "Epoch: 047, Train Acc: 92.5676, Train Loss: 0.1951, Valid Loss: 0.5710, Test Acc: 75.0742\n",
            "Epoch: 048, Train Acc: 93.2162, Train Loss: 0.1818, Valid Loss: 0.5633, Test Acc: 76.5579\n",
            "Epoch: 049, Train Acc: 93.8919, Train Loss: 0.1817, Valid Loss: 0.5717, Test Acc: 76.5579\n",
            "\n",
            "\n",
            "Best test accuracy:  77.1513353115727\n",
            "Best epoch:  50\n"
          ]
        }
      ],
      "source": [
        "#Model-specific arguments\n",
        "\n",
        "# Optimisation params\n",
        "args.lr=0.001\n",
        "args.weight_decay=0.0005\n",
        "\n",
        "# Model configuration\n",
        "args.d=3\n",
        "args.layers=4\n",
        "args.hidden_channels= 15\n",
        "args.input_dropout=0.0\n",
        "args.dropout=0.1\n",
        "#args.orth = \"householder\"   #choices=['matrix_exp', 'cayley', 'householder', 'euler'], parametrization for the orthogonal group\n",
        "args.edges_feat = \"bilinear\"    #choices=['none', 'concat', 'linear', 'bilinear']\n",
        "args.readout = \"mlp\"       #choices=['mean', 'sum', 'max', 'sag', 'mlp', 'concat']\n",
        "\n",
        "\n",
        "config=args.__dict__\n",
        "print(config)\n",
        "\n",
        "model = DiscreteDiagSheafDiffusion(config)\n",
        "#model = DiscreteDiagPoolSheafDiffusion(config)\n",
        "#model = DiscreteBundleSheafDiffusion(config)\n",
        "model = model.to(args.device)\n",
        "\n",
        "sheaf_learner_params, other_params = model.grouped_parameters()\n",
        "\n",
        "# Optimizers \n",
        "optimizer = torch.optim.Adam([\n",
        "    {'params': sheaf_learner_params, 'weight_decay': config['sheaf_decay']},\n",
        "    {'params': other_params, 'weight_decay': config['weight_decay']}\n",
        "], lr=config['lr'])\n",
        "\n",
        "\n",
        "# Training and test criterions\n",
        "criterion_train = torch.nn.CrossEntropyLoss()\n",
        "# In order not to make test loss change with batch_size \n",
        "criterion_test = torch.nn.CrossEntropyLoss(reduction = 'sum')\n",
        "\n",
        "best_test_acc = 0\n",
        "best_epoch = 0\n",
        "# Initial best loss value, used for the early stopping technique\n",
        "best_loss = 100.00\n",
        "patience = args.patience\n",
        "\n",
        "for epoch in range(1, args.epochs):\n",
        "    # Training epoch\n",
        "    train(model, optimizer, criterion_train, train_loader, args.device)\n",
        "\n",
        "\n",
        "    # Compute training, validation and test accuracy\n",
        "    train_acc, train_loss = test(model, criterion_test, train_loader, args.device)\n",
        "    valid_acc, valid_loss = test(model, criterion_test, valid_loader, args.device)\n",
        "    test_acc, test_loss = test(model, criterion_test, test_loader, args.device)\n",
        "\n",
        "    if best_test_acc < test_acc:\n",
        "        best_test_acc = test_acc\n",
        "        best_epoch = epoch\n",
        "\n",
        "    if valid_loss < best_loss:\n",
        "        best_loss = valid_loss\n",
        "        patience = args.patience\n",
        "    else:\n",
        "        patience -= 1\n",
        "        \n",
        "    if patience <= 0: \n",
        "        # Early stopping with patience\n",
        "        break \n",
        "\n",
        "    print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Train Loss: {train_loss:.4f}, Valid Loss: {valid_loss:.4f}, Test Acc: {test_acc:.4f}')\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"Best test accuracy: \", best_test_acc)\n",
        "print(\"Best epoch: \", best_epoch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jden4ZZLpdkC"
      },
      "source": [
        "#### Diagonal sheaf diffusion, edge-features handling through bilinear transform, **global SAG** readout "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iNCxlSvopdkD",
        "outputId": "c74ef6a3-8e20-4ace-950d-55096dc8704f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epochs': 200, 'lr': 0.02, 'weight_decay': 0.0005, 'sheaf_decay': 0.0005, 'patience': 15, 'second_linear': False, 'd': 3, 'layers': 4, 'normalised': True, 'deg_normalised': False, 'linear': False, 'hidden_channels': 15, 'input_dropout': 0.0, 'dropout': 0.1, 'left_weights': True, 'right_weights': True, 'add_lp': False, 'add_hp': False, 'use_act': True, 'sheaf_act': 'tanh', 'edge_weights': True, 'orth': 'householder', 'edges_feat': 'bilinear', 'readout': 'sag', 'dense_intermediate_dim': 256, 'dense_output_graph_dim': 128, 'output_nn_intermediate_dim': 64, 'set_transformer_k': 8, 'input_dim': 14, 'output_dim': 2, 'input_dim_edge': 3, 'max_num_nodes_in_graph': 417, 'device': device(type='cuda', index=0)}\n",
            "Epoch: 001, Train Acc: 59.9730, Train Loss: 0.6564, Valid Loss: 0.6774, Test Acc: 59.0504\n",
            "Epoch: 002, Train Acc: 58.8649, Train Loss: 0.6527, Valid Loss: 0.6715, Test Acc: 61.1276\n",
            "Epoch: 003, Train Acc: 67.9189, Train Loss: 0.5994, Valid Loss: 0.6179, Test Acc: 66.4688\n",
            "Epoch: 004, Train Acc: 72.4865, Train Loss: 0.5681, Valid Loss: 0.6239, Test Acc: 66.1721\n",
            "Epoch: 005, Train Acc: 73.4054, Train Loss: 0.5521, Valid Loss: 0.5946, Test Acc: 67.3591\n",
            "Epoch: 006, Train Acc: 74.4595, Train Loss: 0.5351, Valid Loss: 0.6000, Test Acc: 70.0297\n",
            "Epoch: 007, Train Acc: 75.5676, Train Loss: 0.5291, Valid Loss: 0.5800, Test Acc: 69.7329\n",
            "Epoch: 008, Train Acc: 74.4595, Train Loss: 0.5305, Valid Loss: 0.5740, Test Acc: 72.4036\n",
            "Epoch: 009, Train Acc: 75.5946, Train Loss: 0.5116, Valid Loss: 0.5410, Test Acc: 70.3264\n",
            "Epoch: 010, Train Acc: 74.4324, Train Loss: 0.5246, Valid Loss: 0.5763, Test Acc: 70.6231\n",
            "Epoch: 011, Train Acc: 75.4595, Train Loss: 0.5235, Valid Loss: 0.5561, Test Acc: 70.9199\n",
            "Epoch: 012, Train Acc: 75.8649, Train Loss: 0.5328, Valid Loss: 0.5573, Test Acc: 70.0297\n",
            "Epoch: 013, Train Acc: 74.6216, Train Loss: 0.5296, Valid Loss: 0.5829, Test Acc: 67.6558\n",
            "Epoch: 014, Train Acc: 75.5135, Train Loss: 0.5258, Valid Loss: 0.5687, Test Acc: 71.8101\n",
            "Epoch: 015, Train Acc: 75.7838, Train Loss: 0.5147, Valid Loss: 0.5483, Test Acc: 71.8101\n",
            "Epoch: 016, Train Acc: 76.0000, Train Loss: 0.5068, Valid Loss: 0.5269, Test Acc: 70.6231\n",
            "Epoch: 017, Train Acc: 74.0000, Train Loss: 0.5360, Valid Loss: 0.5717, Test Acc: 70.0297\n",
            "Epoch: 018, Train Acc: 76.7027, Train Loss: 0.5080, Valid Loss: 0.5527, Test Acc: 70.6231\n",
            "Epoch: 019, Train Acc: 75.9730, Train Loss: 0.5096, Valid Loss: 0.5352, Test Acc: 73.2938\n",
            "Epoch: 020, Train Acc: 76.4595, Train Loss: 0.5068, Valid Loss: 0.5471, Test Acc: 72.9970\n",
            "Epoch: 021, Train Acc: 73.5405, Train Loss: 0.5317, Valid Loss: 0.5607, Test Acc: 72.1068\n",
            "Epoch: 022, Train Acc: 75.6757, Train Loss: 0.5201, Valid Loss: 0.5305, Test Acc: 70.9199\n",
            "Epoch: 023, Train Acc: 76.2703, Train Loss: 0.5234, Valid Loss: 0.5407, Test Acc: 71.5134\n",
            "Epoch: 024, Train Acc: 76.5135, Train Loss: 0.4991, Valid Loss: 0.5345, Test Acc: 73.8872\n",
            "Epoch: 025, Train Acc: 77.2162, Train Loss: 0.5119, Valid Loss: 0.5464, Test Acc: 71.8101\n",
            "Epoch: 026, Train Acc: 75.3243, Train Loss: 0.5116, Valid Loss: 0.5511, Test Acc: 72.1068\n",
            "Epoch: 027, Train Acc: 76.5946, Train Loss: 0.5005, Valid Loss: 0.5560, Test Acc: 72.1068\n",
            "Epoch: 028, Train Acc: 76.7568, Train Loss: 0.4931, Valid Loss: 0.5244, Test Acc: 72.1068\n",
            "Epoch: 029, Train Acc: 76.2703, Train Loss: 0.4963, Valid Loss: 0.5537, Test Acc: 70.6231\n",
            "Epoch: 030, Train Acc: 76.6216, Train Loss: 0.4927, Valid Loss: 0.5583, Test Acc: 71.8101\n",
            "Epoch: 031, Train Acc: 76.6216, Train Loss: 0.5007, Valid Loss: 0.5617, Test Acc: 71.2166\n",
            "Epoch: 032, Train Acc: 75.5405, Train Loss: 0.5070, Valid Loss: 0.5393, Test Acc: 70.9199\n",
            "Epoch: 033, Train Acc: 75.7027, Train Loss: 0.5029, Valid Loss: 0.5515, Test Acc: 72.9970\n",
            "Epoch: 034, Train Acc: 76.6216, Train Loss: 0.4964, Valid Loss: 0.5509, Test Acc: 73.2938\n",
            "Epoch: 035, Train Acc: 76.9189, Train Loss: 0.4939, Valid Loss: 0.5384, Test Acc: 75.0742\n",
            "Epoch: 036, Train Acc: 76.2162, Train Loss: 0.5014, Valid Loss: 0.5422, Test Acc: 73.5905\n",
            "Epoch: 037, Train Acc: 77.7838, Train Loss: 0.4830, Valid Loss: 0.5104, Test Acc: 73.8872\n",
            "Epoch: 038, Train Acc: 76.7568, Train Loss: 0.5011, Valid Loss: 0.5461, Test Acc: 72.7003\n",
            "Epoch: 039, Train Acc: 77.0811, Train Loss: 0.4992, Valid Loss: 0.5371, Test Acc: 72.9970\n",
            "Epoch: 040, Train Acc: 75.7568, Train Loss: 0.5052, Valid Loss: 0.5523, Test Acc: 72.1068\n",
            "Epoch: 041, Train Acc: 76.5405, Train Loss: 0.4996, Valid Loss: 0.5409, Test Acc: 72.7003\n",
            "Epoch: 042, Train Acc: 77.5676, Train Loss: 0.4926, Valid Loss: 0.5356, Test Acc: 73.8872\n",
            "Epoch: 043, Train Acc: 74.8378, Train Loss: 0.5134, Valid Loss: 0.5940, Test Acc: 73.8872\n",
            "Epoch: 044, Train Acc: 77.9730, Train Loss: 0.4870, Valid Loss: 0.5347, Test Acc: 74.4807\n",
            "Epoch: 045, Train Acc: 76.9730, Train Loss: 0.4890, Valid Loss: 0.5448, Test Acc: 74.7774\n",
            "Epoch: 046, Train Acc: 76.8378, Train Loss: 0.4943, Valid Loss: 0.5504, Test Acc: 74.7774\n",
            "Epoch: 047, Train Acc: 78.1351, Train Loss: 0.4860, Valid Loss: 0.5553, Test Acc: 73.2938\n",
            "Epoch: 048, Train Acc: 76.0000, Train Loss: 0.4985, Valid Loss: 0.5449, Test Acc: 71.2166\n",
            "Epoch: 049, Train Acc: 77.6757, Train Loss: 0.4755, Valid Loss: 0.5500, Test Acc: 73.5905\n",
            "Epoch: 050, Train Acc: 77.5135, Train Loss: 0.4802, Valid Loss: 0.5426, Test Acc: 73.8872\n",
            "Epoch: 051, Train Acc: 74.8919, Train Loss: 0.5090, Valid Loss: 0.5721, Test Acc: 71.2166\n",
            "\n",
            "\n",
            "Best test accuracy:  75.07418397626114\n",
            "Best epoch:  35\n"
          ]
        }
      ],
      "source": [
        "#Model-specific arguments\n",
        "\n",
        "# Optimisation params\n",
        "args.lr=0.02\n",
        "args.weight_decay=0.0005\n",
        "\n",
        "# Model configuration\n",
        "args.d=3\n",
        "args.layers=4\n",
        "args.hidden_channels= 15\n",
        "args.input_dropout=0.0\n",
        "args.dropout=0.1\n",
        "#args.orth = \"householder\"   #choices=['matrix_exp', 'cayley', 'householder', 'euler'], parametrization for the orthogonal group\n",
        "args.edges_feat = \"bilinear\"    #choices=['none', 'concat', 'linear', 'bilinear']\n",
        "args.readout = \"sag\"       #choices=['mean', 'sum', 'max', 'sag', 'mlp', 'concat']\n",
        "\n",
        "\n",
        "config=args.__dict__\n",
        "print(config)\n",
        "\n",
        "model = DiscreteDiagSheafDiffusion(config)\n",
        "#model = DiscreteDiagPoolSheafDiffusion(config)\n",
        "#model = DiscreteBundleSheafDiffusion(config)\n",
        "model = model.to(args.device)\n",
        "\n",
        "sheaf_learner_params, other_params = model.grouped_parameters()\n",
        "\n",
        "# Optimizers \n",
        "optimizer = torch.optim.Adam([\n",
        "    {'params': sheaf_learner_params, 'weight_decay': config['sheaf_decay']},\n",
        "    {'params': other_params, 'weight_decay': config['weight_decay']}\n",
        "], lr=config['lr'])\n",
        "\n",
        "\n",
        "# Training and test criterions\n",
        "criterion_train = torch.nn.CrossEntropyLoss()\n",
        "# In order not to make test loss change with batch_size \n",
        "criterion_test = torch.nn.CrossEntropyLoss(reduction = 'sum')\n",
        "\n",
        "best_test_acc = 0\n",
        "best_epoch = 0\n",
        "# Initial best loss value, used for the early stopping technique\n",
        "best_loss = 100.00\n",
        "patience = args.patience\n",
        "\n",
        "for epoch in range(1, args.epochs):\n",
        "    # Training epoch\n",
        "    train(model, optimizer, criterion_train, train_loader, args.device)\n",
        "\n",
        "\n",
        "    # Compute training, validation and test accuracy\n",
        "    train_acc, train_loss = test(model, criterion_test, train_loader, args.device)\n",
        "    valid_acc, valid_loss = test(model, criterion_test, valid_loader, args.device)\n",
        "    test_acc, test_loss = test(model, criterion_test, test_loader, args.device)\n",
        "\n",
        "    if best_test_acc < test_acc:\n",
        "        best_test_acc = test_acc\n",
        "        best_epoch = epoch\n",
        "\n",
        "    if valid_loss < best_loss:\n",
        "        best_loss = valid_loss\n",
        "        patience = args.patience\n",
        "    else:\n",
        "        patience -= 1\n",
        "        \n",
        "    if patience <= 0: \n",
        "        # Early stopping with patience\n",
        "        break \n",
        "\n",
        "    print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Train Loss: {train_loss:.4f}, Valid Loss: {valid_loss:.4f}, Test Acc: {test_acc:.4f}')\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"Best test accuracy: \", best_test_acc)\n",
        "print(\"Best epoch: \", best_epoch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wX2pFDwepdkF"
      },
      "source": [
        "#### Diagonal sheaf diffusion, edge-features handling through bilinear transform, **hierarchical SAG** readout "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "akfpxI1OpdkF",
        "outputId": "c7735fe4-6dbe-41cf-bc9f-2e2bd193580b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epochs': 200, 'lr': 0.005, 'weight_decay': 0.0005, 'sheaf_decay': 0.0005, 'patience': 15, 'second_linear': False, 'd': 3, 'layers': 2, 'normalised': True, 'deg_normalised': False, 'linear': False, 'hidden_channels': 15, 'input_dropout': 0.0, 'dropout': 0.0, 'left_weights': True, 'right_weights': True, 'add_lp': False, 'add_hp': False, 'use_act': True, 'sheaf_act': 'tanh', 'edge_weights': True, 'orth': 'householder', 'edges_feat': 'bilinear', 'readout': 'sag', 'dense_intermediate_dim': 256, 'dense_output_graph_dim': 128, 'output_nn_intermediate_dim': 64, 'set_transformer_k': 8, 'input_dim': 14, 'output_dim': 2, 'input_dim_edge': 3, 'max_num_nodes_in_graph': 417, 'device': device(type='cuda', index=0)}\n",
            "Epoch: 001, Train Acc: 65.2432, Train Loss: 0.6438, Valid Loss: 0.6474, Test Acc: 59.6439\n",
            "Epoch: 002, Train Acc: 63.1351, Train Loss: 0.6265, Valid Loss: 0.6253, Test Acc: 64.0950\n",
            "Epoch: 003, Train Acc: 66.4865, Train Loss: 0.6116, Valid Loss: 0.6301, Test Acc: 62.0178\n",
            "Epoch: 004, Train Acc: 65.2432, Train Loss: 0.6108, Valid Loss: 0.6310, Test Acc: 65.8754\n",
            "Epoch: 005, Train Acc: 66.3784, Train Loss: 0.5988, Valid Loss: 0.6181, Test Acc: 65.5786\n",
            "Epoch: 006, Train Acc: 67.6486, Train Loss: 0.5960, Valid Loss: 0.6180, Test Acc: 63.7982\n",
            "Epoch: 007, Train Acc: 68.3514, Train Loss: 0.5846, Valid Loss: 0.6132, Test Acc: 65.2819\n",
            "Epoch: 008, Train Acc: 66.2703, Train Loss: 0.5948, Valid Loss: 0.6062, Test Acc: 66.1721\n",
            "Epoch: 009, Train Acc: 69.5135, Train Loss: 0.5848, Valid Loss: 0.6062, Test Acc: 65.8754\n",
            "Epoch: 010, Train Acc: 69.2162, Train Loss: 0.5787, Valid Loss: 0.5998, Test Acc: 67.6558\n",
            "Epoch: 011, Train Acc: 70.3514, Train Loss: 0.5719, Valid Loss: 0.5913, Test Acc: 67.9525\n",
            "Epoch: 012, Train Acc: 70.3243, Train Loss: 0.5779, Valid Loss: 0.6054, Test Acc: 67.9525\n",
            "Epoch: 013, Train Acc: 71.6486, Train Loss: 0.5699, Valid Loss: 0.5855, Test Acc: 67.9525\n",
            "Epoch: 014, Train Acc: 68.5135, Train Loss: 0.5766, Valid Loss: 0.5811, Test Acc: 68.2493\n",
            "Epoch: 015, Train Acc: 69.4595, Train Loss: 0.5692, Valid Loss: 0.5803, Test Acc: 67.9525\n",
            "Epoch: 016, Train Acc: 72.3784, Train Loss: 0.5695, Valid Loss: 0.5799, Test Acc: 70.6231\n",
            "Epoch: 017, Train Acc: 70.2703, Train Loss: 0.5626, Valid Loss: 0.5644, Test Acc: 69.1395\n",
            "Epoch: 018, Train Acc: 71.3514, Train Loss: 0.5680, Valid Loss: 0.5715, Test Acc: 68.2493\n",
            "Epoch: 019, Train Acc: 72.6486, Train Loss: 0.5585, Valid Loss: 0.5631, Test Acc: 71.5134\n",
            "Epoch: 020, Train Acc: 71.2973, Train Loss: 0.5561, Valid Loss: 0.5618, Test Acc: 69.7329\n",
            "Epoch: 021, Train Acc: 71.6757, Train Loss: 0.5661, Valid Loss: 0.5828, Test Acc: 68.5460\n",
            "Epoch: 022, Train Acc: 72.8649, Train Loss: 0.5533, Valid Loss: 0.5709, Test Acc: 68.8427\n",
            "Epoch: 023, Train Acc: 72.9459, Train Loss: 0.5535, Valid Loss: 0.5691, Test Acc: 70.6231\n",
            "Epoch: 024, Train Acc: 72.8649, Train Loss: 0.5625, Valid Loss: 0.5780, Test Acc: 71.5134\n",
            "Epoch: 025, Train Acc: 71.4865, Train Loss: 0.5594, Valid Loss: 0.5743, Test Acc: 67.9525\n",
            "Epoch: 026, Train Acc: 73.4865, Train Loss: 0.5435, Valid Loss: 0.5672, Test Acc: 72.4036\n",
            "Epoch: 027, Train Acc: 73.5676, Train Loss: 0.5444, Valid Loss: 0.5570, Test Acc: 74.4807\n",
            "Epoch: 028, Train Acc: 70.8108, Train Loss: 0.5628, Valid Loss: 0.5768, Test Acc: 67.9525\n",
            "Epoch: 029, Train Acc: 73.5676, Train Loss: 0.5497, Valid Loss: 0.5718, Test Acc: 72.4036\n",
            "Epoch: 030, Train Acc: 73.5946, Train Loss: 0.5349, Valid Loss: 0.5577, Test Acc: 72.7003\n",
            "Epoch: 031, Train Acc: 73.4595, Train Loss: 0.5349, Valid Loss: 0.5510, Test Acc: 71.2166\n",
            "Epoch: 032, Train Acc: 74.1892, Train Loss: 0.5351, Valid Loss: 0.5656, Test Acc: 72.1068\n",
            "Epoch: 033, Train Acc: 73.5676, Train Loss: 0.5311, Valid Loss: 0.5446, Test Acc: 70.3264\n",
            "Epoch: 034, Train Acc: 74.5405, Train Loss: 0.5259, Valid Loss: 0.5650, Test Acc: 73.2938\n",
            "Epoch: 035, Train Acc: 73.4054, Train Loss: 0.5404, Valid Loss: 0.5451, Test Acc: 70.9199\n",
            "Epoch: 036, Train Acc: 69.6486, Train Loss: 0.5577, Valid Loss: 0.5918, Test Acc: 67.3591\n",
            "Epoch: 037, Train Acc: 74.1622, Train Loss: 0.5286, Valid Loss: 0.5547, Test Acc: 70.6231\n",
            "Epoch: 038, Train Acc: 74.1622, Train Loss: 0.5258, Valid Loss: 0.5532, Test Acc: 73.2938\n",
            "Epoch: 039, Train Acc: 71.3784, Train Loss: 0.5448, Valid Loss: 0.5785, Test Acc: 69.1395\n",
            "Epoch: 040, Train Acc: 73.2973, Train Loss: 0.5461, Valid Loss: 0.5671, Test Acc: 70.6231\n",
            "Epoch: 041, Train Acc: 74.7027, Train Loss: 0.5178, Valid Loss: 0.5510, Test Acc: 73.8872\n",
            "Epoch: 042, Train Acc: 72.4865, Train Loss: 0.5414, Valid Loss: 0.5567, Test Acc: 69.4362\n",
            "Epoch: 043, Train Acc: 73.7568, Train Loss: 0.5382, Valid Loss: 0.5647, Test Acc: 72.4036\n",
            "Epoch: 044, Train Acc: 73.5135, Train Loss: 0.5359, Valid Loss: 0.5679, Test Acc: 74.7774\n",
            "Epoch: 045, Train Acc: 72.7027, Train Loss: 0.5475, Valid Loss: 0.5661, Test Acc: 71.8101\n",
            "Epoch: 046, Train Acc: 72.8378, Train Loss: 0.5423, Valid Loss: 0.5437, Test Acc: 72.9970\n",
            "Epoch: 047, Train Acc: 72.4595, Train Loss: 0.5410, Valid Loss: 0.5568, Test Acc: 71.8101\n",
            "Epoch: 048, Train Acc: 71.7297, Train Loss: 0.5449, Valid Loss: 0.5728, Test Acc: 71.5134\n",
            "Epoch: 049, Train Acc: 73.5135, Train Loss: 0.5295, Valid Loss: 0.5498, Test Acc: 72.7003\n",
            "Epoch: 050, Train Acc: 74.0270, Train Loss: 0.5315, Valid Loss: 0.5583, Test Acc: 71.5134\n",
            "Epoch: 051, Train Acc: 75.4595, Train Loss: 0.5138, Valid Loss: 0.5657, Test Acc: 74.4807\n",
            "Epoch: 052, Train Acc: 73.1892, Train Loss: 0.5401, Valid Loss: 0.5586, Test Acc: 72.1068\n",
            "Epoch: 053, Train Acc: 73.7568, Train Loss: 0.5308, Valid Loss: 0.5768, Test Acc: 69.4362\n",
            "Epoch: 054, Train Acc: 73.0000, Train Loss: 0.5233, Valid Loss: 0.5499, Test Acc: 71.8101\n",
            "Epoch: 055, Train Acc: 74.1622, Train Loss: 0.5264, Valid Loss: 0.5505, Test Acc: 72.4036\n",
            "Epoch: 056, Train Acc: 73.7568, Train Loss: 0.5167, Valid Loss: 0.5547, Test Acc: 72.1068\n",
            "Epoch: 057, Train Acc: 71.9730, Train Loss: 0.5353, Valid Loss: 0.5524, Test Acc: 70.6231\n",
            "Epoch: 058, Train Acc: 75.2432, Train Loss: 0.5235, Valid Loss: 0.5515, Test Acc: 74.1840\n",
            "Epoch: 059, Train Acc: 73.6757, Train Loss: 0.5142, Valid Loss: 0.5524, Test Acc: 76.5579\n",
            "Epoch: 060, Train Acc: 71.6486, Train Loss: 0.5709, Valid Loss: 0.5906, Test Acc: 67.3591\n",
            "Epoch: 061, Train Acc: 73.7838, Train Loss: 0.5284, Valid Loss: 0.5295, Test Acc: 75.6677\n",
            "Epoch: 062, Train Acc: 73.2703, Train Loss: 0.5335, Valid Loss: 0.5721, Test Acc: 69.4362\n",
            "Epoch: 063, Train Acc: 74.0541, Train Loss: 0.5158, Valid Loss: 0.5508, Test Acc: 72.7003\n",
            "Epoch: 064, Train Acc: 74.4054, Train Loss: 0.5307, Valid Loss: 0.5617, Test Acc: 71.8101\n",
            "Epoch: 065, Train Acc: 72.4054, Train Loss: 0.5481, Valid Loss: 0.5549, Test Acc: 69.4362\n",
            "Epoch: 066, Train Acc: 75.3784, Train Loss: 0.5073, Valid Loss: 0.5529, Test Acc: 70.0297\n",
            "Epoch: 067, Train Acc: 74.2432, Train Loss: 0.5212, Valid Loss: 0.5571, Test Acc: 72.9970\n",
            "Epoch: 068, Train Acc: 73.7568, Train Loss: 0.5284, Valid Loss: 0.5453, Test Acc: 72.4036\n",
            "Epoch: 069, Train Acc: 74.7027, Train Loss: 0.5163, Valid Loss: 0.5477, Test Acc: 71.5134\n",
            "Epoch: 070, Train Acc: 72.9730, Train Loss: 0.5210, Valid Loss: 0.5463, Test Acc: 70.9199\n",
            "Epoch: 071, Train Acc: 74.9189, Train Loss: 0.5153, Valid Loss: 0.5446, Test Acc: 72.7003\n",
            "Epoch: 072, Train Acc: 72.0000, Train Loss: 0.5467, Valid Loss: 0.5670, Test Acc: 70.6231\n",
            "Epoch: 073, Train Acc: 75.7027, Train Loss: 0.5063, Valid Loss: 0.5198, Test Acc: 73.8872\n",
            "Epoch: 074, Train Acc: 74.6486, Train Loss: 0.4996, Valid Loss: 0.5399, Test Acc: 72.7003\n",
            "Epoch: 075, Train Acc: 75.4595, Train Loss: 0.5084, Valid Loss: 0.5492, Test Acc: 72.1068\n",
            "Epoch: 076, Train Acc: 74.5946, Train Loss: 0.5153, Valid Loss: 0.5380, Test Acc: 72.9970\n",
            "Epoch: 077, Train Acc: 72.7297, Train Loss: 0.5431, Valid Loss: 0.5491, Test Acc: 68.2493\n",
            "Epoch: 078, Train Acc: 75.4324, Train Loss: 0.5115, Valid Loss: 0.5343, Test Acc: 70.0297\n",
            "Epoch: 079, Train Acc: 74.7297, Train Loss: 0.5217, Valid Loss: 0.5456, Test Acc: 70.0297\n",
            "Epoch: 080, Train Acc: 71.0000, Train Loss: 0.5425, Valid Loss: 0.5718, Test Acc: 70.6231\n",
            "Epoch: 081, Train Acc: 74.6486, Train Loss: 0.5189, Valid Loss: 0.5693, Test Acc: 73.5905\n",
            "Epoch: 082, Train Acc: 74.2973, Train Loss: 0.5326, Valid Loss: 0.5555, Test Acc: 72.4036\n",
            "Epoch: 083, Train Acc: 74.4054, Train Loss: 0.5267, Valid Loss: 0.5476, Test Acc: 69.7329\n",
            "Epoch: 084, Train Acc: 73.5676, Train Loss: 0.5181, Valid Loss: 0.5087, Test Acc: 72.7003\n",
            "Epoch: 085, Train Acc: 75.4865, Train Loss: 0.5072, Valid Loss: 0.5529, Test Acc: 72.4036\n",
            "Epoch: 086, Train Acc: 72.9459, Train Loss: 0.5295, Valid Loss: 0.5426, Test Acc: 70.6231\n",
            "Epoch: 087, Train Acc: 75.3784, Train Loss: 0.5188, Valid Loss: 0.5516, Test Acc: 72.9970\n",
            "Epoch: 088, Train Acc: 74.7297, Train Loss: 0.5025, Valid Loss: 0.5371, Test Acc: 71.8101\n",
            "Epoch: 089, Train Acc: 74.2162, Train Loss: 0.5333, Valid Loss: 0.5464, Test Acc: 70.6231\n",
            "Epoch: 090, Train Acc: 73.5676, Train Loss: 0.5305, Valid Loss: 0.5446, Test Acc: 72.7003\n",
            "Epoch: 091, Train Acc: 75.3514, Train Loss: 0.5019, Valid Loss: 0.5267, Test Acc: 74.1840\n",
            "Epoch: 092, Train Acc: 74.3514, Train Loss: 0.5167, Valid Loss: 0.5305, Test Acc: 70.6231\n",
            "Epoch: 093, Train Acc: 74.4054, Train Loss: 0.5231, Valid Loss: 0.5294, Test Acc: 73.2938\n",
            "Epoch: 094, Train Acc: 74.2162, Train Loss: 0.5268, Valid Loss: 0.5374, Test Acc: 72.4036\n",
            "Epoch: 095, Train Acc: 75.8919, Train Loss: 0.4972, Valid Loss: 0.5556, Test Acc: 71.8101\n",
            "Epoch: 096, Train Acc: 75.7568, Train Loss: 0.5084, Valid Loss: 0.5412, Test Acc: 73.2938\n",
            "Epoch: 097, Train Acc: 75.2432, Train Loss: 0.4973, Valid Loss: 0.5331, Test Acc: 72.7003\n",
            "Epoch: 098, Train Acc: 75.6757, Train Loss: 0.4923, Valid Loss: 0.5343, Test Acc: 72.9970\n",
            "\n",
            "\n",
            "Best test accuracy:  76.55786350148368\n",
            "Best epoch:  59\n"
          ]
        }
      ],
      "source": [
        "#Model-specific arguments\n",
        "\n",
        "# Optimisation params\n",
        "args.lr=0.005\n",
        "args.weight_decay=0.0005\n",
        "\n",
        "# Model configuration\n",
        "args.d=3\n",
        "args.layers=2\n",
        "args.hidden_channels= 15\n",
        "args.input_dropout=0.0\n",
        "args.dropout=0.0\n",
        "#args.orth = \"householder\"   #choices=['matrix_exp', 'cayley', 'householder', 'euler'], parametrization for the orthogonal group\n",
        "args.edges_feat = \"bilinear\"    #choices=['none', 'concat', 'linear', 'bilinear']\n",
        "#args.readout = \"sag\"       #choices=['mean', 'sum', 'max', 'sag', 'mlp', 'concat']\n",
        "\n",
        "\n",
        "config=args.__dict__\n",
        "print(config)\n",
        "\n",
        "#model = DiscreteDiagSheafDiffusion(config)\n",
        "model = DiscreteDiagPoolSheafDiffusion(config)\n",
        "#model = DiscreteBundleSheafDiffusion(config)\n",
        "model = model.to(args.device)\n",
        "\n",
        "sheaf_learner_params, other_params = model.grouped_parameters()\n",
        "\n",
        "# Optimizers \n",
        "optimizer = torch.optim.Adam([\n",
        "    {'params': sheaf_learner_params, 'weight_decay': config['sheaf_decay']},\n",
        "    {'params': other_params, 'weight_decay': config['weight_decay']}\n",
        "], lr=config['lr'])\n",
        "\n",
        "\n",
        "# Training and test criterions\n",
        "criterion_train = torch.nn.CrossEntropyLoss()\n",
        "# In order not to make test loss change with batch_size \n",
        "criterion_test = torch.nn.CrossEntropyLoss(reduction = 'sum')\n",
        "\n",
        "best_test_acc = 0\n",
        "best_epoch = 0\n",
        "# Initial best loss value, used for the early stopping technique\n",
        "best_loss = 100.00\n",
        "patience = args.patience\n",
        "\n",
        "for epoch in range(1, args.epochs):\n",
        "    # Training epoch\n",
        "    train(model, optimizer, criterion_train, train_loader, args.device)\n",
        "\n",
        "\n",
        "    # Compute training, validation and test accuracy\n",
        "    train_acc, train_loss = test(model, criterion_test, train_loader, args.device)\n",
        "    valid_acc, valid_loss = test(model, criterion_test, valid_loader, args.device)\n",
        "    test_acc, test_loss = test(model, criterion_test, test_loader, args.device)\n",
        "\n",
        "    if best_test_acc < test_acc:\n",
        "        best_test_acc = test_acc\n",
        "        best_epoch = epoch\n",
        "\n",
        "    if valid_loss < best_loss:\n",
        "        best_loss = valid_loss\n",
        "        patience = args.patience\n",
        "    else:\n",
        "        patience -= 1\n",
        "        \n",
        "    if patience <= 0: \n",
        "        # Early stopping with patience\n",
        "        break \n",
        "\n",
        "    print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Train Loss: {train_loss:.4f}, Valid Loss: {valid_loss:.4f}, Test Acc: {test_acc:.4f}')\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"Best test accuracy: \", best_test_acc)\n",
        "print(\"Best epoch: \", best_epoch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3_-Vg6oxuXu"
      },
      "source": [
        "# Benchmarks: SOTA for Graph Classification\n",
        "\n",
        "References:\n",
        "- https://paperswithcode.com/sota/graph-classification-on-enzymes\n",
        "- https://paperswithcode.com/sota/graph-classification-on-mutagenicity \n",
        "- https://paperswithcode.com/paper/hierarchical-graph-pooling-with-structure\n",
        "- https://github.com/qslim/gnn-spectrum/tree/main/tu\n",
        "- https://github.com/cszhangzhen/HGP-SL "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVdR3UoB_GoA"
      },
      "source": [
        "## HGP-SL: Hierarchical Graph Pooling with Structure Learning"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Zrn2u_fUElYi"
      },
      "source": [
        "### Downloads and imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mLkJbI6f_UuY",
        "outputId": "fdf95126-dfae-4963-c497-119f15801dd2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'dgl'...\n",
            "remote: Enumerating objects: 37060, done.\u001b[K\n",
            "remote: Counting objects: 100% (54/54), done.\u001b[K\n",
            "remote: Compressing objects: 100% (50/50), done.\u001b[K\n",
            "remote: Total 37060 (delta 11), reused 20 (delta 4), pack-reused 37006\u001b[K\n",
            "Receiving objects: 100% (37060/37060), 20.92 MiB | 20.74 MiB/s, done.\n",
            "Resolving deltas: 100% (24446/24446), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/dmlc/dgl.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pAIZVybFAg7L",
        "outputId": "1871e974-43dd-490a-da44-dbf43dc5d482"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/dgl\n",
            "/content/dgl/examples\n",
            "/content/dgl/examples/pytorch\n",
            "/content/dgl/examples/pytorch/hgp_sl\n",
            "functions.py  layers.py  main.py  networks.py  README.md  utils.py\n"
          ]
        }
      ],
      "source": [
        "%cd dgl\n",
        "%cd examples\n",
        "%cd pytorch\n",
        "%cd hgp_sl/\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sriUSwhHMScx"
      },
      "outputs": [],
      "source": [
        "!pip install dgl-cu113 dglgo -f https://data.dgl.ai/wheels/repo.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKjCtAbYg3sC"
      },
      "source": [
        "### ENZYMES (HGP-SL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DsloSUxkoki5",
        "outputId": "672c1ab0-24b7-4c6d-8e33-d7bfacac8d4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trial 1/1\n",
            "Downloading ./dataset/ENZYMES.zip from https://www.chrsmrrs.com/graphkerneldatasets/ENZYMES.zip...\n",
            "Extracting file to ./dataset/ENZYMES\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Epoch 10: loss=1.7064, val_acc=0.2167, final_test_acc=0.2333\n",
            "Epoch 20: loss=1.6623, val_acc=0.3167, final_test_acc=0.3167\n",
            "Epoch 30: loss=1.6423, val_acc=0.3333, final_test_acc=0.3167\n",
            "Epoch 40: loss=1.5955, val_acc=0.2833, final_test_acc=0.3167\n",
            "Epoch 50: loss=1.5488, val_acc=0.2667, final_test_acc=0.3167\n",
            "Epoch 60: loss=1.5139, val_acc=0.3500, final_test_acc=0.3167\n",
            "Epoch 70: loss=1.4553, val_acc=0.3000, final_test_acc=0.3500\n",
            "Epoch 80: loss=1.4471, val_acc=0.3333, final_test_acc=0.4167\n",
            "Epoch 90: loss=1.3703, val_acc=0.4667, final_test_acc=0.4500\n",
            "Epoch 100: loss=1.3047, val_acc=0.4667, final_test_acc=0.4333\n",
            "Epoch 110: loss=1.2902, val_acc=0.5000, final_test_acc=0.4667\n",
            "Epoch 120: loss=1.2161, val_acc=0.2833, final_test_acc=0.4667\n",
            "Epoch 130: loss=1.2111, val_acc=0.3667, final_test_acc=0.4167\n",
            "Epoch 140: loss=1.1071, val_acc=0.5500, final_test_acc=0.4833\n",
            "Epoch 150: loss=1.0761, val_acc=0.5500, final_test_acc=0.5000\n",
            "Epoch 160: loss=1.0795, val_acc=0.5667, final_test_acc=0.5000\n",
            "Epoch 170: loss=1.0275, val_acc=0.4833, final_test_acc=0.5000\n",
            "Epoch 180: loss=0.9835, val_acc=0.5167, final_test_acc=0.5167\n",
            "Epoch 190: loss=0.9982, val_acc=0.5167, final_test_acc=0.5167\n",
            "Epoch 200: loss=0.9129, val_acc=0.6333, final_test_acc=0.4833\n",
            "Epoch 210: loss=0.8302, val_acc=0.5667, final_test_acc=0.4833\n",
            "Epoch 220: loss=0.8188, val_acc=0.5833, final_test_acc=0.4833\n",
            "Epoch 230: loss=0.7255, val_acc=0.5167, final_test_acc=0.4833\n",
            "Epoch 240: loss=0.7484, val_acc=0.5833, final_test_acc=0.4833\n",
            "Epoch 250: loss=0.7910, val_acc=0.5833, final_test_acc=0.4833\n",
            "Epoch 260: loss=0.7171, val_acc=0.5667, final_test_acc=0.4833\n",
            "Epoch 270: loss=0.6254, val_acc=0.5500, final_test_acc=0.4833\n",
            "Epoch 280: loss=0.5811, val_acc=0.5667, final_test_acc=0.4833\n",
            "Epoch 290: loss=0.5711, val_acc=0.5500, final_test_acc=0.4833\n",
            "Best Epoch 197, final test acc 0.4833\n",
            "mean acc: 0.4833, error bound: nan\n"
          ]
        }
      ],
      "source": [
        "!python main.py --device 0 --dataset ENZYMES --lr 0.001 --batch_size 128 --pool_ratio 0.8 --dropout 0.0 --conv_layers 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxLH-riEgriQ"
      },
      "source": [
        "### Mutagenicity (HGP-SL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GVSJSoz_BUmW",
        "outputId": "190052cb-62b8-4fa4-c273-ceb9bfe70a2f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DGL backend not selected or invalid.  Assuming PyTorch for now.\n",
            "Setting the default backend to \"pytorch\". You can change it in the ~/.dgl/config.json file or export the DGLBACKEND environment variable.  Valid options are: pytorch, mxnet, tensorflow (all lowercase)\n",
            "Trial 1/1\n",
            "Downloading ./dataset/Mutagenicity.zip from https://www.chrsmrrs.com/graphkerneldatasets/Mutagenicity.zip...\n",
            "Extracting file to ./dataset/Mutagenicity\n",
            "No Node Attribute Data\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Epoch 10: loss=0.5848, val_acc=0.7159, final_test_acc=0.6253\n",
            "Epoch 20: loss=0.5566, val_acc=0.7344, final_test_acc=0.6529\n",
            "Epoch 30: loss=0.5433, val_acc=0.7390, final_test_acc=0.6920\n",
            "Epoch 40: loss=0.5248, val_acc=0.7575, final_test_acc=0.7080\n",
            "Epoch 50: loss=0.5177, val_acc=0.7644, final_test_acc=0.7149\n",
            "Epoch 60: loss=0.4922, val_acc=0.7806, final_test_acc=0.7218\n",
            "Epoch 70: loss=0.4845, val_acc=0.7737, final_test_acc=0.7471\n",
            "Epoch 80: loss=0.4833, val_acc=0.7945, final_test_acc=0.7494\n",
            "Epoch 90: loss=0.4784, val_acc=0.7921, final_test_acc=0.7517\n",
            "Epoch 100: loss=0.4605, val_acc=0.7945, final_test_acc=0.7701\n",
            "Epoch 110: loss=0.4566, val_acc=0.7691, final_test_acc=0.7609\n",
            "Epoch 120: loss=0.4443, val_acc=0.7760, final_test_acc=0.7678\n",
            "Epoch 130: loss=0.4272, val_acc=0.7598, final_test_acc=0.7862\n",
            "Epoch 140: loss=0.4297, val_acc=0.7875, final_test_acc=0.7839\n",
            "Epoch 150: loss=0.4252, val_acc=0.7667, final_test_acc=0.7793\n",
            "Epoch 160: loss=0.4312, val_acc=0.7921, final_test_acc=0.7793\n",
            "Epoch 170: loss=0.4225, val_acc=0.7968, final_test_acc=0.7862\n",
            "Epoch 180: loss=0.4033, val_acc=0.7875, final_test_acc=0.7885\n",
            "Epoch 190: loss=0.3963, val_acc=0.8037, final_test_acc=0.7954\n",
            "Epoch 200: loss=0.3987, val_acc=0.8014, final_test_acc=0.7816\n",
            "Epoch 210: loss=0.3877, val_acc=0.7945, final_test_acc=0.7816\n",
            "Epoch 220: loss=0.3827, val_acc=0.8060, final_test_acc=0.7885\n",
            "Epoch 230: loss=0.3876, val_acc=0.7921, final_test_acc=0.7862\n",
            "Epoch 240: loss=0.3903, val_acc=0.7852, final_test_acc=0.7862\n",
            "Epoch 250: loss=0.3799, val_acc=0.8176, final_test_acc=0.7862\n",
            "Epoch 260: loss=0.3759, val_acc=0.8129, final_test_acc=0.7862\n",
            "Epoch 270: loss=0.3695, val_acc=0.8060, final_test_acc=0.7862\n",
            "Epoch 280: loss=0.3563, val_acc=0.8083, final_test_acc=0.7862\n",
            "Epoch 290: loss=0.3588, val_acc=0.8129, final_test_acc=0.7862\n",
            "Epoch 300: loss=0.3536, val_acc=0.8106, final_test_acc=0.7862\n",
            "Epoch 310: loss=0.3384, val_acc=0.8176, final_test_acc=0.7862\n",
            "Epoch 320: loss=0.3415, val_acc=0.8152, final_test_acc=0.8184\n",
            "Epoch 330: loss=0.3727, val_acc=0.7667, final_test_acc=0.8046\n",
            "Epoch 340: loss=0.3326, val_acc=0.8129, final_test_acc=0.7954\n",
            "Epoch 350: loss=0.3325, val_acc=0.8152, final_test_acc=0.7954\n",
            "Epoch 360: loss=0.3462, val_acc=0.8129, final_test_acc=0.7954\n",
            "Epoch 370: loss=0.3469, val_acc=0.7829, final_test_acc=0.7954\n",
            "Epoch 380: loss=0.3270, val_acc=0.8129, final_test_acc=0.7954\n",
            "Epoch 390: loss=0.3336, val_acc=0.8152, final_test_acc=0.7954\n",
            "Epoch 400: loss=0.3256, val_acc=0.7921, final_test_acc=0.7954\n",
            "Epoch 410: loss=0.3392, val_acc=0.7921, final_test_acc=0.7954\n",
            "Epoch 420: loss=0.3229, val_acc=0.8152, final_test_acc=0.7954\n",
            "Epoch 430: loss=0.3140, val_acc=0.7991, final_test_acc=0.7954\n",
            "Best Epoch 332, final test acc 0.7954\n",
            "mean acc: 0.7954, error bound: nan\n"
          ]
        }
      ],
      "source": [
        "!python main.py --device 0 --dataset Mutagenicity --lr 0.001 --batch_size 512 --pool_ratio 0.8 --dropout 0.0 --conv_layers 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZJY5Mq8nR99"
      },
      "source": [
        "## Norm-GN: A New Perspective on the Effects of Spectrum in Graph Neural Networks"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_LUcsqWO1QVs"
      },
      "source": [
        "### Downloads and imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9cA1zT-2nR-A",
        "outputId": "d7c944e2-cf1a-4a3e-bc02-7d9e3cf045d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'gnn-spectrum'...\n",
            "remote: Enumerating objects: 38, done.\u001b[K\n",
            "remote: Counting objects: 100% (38/38), done.\u001b[K\n",
            "remote: Compressing objects: 100% (29/29), done.\u001b[K\n",
            "remote: Total 38 (delta 8), reused 38 (delta 8), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (38/38), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/qslim/gnn-spectrum.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_6OvPEyELKki"
      },
      "outputs": [],
      "source": [
        "!pip install dgl-cu113 dglgo -f https://data.dgl.ai/wheels/repo.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zrgPqmavn1Bi"
      },
      "outputs": [],
      "source": [
        "!pip install ogb==1.3.1\n",
        "!pip install numpy\n",
        "!pip install easydict\n",
        "!pip install tensorboard\n",
        "!pip install tqdm\n",
        "!pip install json5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RC8FEwP4ofwk",
        "outputId": "b98ef286-109a-4b47-a1fa-9a2ebeace772"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "gnn-spectrum  sample_data\n",
            "/content/gnn-spectrum/tu\n",
            "configs  main.py  model.py  run_script.sh  tu_dataloader.py\n"
          ]
        }
      ],
      "source": [
        "%cd gnn-spectrum/tu\n",
        "!ls"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "IGL2Xrsd1VtP"
      },
      "source": [
        "### ENZYMES (Norm-GN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tjTDorSpo9XJ",
        "outputId": "91002e25-1aba-4384-b8ab-370cd2f6b929"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mOutput streaming troncato alle ultime 5000 righe.\u001b[0m\n",
            "Epoch: 078, Train Loss: 0.0064897, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 079, Train Loss: 0.0062109, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 080, Train Loss: 0.0061906, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 081, Train Loss: 0.0056912, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 082, Train Loss: 0.0051489, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 083, Train Loss: 0.0048548, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 084, Train Loss: 0.0052254, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 085, Train Loss: 0.0049430, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 086, Train Loss: 0.0043917, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 087, Train Loss: 0.0052839, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 088, Train Loss: 0.0069308, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 089, Train Loss: 0.0049746, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 090, Train Loss: 0.0042355, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 091, Train Loss: 0.0040328, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 092, Train Loss: 0.0034789, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 093, Train Loss: 0.0037272, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 094, Train Loss: 0.0036071, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 095, Train Loss: 0.0034782, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 096, Train Loss: 0.0034010, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 097, Train Loss: 0.0032144, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 098, Train Loss: 0.0028831, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 099, Train Loss: 0.0031322, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 100, Train Loss: 0.0029590, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 101, Train Loss: 0.0028218, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 102, Train Loss: 0.0027230, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 103, Train Loss: 0.0026568, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 104, Train Loss: 0.0026946, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 105, Train Loss: 0.0027023, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 106, Train Loss: 0.0024766, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 107, Train Loss: 0.0023469, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 108, Train Loss: 0.0023581, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 109, Train Loss: 0.0025578, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 110, Train Loss: 0.0028284, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 111, Train Loss: 0.0023237, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 112, Train Loss: 0.0024591, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 113, Train Loss: 0.0021817, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 114, Train Loss: 0.0020257, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 115, Train Loss: 0.0020999, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 116, Train Loss: 0.0020842, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 117, Train Loss: 0.0021046, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 118, Train Loss: 0.0020928, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 119, Train Loss: 0.0019896, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 120, Train Loss: 0.0024186, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 121, Train Loss: 0.0020283, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 122, Train Loss: 0.0020776, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 123, Train Loss: 0.0018910, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 124, Train Loss: 0.0018370, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 125, Train Loss: 0.0020022, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 126, Train Loss: 0.0021543, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 127, Train Loss: 0.0019744, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 128, Train Loss: 0.0019414, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 129, Train Loss: 0.0018669, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 130, Train Loss: 0.0016257, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 131, Train Loss: 0.0017378, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 132, Train Loss: 0.0016652, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 133, Train Loss: 0.0017319, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 134, Train Loss: 0.0017149, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 135, Train Loss: 0.0015735, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 136, Train Loss: 0.0017704, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 137, Train Loss: 0.0016319, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 138, Train Loss: 0.0020842, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 139, Train Loss: 0.0017541, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 140, Train Loss: 0.0015625, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 141, Train Loss: 0.0016423, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 142, Train Loss: 0.0015759, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 143, Train Loss: 0.0014757, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 144, Train Loss: 0.0014006, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 145, Train Loss: 0.0015553, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 146, Train Loss: 0.0014127, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 147, Train Loss: 0.0014458, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 148, Train Loss: 0.0014203, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 149, Train Loss: 0.0014134, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 150, Train Loss: 0.0013128, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 151, Train Loss: 0.0012563, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 152, Train Loss: 0.0012372, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 153, Train Loss: 0.0012783, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 154, Train Loss: 0.0012329, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 155, Train Loss: 0.0012279, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 156, Train Loss: 0.0011906, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 157, Train Loss: 0.0011905, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 158, Train Loss: 0.0011979, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 159, Train Loss: 0.0013044, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 160, Train Loss: 0.0011954, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 161, Train Loss: 0.0012358, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 162, Train Loss: 0.0012262, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 163, Train Loss: 0.0011975, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 164, Train Loss: 0.0011542, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 165, Train Loss: 0.0010728, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 166, Train Loss: 0.0010904, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 167, Train Loss: 0.0010854, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 168, Train Loss: 0.0011542, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 169, Train Loss: 0.0011563, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 170, Train Loss: 0.0010639, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 171, Train Loss: 0.0010571, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 172, Train Loss: 0.0010730, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 173, Train Loss: 0.0010540, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 174, Train Loss: 0.0010464, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 175, Train Loss: 0.0010100, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 176, Train Loss: 0.0009820, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 177, Train Loss: 0.0011642, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 178, Train Loss: 0.0010477, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 179, Train Loss: 0.0010508, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 180, Train Loss: 0.0009970, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 181, Train Loss: 0.0009942, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 182, Train Loss: 0.0009634, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 183, Train Loss: 0.0009486, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 184, Train Loss: 0.0010484, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 185, Train Loss: 0.0009867, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 186, Train Loss: 0.0009438, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 187, Train Loss: 0.0009055, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 188, Train Loss: 0.0009390, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 189, Train Loss: 0.0008798, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 190, Train Loss: 0.0008989, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 191, Train Loss: 0.0008836, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 192, Train Loss: 0.0009271, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 193, Train Loss: 0.0009037, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 194, Train Loss: 0.0008751, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 195, Train Loss: 0.0008826, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 196, Train Loss: 0.0009775, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 197, Train Loss: 0.0008900, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 198, Train Loss: 0.0008896, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 199, Train Loss: 0.0008480, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 200, Train Loss: 0.0008668, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 201, Train Loss: 0.0008411, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 202, Train Loss: 0.0009136, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 203, Train Loss: 0.0008664, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 204, Train Loss: 0.0009368, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 205, Train Loss: 0.0008668, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 206, Train Loss: 0.0008565, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 207, Train Loss: 0.0008101, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 208, Train Loss: 0.0007972, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 209, Train Loss: 0.0009120, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 210, Train Loss: 0.0008402, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 211, Train Loss: 0.0008666, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 212, Train Loss: 0.0008120, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 213, Train Loss: 0.0008116, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 214, Train Loss: 0.0007925, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 215, Train Loss: 0.0007786, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 216, Train Loss: 0.0008198, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 217, Train Loss: 0.0007851, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 218, Train Loss: 0.0008137, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 219, Train Loss: 0.0007786, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 220, Train Loss: 0.0007496, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 221, Train Loss: 0.0007720, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 222, Train Loss: 0.0009198, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 223, Train Loss: 0.0031837, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 224, Train Loss: 0.0340929, Train Acc: 0.9925926, Test Acc: 0.6833333\n",
            "Epoch: 225, Train Loss: 0.0157069, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 226, Train Loss: 0.0075807, Train Acc: 1.0000000, Test Acc: 0.6166667\n",
            "Epoch: 227, Train Loss: 0.0042917, Train Acc: 1.0000000, Test Acc: 0.6333333\n",
            "Epoch: 228, Train Loss: 0.0033194, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 229, Train Loss: 0.0028000, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 230, Train Loss: 0.0024348, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 231, Train Loss: 0.0022001, Train Acc: 1.0000000, Test Acc: 0.6500000\n",
            "Epoch: 232, Train Loss: 0.0020288, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 233, Train Loss: 0.0019234, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 234, Train Loss: 0.0017156, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 235, Train Loss: 0.0015854, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 236, Train Loss: 0.0015342, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 237, Train Loss: 0.0014747, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 238, Train Loss: 0.0013569, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 239, Train Loss: 0.0013283, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 240, Train Loss: 0.0012994, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 241, Train Loss: 0.0012771, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 242, Train Loss: 0.0012091, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 243, Train Loss: 0.0011930, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 244, Train Loss: 0.0011824, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 245, Train Loss: 0.0011062, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 246, Train Loss: 0.0012481, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 247, Train Loss: 0.0012484, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 248, Train Loss: 0.0011901, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 249, Train Loss: 0.0011649, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 250, Train Loss: 0.0010665, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 251, Train Loss: 0.0011035, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 252, Train Loss: 0.0010772, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 253, Train Loss: 0.0010158, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 254, Train Loss: 0.0010099, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 255, Train Loss: 0.0009747, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 256, Train Loss: 0.0009224, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 257, Train Loss: 0.0009456, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 258, Train Loss: 0.0009328, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 259, Train Loss: 0.0009680, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 260, Train Loss: 0.0011471, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 261, Train Loss: 0.0010584, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 262, Train Loss: 0.0010364, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 263, Train Loss: 0.0010282, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 264, Train Loss: 0.0009418, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 265, Train Loss: 0.0009235, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 266, Train Loss: 0.0009866, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 267, Train Loss: 0.0008695, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 268, Train Loss: 0.0008605, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 269, Train Loss: 0.0008535, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 270, Train Loss: 0.0008951, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 271, Train Loss: 0.0008515, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 272, Train Loss: 0.0008555, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 273, Train Loss: 0.0008387, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 274, Train Loss: 0.0008178, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 275, Train Loss: 0.0009060, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 276, Train Loss: 0.0008297, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 277, Train Loss: 0.0008173, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 278, Train Loss: 0.0007915, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 279, Train Loss: 0.0008121, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 280, Train Loss: 0.0007999, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 281, Train Loss: 0.0008241, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 282, Train Loss: 0.0007789, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 283, Train Loss: 0.0007943, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 284, Train Loss: 0.0008156, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 285, Train Loss: 0.0007624, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 286, Train Loss: 0.0007668, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 287, Train Loss: 0.0007618, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 288, Train Loss: 0.0007687, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 289, Train Loss: 0.0007475, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 290, Train Loss: 0.0007656, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 291, Train Loss: 0.0007289, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 292, Train Loss: 0.0007256, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 293, Train Loss: 0.0007094, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 294, Train Loss: 0.0007223, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 295, Train Loss: 0.0007380, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 296, Train Loss: 0.0007367, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 297, Train Loss: 0.0007206, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 298, Train Loss: 0.0007301, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 299, Train Loss: 0.0007104, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 300, Train Loss: 0.0007160, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 301, Train Loss: 0.0006789, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 302, Train Loss: 0.0006822, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 303, Train Loss: 0.0006956, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 304, Train Loss: 0.0007033, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 305, Train Loss: 0.0006845, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 306, Train Loss: 0.0006634, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 307, Train Loss: 0.0007918, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 308, Train Loss: 0.0007282, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 309, Train Loss: 0.0006929, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 310, Train Loss: 0.0007553, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 311, Train Loss: 0.0007093, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 312, Train Loss: 0.0006600, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 313, Train Loss: 0.0006550, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 314, Train Loss: 0.0006854, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 315, Train Loss: 0.0006692, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 316, Train Loss: 0.0006629, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 317, Train Loss: 0.0006726, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 318, Train Loss: 0.0006399, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 319, Train Loss: 0.0006447, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 320, Train Loss: 0.0006140, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 321, Train Loss: 0.0006262, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 322, Train Loss: 0.0006094, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 323, Train Loss: 0.0006251, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 324, Train Loss: 0.0006458, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 325, Train Loss: 0.0006299, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 326, Train Loss: 0.0006212, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 327, Train Loss: 0.0007079, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 328, Train Loss: 0.0006756, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 329, Train Loss: 0.0007113, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 330, Train Loss: 0.0006761, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 331, Train Loss: 0.0006708, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 332, Train Loss: 0.0006385, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 333, Train Loss: 0.0006367, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 334, Train Loss: 0.0006266, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 335, Train Loss: 0.0006137, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 336, Train Loss: 0.0006087, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 337, Train Loss: 0.0005985, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 338, Train Loss: 0.0006224, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 339, Train Loss: 0.0006155, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 340, Train Loss: 0.0006000, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 341, Train Loss: 0.0005975, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 342, Train Loss: 0.0005843, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 343, Train Loss: 0.0005850, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 344, Train Loss: 0.0005835, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 345, Train Loss: 0.0005892, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 346, Train Loss: 0.0005762, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 347, Train Loss: 0.0006865, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 348, Train Loss: 0.0006171, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 349, Train Loss: 0.0005832, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 350, Train Loss: 0.0005646, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 351, Train Loss: 0.0005654, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 352, Train Loss: 0.0005632, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 353, Train Loss: 0.0005756, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 354, Train Loss: 0.0005627, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 355, Train Loss: 0.0005709, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 356, Train Loss: 0.0005645, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 357, Train Loss: 0.0005447, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 358, Train Loss: 0.0005579, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 359, Train Loss: 0.0005605, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 360, Train Loss: 0.0005755, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 361, Train Loss: 0.0005613, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 362, Train Loss: 0.0005683, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 363, Train Loss: 0.0005675, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 364, Train Loss: 0.0005668, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 365, Train Loss: 0.0006333, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 366, Train Loss: 0.0005997, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 367, Train Loss: 0.0005579, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 368, Train Loss: 0.0005494, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 369, Train Loss: 0.0005820, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 370, Train Loss: 0.0005679, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 371, Train Loss: 0.0005584, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 372, Train Loss: 0.0005455, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 373, Train Loss: 0.0005303, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 374, Train Loss: 0.0006103, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 375, Train Loss: 0.0005452, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 376, Train Loss: 0.0005555, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 377, Train Loss: 0.0005747, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 378, Train Loss: 0.0005583, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 379, Train Loss: 0.0005670, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 380, Train Loss: 0.0005675, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 381, Train Loss: 0.0005817, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 382, Train Loss: 0.0005530, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 383, Train Loss: 0.0005559, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 384, Train Loss: 0.0005524, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 385, Train Loss: 0.0005353, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 386, Train Loss: 0.0005272, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 387, Train Loss: 0.0005200, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 388, Train Loss: 0.0005428, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 389, Train Loss: 0.0006047, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 390, Train Loss: 0.0005346, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 391, Train Loss: 0.0005605, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 392, Train Loss: 0.0005348, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 393, Train Loss: 0.0005571, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 394, Train Loss: 0.0005370, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 395, Train Loss: 0.0005912, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 396, Train Loss: 0.0005745, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 397, Train Loss: 0.0005443, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 398, Train Loss: 0.0005323, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 399, Train Loss: 0.0005075, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 400, Train Loss: 0.0004997, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 401, Train Loss: 0.0004947, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 402, Train Loss: 0.0005191, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 403, Train Loss: 0.0005124, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 404, Train Loss: 0.0005274, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 405, Train Loss: 0.0005451, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 406, Train Loss: 0.0005211, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 407, Train Loss: 0.0004983, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 408, Train Loss: 0.0005075, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 409, Train Loss: 0.0005121, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 410, Train Loss: 0.0005176, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 411, Train Loss: 0.0004986, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 412, Train Loss: 0.0005140, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 413, Train Loss: 0.0005167, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 414, Train Loss: 0.0005071, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 415, Train Loss: 0.0005222, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 416, Train Loss: 0.0005031, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 417, Train Loss: 0.0005157, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 418, Train Loss: 0.0005204, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 419, Train Loss: 0.0005611, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 420, Train Loss: 0.0005056, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 421, Train Loss: 0.0005117, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 422, Train Loss: 0.0005004, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 423, Train Loss: 0.0005112, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 424, Train Loss: 0.0004714, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 425, Train Loss: 0.0005087, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 426, Train Loss: 0.0005137, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 427, Train Loss: 0.0004776, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 428, Train Loss: 0.0004897, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 429, Train Loss: 0.0004893, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 430, Train Loss: 0.0005022, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 431, Train Loss: 0.0004898, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 432, Train Loss: 0.0004841, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 433, Train Loss: 0.0004891, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 434, Train Loss: 0.0004895, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 435, Train Loss: 0.0004733, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 436, Train Loss: 0.0004678, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 437, Train Loss: 0.0005065, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 438, Train Loss: 0.0004878, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 439, Train Loss: 0.0004954, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 440, Train Loss: 0.0004814, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 441, Train Loss: 0.0004844, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 442, Train Loss: 0.0005174, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 443, Train Loss: 0.0005127, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 444, Train Loss: 0.0005106, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 445, Train Loss: 0.0005024, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 446, Train Loss: 0.0005509, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 447, Train Loss: 0.0005745, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 448, Train Loss: 0.0005185, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 449, Train Loss: 0.0004995, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 450, Train Loss: 0.0004710, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 451, Train Loss: 0.0005441, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 452, Train Loss: 0.0005282, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 453, Train Loss: 0.0005658, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 454, Train Loss: 0.0005108, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 455, Train Loss: 0.0005041, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 456, Train Loss: 0.0004882, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 457, Train Loss: 0.0004950, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 458, Train Loss: 0.0004942, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 459, Train Loss: 0.0004840, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 460, Train Loss: 0.0004869, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 461, Train Loss: 0.0004848, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 462, Train Loss: 0.0004888, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 463, Train Loss: 0.0005708, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 464, Train Loss: 0.0005093, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 465, Train Loss: 0.0004914, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 466, Train Loss: 0.0004815, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 467, Train Loss: 0.0004764, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 468, Train Loss: 0.0004744, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 469, Train Loss: 0.0004870, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 470, Train Loss: 0.0004690, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 471, Train Loss: 0.0004562, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 472, Train Loss: 0.0004528, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 473, Train Loss: 0.0004539, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 474, Train Loss: 0.0004557, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 475, Train Loss: 0.0004731, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 476, Train Loss: 0.0004767, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 477, Train Loss: 0.0004769, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 478, Train Loss: 0.0004662, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 479, Train Loss: 0.0004551, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 480, Train Loss: 0.0004411, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 481, Train Loss: 0.0004344, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 482, Train Loss: 0.0004622, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 483, Train Loss: 0.0004683, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 484, Train Loss: 0.0004518, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 485, Train Loss: 0.0004596, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 486, Train Loss: 0.0004648, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 487, Train Loss: 0.0005456, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 488, Train Loss: 0.0004946, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 489, Train Loss: 0.0004634, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 490, Train Loss: 0.0004572, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 491, Train Loss: 0.0004491, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 492, Train Loss: 0.0004520, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 493, Train Loss: 0.0004987, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 494, Train Loss: 0.0005204, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 495, Train Loss: 0.0004747, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 496, Train Loss: 0.0004874, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 497, Train Loss: 0.0004584, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 498, Train Loss: 0.0004460, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 499, Train Loss: 0.0004756, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 500, Train Loss: 0.0004546, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "--------\n",
            "Best Test Acc:   0.7333333333333333_0.0, Epoch: 109\n",
            "Best Train Loss: 0.00043442474410834687_0.0\n",
            "Best Train Acc:  1.0_0.0\n",
            "FOLD1, ENZYMES, 1668698769/_rhoE0.5P18I1_GELU_X_6_256_0_0.001_50_0.6_B64S0W8, BT=0.7333333333333333_0.0, BE=109, ID=\n",
            "train_set : test_set = %d : %d 540 60\n",
            "-------- FOLD1 DATASET=ENZYMES, COMMIT_ID=\n",
            "#Params: 1334534\n",
            "Epoch: 001, Train Loss: 5.2833894, Train Acc: 0.2740741, Test Acc: 0.2166667\n",
            "Epoch: 002, Train Loss: 2.4887914, Train Acc: 0.2111111, Test Acc: 0.2166667\n",
            "Epoch: 003, Train Loss: 2.2243023, Train Acc: 0.2462963, Test Acc: 0.2500000\n",
            "Epoch: 004, Train Loss: 1.6831021, Train Acc: 0.3185185, Test Acc: 0.3333333\n",
            "Epoch: 005, Train Loss: 1.5393302, Train Acc: 0.3907407, Test Acc: 0.2833333\n",
            "Epoch: 006, Train Loss: 1.5462181, Train Acc: 0.3259259, Test Acc: 0.2833333\n",
            "Epoch: 007, Train Loss: 1.4943567, Train Acc: 0.3648148, Test Acc: 0.3833333\n",
            "Epoch: 008, Train Loss: 1.4480345, Train Acc: 0.4444444, Test Acc: 0.5000000\n",
            "Epoch: 009, Train Loss: 1.5441327, Train Acc: 0.3870370, Test Acc: 0.3166667\n",
            "Epoch: 010, Train Loss: 1.3878191, Train Acc: 0.4555556, Test Acc: 0.4166667\n",
            "Epoch: 011, Train Loss: 1.4293299, Train Acc: 0.4277778, Test Acc: 0.4166667\n",
            "Epoch: 012, Train Loss: 1.2828732, Train Acc: 0.5740741, Test Acc: 0.4166667\n",
            "Epoch: 013, Train Loss: 1.1863880, Train Acc: 0.5611111, Test Acc: 0.4666667\n",
            "Epoch: 014, Train Loss: 1.1407610, Train Acc: 0.6222222, Test Acc: 0.4833333\n",
            "Epoch: 015, Train Loss: 1.1219844, Train Acc: 0.5722222, Test Acc: 0.4833333\n",
            "Epoch: 016, Train Loss: 1.1305562, Train Acc: 0.5814815, Test Acc: 0.4000000\n",
            "Epoch: 017, Train Loss: 1.1401582, Train Acc: 0.5388889, Test Acc: 0.5000000\n",
            "Epoch: 018, Train Loss: 1.4113156, Train Acc: 0.4537037, Test Acc: 0.4166667\n",
            "Epoch: 019, Train Loss: 1.0692548, Train Acc: 0.6407407, Test Acc: 0.5166667\n",
            "Epoch: 020, Train Loss: 1.1991754, Train Acc: 0.5500000, Test Acc: 0.4833333\n",
            "Epoch: 021, Train Loss: 1.1644694, Train Acc: 0.5296296, Test Acc: 0.3833333\n",
            "Epoch: 022, Train Loss: 1.0169336, Train Acc: 0.6351852, Test Acc: 0.4500000\n",
            "Epoch: 023, Train Loss: 1.2302298, Train Acc: 0.5648148, Test Acc: 0.4500000\n",
            "Epoch: 024, Train Loss: 1.1368299, Train Acc: 0.5759259, Test Acc: 0.4000000\n",
            "Epoch: 025, Train Loss: 0.7793496, Train Acc: 0.7537037, Test Acc: 0.6666667\n",
            "Epoch: 026, Train Loss: 0.8338186, Train Acc: 0.7185185, Test Acc: 0.5833333\n",
            "Epoch: 027, Train Loss: 1.0419723, Train Acc: 0.5500000, Test Acc: 0.4666667\n",
            "Epoch: 028, Train Loss: 0.7058328, Train Acc: 0.8018519, Test Acc: 0.6000000\n",
            "Epoch: 029, Train Loss: 0.8631871, Train Acc: 0.6962963, Test Acc: 0.5666667\n",
            "Epoch: 030, Train Loss: 0.8143812, Train Acc: 0.7333333, Test Acc: 0.4500000\n",
            "Epoch: 031, Train Loss: 0.5854213, Train Acc: 0.8166667, Test Acc: 0.5833333\n",
            "Epoch: 032, Train Loss: 0.6222483, Train Acc: 0.7851852, Test Acc: 0.6500000\n",
            "Epoch: 033, Train Loss: 0.5595254, Train Acc: 0.8111111, Test Acc: 0.6833333\n",
            "Epoch: 034, Train Loss: 0.6340990, Train Acc: 0.7925926, Test Acc: 0.4833333\n",
            "Epoch: 035, Train Loss: 0.5427894, Train Acc: 0.8037037, Test Acc: 0.6000000\n",
            "Epoch: 036, Train Loss: 0.4406178, Train Acc: 0.8537037, Test Acc: 0.6500000\n",
            "Epoch: 037, Train Loss: 0.8095739, Train Acc: 0.7425926, Test Acc: 0.6000000\n",
            "Epoch: 038, Train Loss: 0.5061575, Train Acc: 0.8185185, Test Acc: 0.4833333\n",
            "Epoch: 039, Train Loss: 0.2958799, Train Acc: 0.9259259, Test Acc: 0.7333333\n",
            "Epoch: 040, Train Loss: 0.5472024, Train Acc: 0.7888889, Test Acc: 0.6333333\n",
            "Epoch: 041, Train Loss: 0.4364587, Train Acc: 0.8462963, Test Acc: 0.7166667\n",
            "Epoch: 042, Train Loss: 0.2264747, Train Acc: 0.9592593, Test Acc: 0.7000000\n",
            "Epoch: 043, Train Loss: 0.1984385, Train Acc: 0.9685185, Test Acc: 0.6333333\n",
            "Epoch: 044, Train Loss: 0.2394719, Train Acc: 0.9388889, Test Acc: 0.7000000\n",
            "Epoch: 045, Train Loss: 0.1563447, Train Acc: 0.9777778, Test Acc: 0.7000000\n",
            "Epoch: 046, Train Loss: 0.3074812, Train Acc: 0.9000000, Test Acc: 0.7166667\n",
            "Epoch: 047, Train Loss: 0.2019625, Train Acc: 0.9296296, Test Acc: 0.7166667\n",
            "Epoch: 048, Train Loss: 0.2306110, Train Acc: 0.9388889, Test Acc: 0.7166667\n",
            "Epoch: 049, Train Loss: 0.1168847, Train Acc: 0.9777778, Test Acc: 0.7000000\n",
            "Epoch: 050, Train Loss: 0.1745926, Train Acc: 0.9574074, Test Acc: 0.6500000\n",
            "Epoch: 051, Train Loss: 0.1065775, Train Acc: 0.9851852, Test Acc: 0.7000000\n",
            "Epoch: 052, Train Loss: 0.0398596, Train Acc: 1.0000000, Test Acc: 0.7500000\n",
            "Epoch: 053, Train Loss: 0.0397774, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 054, Train Loss: 0.0406239, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 055, Train Loss: 0.0293083, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 056, Train Loss: 0.0259641, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 057, Train Loss: 0.0181273, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 058, Train Loss: 0.0157746, Train Acc: 1.0000000, Test Acc: 0.7500000\n",
            "Epoch: 059, Train Loss: 0.0154813, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 060, Train Loss: 0.0154560, Train Acc: 1.0000000, Test Acc: 0.7500000\n",
            "Epoch: 061, Train Loss: 0.0134089, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 062, Train Loss: 0.0129936, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 063, Train Loss: 0.0118543, Train Acc: 1.0000000, Test Acc: 0.8000000\n",
            "Epoch: 064, Train Loss: 0.0113096, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 065, Train Loss: 0.0106025, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 066, Train Loss: 0.0102234, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 067, Train Loss: 0.0115362, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 068, Train Loss: 0.0133912, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 069, Train Loss: 0.0103297, Train Acc: 1.0000000, Test Acc: 0.7500000\n",
            "Epoch: 070, Train Loss: 0.0087303, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 071, Train Loss: 0.0118546, Train Acc: 1.0000000, Test Acc: 0.7500000\n",
            "Epoch: 072, Train Loss: 0.0104981, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 073, Train Loss: 0.0090653, Train Acc: 1.0000000, Test Acc: 0.7500000\n",
            "Epoch: 074, Train Loss: 0.0089864, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 075, Train Loss: 0.0168324, Train Acc: 0.9981481, Test Acc: 0.7666667\n",
            "Epoch: 076, Train Loss: 0.0859103, Train Acc: 0.9777778, Test Acc: 0.6500000\n",
            "Epoch: 077, Train Loss: 0.3953812, Train Acc: 0.8888889, Test Acc: 0.7666667\n",
            "Epoch: 078, Train Loss: 0.0329103, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 079, Train Loss: 0.0335260, Train Acc: 0.9981481, Test Acc: 0.7333333\n",
            "Epoch: 080, Train Loss: 0.0546979, Train Acc: 0.9962963, Test Acc: 0.7333333\n",
            "Epoch: 081, Train Loss: 0.0139350, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 082, Train Loss: 0.0141466, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 083, Train Loss: 0.0116764, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 084, Train Loss: 0.0094696, Train Acc: 1.0000000, Test Acc: 0.7500000\n",
            "Epoch: 085, Train Loss: 0.0073784, Train Acc: 1.0000000, Test Acc: 0.8166667\n",
            "Epoch: 086, Train Loss: 0.0054973, Train Acc: 1.0000000, Test Acc: 0.7500000\n",
            "Epoch: 087, Train Loss: 0.0050440, Train Acc: 1.0000000, Test Acc: 0.8000000\n",
            "Epoch: 088, Train Loss: 0.0048845, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 089, Train Loss: 0.0043782, Train Acc: 1.0000000, Test Acc: 0.7500000\n",
            "Epoch: 090, Train Loss: 0.0047117, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 091, Train Loss: 0.0047602, Train Acc: 1.0000000, Test Acc: 0.7500000\n",
            "Epoch: 092, Train Loss: 0.0070677, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 093, Train Loss: 0.0057524, Train Acc: 1.0000000, Test Acc: 0.8000000\n",
            "Epoch: 094, Train Loss: 0.0040790, Train Acc: 1.0000000, Test Acc: 0.8000000\n",
            "Epoch: 095, Train Loss: 0.0038207, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 096, Train Loss: 0.0033711, Train Acc: 1.0000000, Test Acc: 0.7500000\n",
            "Epoch: 097, Train Loss: 0.0032885, Train Acc: 1.0000000, Test Acc: 0.7500000\n",
            "Epoch: 098, Train Loss: 0.0039434, Train Acc: 1.0000000, Test Acc: 0.8000000\n",
            "Epoch: 099, Train Loss: 0.0032674, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 100, Train Loss: 0.0030461, Train Acc: 1.0000000, Test Acc: 0.7500000\n",
            "Epoch: 101, Train Loss: 0.0027576, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 102, Train Loss: 0.0025261, Train Acc: 1.0000000, Test Acc: 0.8000000\n",
            "Epoch: 103, Train Loss: 0.0023963, Train Acc: 1.0000000, Test Acc: 0.8000000\n",
            "Epoch: 104, Train Loss: 0.0023498, Train Acc: 1.0000000, Test Acc: 0.7500000\n",
            "Epoch: 105, Train Loss: 0.0022926, Train Acc: 1.0000000, Test Acc: 0.7500000\n",
            "Epoch: 106, Train Loss: 0.0022313, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 107, Train Loss: 0.0023223, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 108, Train Loss: 0.0022152, Train Acc: 1.0000000, Test Acc: 0.7500000\n",
            "Epoch: 109, Train Loss: 0.0021274, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 110, Train Loss: 0.0021172, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 111, Train Loss: 0.0019850, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 112, Train Loss: 0.0020552, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 113, Train Loss: 0.0019523, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 114, Train Loss: 0.0019465, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 115, Train Loss: 0.0018607, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 116, Train Loss: 0.0019872, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 117, Train Loss: 0.0018397, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 118, Train Loss: 0.0018310, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 119, Train Loss: 0.0018225, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 120, Train Loss: 0.0019670, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 121, Train Loss: 0.0018260, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 122, Train Loss: 0.0017466, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 123, Train Loss: 0.0016536, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 124, Train Loss: 0.0016737, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 125, Train Loss: 0.0016599, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 126, Train Loss: 0.0016572, Train Acc: 1.0000000, Test Acc: 0.7500000\n",
            "Epoch: 127, Train Loss: 0.0016479, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 128, Train Loss: 0.0026424, Train Acc: 1.0000000, Test Acc: 0.8000000\n",
            "Epoch: 129, Train Loss: 0.0029272, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 130, Train Loss: 0.0022737, Train Acc: 1.0000000, Test Acc: 0.8166667\n",
            "Epoch: 131, Train Loss: 0.0049790, Train Acc: 0.9981481, Test Acc: 0.7666667\n",
            "Epoch: 132, Train Loss: 0.0516165, Train Acc: 0.9870370, Test Acc: 0.7500000\n",
            "Epoch: 133, Train Loss: 0.0037145, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 134, Train Loss: 0.0054745, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 135, Train Loss: 0.0039692, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 136, Train Loss: 0.0030824, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 137, Train Loss: 0.0026519, Train Acc: 1.0000000, Test Acc: 0.7500000\n",
            "Epoch: 138, Train Loss: 0.0021444, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 139, Train Loss: 0.0020559, Train Acc: 1.0000000, Test Acc: 0.7500000\n",
            "Epoch: 140, Train Loss: 0.0025043, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 141, Train Loss: 0.0020878, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 142, Train Loss: 0.0019143, Train Acc: 1.0000000, Test Acc: 0.8000000\n",
            "Epoch: 143, Train Loss: 0.0018884, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 144, Train Loss: 0.0016207, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 145, Train Loss: 0.0017639, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 146, Train Loss: 0.0016861, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 147, Train Loss: 0.0015129, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 148, Train Loss: 0.0014268, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 149, Train Loss: 0.0014180, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 150, Train Loss: 0.0013968, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 151, Train Loss: 0.0015167, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 152, Train Loss: 0.0013320, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 153, Train Loss: 0.0013585, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 154, Train Loss: 0.0013278, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 155, Train Loss: 0.0013057, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 156, Train Loss: 0.0014065, Train Acc: 1.0000000, Test Acc: 0.8000000\n",
            "Epoch: 157, Train Loss: 0.0012971, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 158, Train Loss: 0.0012614, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 159, Train Loss: 0.0012248, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 160, Train Loss: 0.0011715, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 161, Train Loss: 0.0012834, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 162, Train Loss: 0.0012129, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 163, Train Loss: 0.0011297, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 164, Train Loss: 0.0010931, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 165, Train Loss: 0.0011465, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 166, Train Loss: 0.0011202, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 167, Train Loss: 0.0011384, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 168, Train Loss: 0.0011470, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 169, Train Loss: 0.0010860, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 170, Train Loss: 0.0010561, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 171, Train Loss: 0.0010239, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 172, Train Loss: 0.0010164, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 173, Train Loss: 0.0010134, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 174, Train Loss: 0.0009892, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 175, Train Loss: 0.0009482, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 176, Train Loss: 0.0009708, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 177, Train Loss: 0.0009253, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 178, Train Loss: 0.0009623, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 179, Train Loss: 0.0010143, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 180, Train Loss: 0.0024706, Train Acc: 1.0000000, Test Acc: 0.7500000\n",
            "Epoch: 181, Train Loss: 0.0027539, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 182, Train Loss: 0.0016613, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 183, Train Loss: 0.0014578, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 184, Train Loss: 0.0013884, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 185, Train Loss: 0.0011770, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 186, Train Loss: 0.0010710, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 187, Train Loss: 0.0009671, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 188, Train Loss: 0.0009963, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 189, Train Loss: 0.0009405, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 190, Train Loss: 0.0009256, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 191, Train Loss: 0.0009486, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 192, Train Loss: 0.0009170, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 193, Train Loss: 0.0008998, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 194, Train Loss: 0.0008838, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 195, Train Loss: 0.0008579, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 196, Train Loss: 0.0008364, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 197, Train Loss: 0.0009933, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 198, Train Loss: 0.0010982, Train Acc: 1.0000000, Test Acc: 0.7500000\n",
            "Epoch: 199, Train Loss: 0.0009845, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 200, Train Loss: 0.0008154, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 201, Train Loss: 0.0008017, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 202, Train Loss: 0.0007914, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 203, Train Loss: 0.0007608, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 204, Train Loss: 0.0007925, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 205, Train Loss: 0.0007496, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 206, Train Loss: 0.0007326, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 207, Train Loss: 0.0007455, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 208, Train Loss: 0.0007794, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 209, Train Loss: 0.0007490, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 210, Train Loss: 0.0007777, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 211, Train Loss: 0.0007234, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 212, Train Loss: 0.0007128, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 213, Train Loss: 0.0007493, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 214, Train Loss: 0.0007624, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 215, Train Loss: 0.0007468, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 216, Train Loss: 0.0007050, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 217, Train Loss: 0.0006962, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 218, Train Loss: 0.0006840, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 219, Train Loss: 0.0006902, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 220, Train Loss: 0.0007060, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 221, Train Loss: 0.0007214, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 222, Train Loss: 0.0006836, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 223, Train Loss: 0.0006711, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 224, Train Loss: 0.0006550, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 225, Train Loss: 0.0006759, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 226, Train Loss: 0.0006706, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 227, Train Loss: 0.0006517, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 228, Train Loss: 0.0007525, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 229, Train Loss: 0.0007078, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 230, Train Loss: 0.0006639, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 231, Train Loss: 0.0006214, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 232, Train Loss: 0.0006237, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 233, Train Loss: 0.0006579, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 234, Train Loss: 0.0006280, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 235, Train Loss: 0.0006229, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 236, Train Loss: 0.0006036, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 237, Train Loss: 0.0006335, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 238, Train Loss: 0.0006618, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 239, Train Loss: 0.0006393, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 240, Train Loss: 0.0006083, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 241, Train Loss: 0.0006146, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 242, Train Loss: 0.0006159, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 243, Train Loss: 0.0005887, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 244, Train Loss: 0.0005805, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 245, Train Loss: 0.0005900, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 246, Train Loss: 0.0005917, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 247, Train Loss: 0.0006245, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 248, Train Loss: 0.0006254, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 249, Train Loss: 0.0005885, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 250, Train Loss: 0.0005894, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 251, Train Loss: 0.0005858, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 252, Train Loss: 0.0006078, Train Acc: 1.0000000, Test Acc: 0.8000000\n",
            "Epoch: 253, Train Loss: 0.0005906, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 254, Train Loss: 0.0006115, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 255, Train Loss: 0.0006028, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 256, Train Loss: 0.0005988, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 257, Train Loss: 0.0005876, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 258, Train Loss: 0.0005929, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 259, Train Loss: 0.0005427, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 260, Train Loss: 0.0005842, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 261, Train Loss: 0.0005596, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 262, Train Loss: 0.0005241, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 263, Train Loss: 0.0006233, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 264, Train Loss: 0.0005793, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 265, Train Loss: 0.0005427, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 266, Train Loss: 0.0005386, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 267, Train Loss: 0.0005338, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 268, Train Loss: 0.0005429, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 269, Train Loss: 0.0005517, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 270, Train Loss: 0.0005336, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 271, Train Loss: 0.0005617, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 272, Train Loss: 0.0005295, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 273, Train Loss: 0.0005073, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 274, Train Loss: 0.0005237, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 275, Train Loss: 0.0005083, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 276, Train Loss: 0.0005020, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 277, Train Loss: 0.0005058, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 278, Train Loss: 0.0004923, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 279, Train Loss: 0.0005002, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 280, Train Loss: 0.0005197, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 281, Train Loss: 0.0005186, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 282, Train Loss: 0.0004933, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 283, Train Loss: 0.0004873, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 284, Train Loss: 0.0004963, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 285, Train Loss: 0.0005424, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 286, Train Loss: 0.0005314, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 287, Train Loss: 0.0005452, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 288, Train Loss: 0.0005128, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 289, Train Loss: 0.0004849, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 290, Train Loss: 0.0004846, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 291, Train Loss: 0.0004848, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 292, Train Loss: 0.0004958, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 293, Train Loss: 0.0005043, Train Acc: 1.0000000, Test Acc: 0.8000000\n",
            "Epoch: 294, Train Loss: 0.0004935, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 295, Train Loss: 0.0004880, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 296, Train Loss: 0.0004793, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 297, Train Loss: 0.0004702, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 298, Train Loss: 0.0004916, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 299, Train Loss: 0.0004784, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 300, Train Loss: 0.0004788, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 301, Train Loss: 0.0004712, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 302, Train Loss: 0.0004862, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 303, Train Loss: 0.0004797, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 304, Train Loss: 0.0004837, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 305, Train Loss: 0.0004917, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 306, Train Loss: 0.0004698, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 307, Train Loss: 0.0004609, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 308, Train Loss: 0.0004525, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 309, Train Loss: 0.0004489, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 310, Train Loss: 0.0004478, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 311, Train Loss: 0.0004668, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 312, Train Loss: 0.0004641, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 313, Train Loss: 0.0004585, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 314, Train Loss: 0.0004532, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 315, Train Loss: 0.0004591, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 316, Train Loss: 0.0004777, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 317, Train Loss: 0.0005265, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 318, Train Loss: 0.0005891, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 319, Train Loss: 0.0005089, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 320, Train Loss: 0.0006272, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 321, Train Loss: 0.0005242, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 322, Train Loss: 0.0005160, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 323, Train Loss: 0.0004817, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 324, Train Loss: 0.0004776, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 325, Train Loss: 0.0004806, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 326, Train Loss: 0.0004568, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 327, Train Loss: 0.0004591, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 328, Train Loss: 0.0004608, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 329, Train Loss: 0.0004566, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 330, Train Loss: 0.0004662, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 331, Train Loss: 0.0004531, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 332, Train Loss: 0.0004451, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 333, Train Loss: 0.0004308, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 334, Train Loss: 0.0004475, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 335, Train Loss: 0.0004279, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 336, Train Loss: 0.0004364, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 337, Train Loss: 0.0004494, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 338, Train Loss: 0.0004960, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 339, Train Loss: 0.0004558, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 340, Train Loss: 0.0004310, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 341, Train Loss: 0.0004254, Train Acc: 1.0000000, Test Acc: 0.8000000\n",
            "Epoch: 342, Train Loss: 0.0004254, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 343, Train Loss: 0.0004201, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 344, Train Loss: 0.0004367, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 345, Train Loss: 0.0004078, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 346, Train Loss: 0.0004071, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 347, Train Loss: 0.0004110, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 348, Train Loss: 0.0003993, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 349, Train Loss: 0.0004173, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 350, Train Loss: 0.0004162, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 351, Train Loss: 0.0004138, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 352, Train Loss: 0.0003946, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 353, Train Loss: 0.0004073, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 354, Train Loss: 0.0004163, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 355, Train Loss: 0.0004065, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 356, Train Loss: 0.0004044, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 357, Train Loss: 0.0004073, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 358, Train Loss: 0.0004142, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 359, Train Loss: 0.0004180, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 360, Train Loss: 0.0004175, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 361, Train Loss: 0.0004245, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 362, Train Loss: 0.0004322, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 363, Train Loss: 0.0005279, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 364, Train Loss: 0.0004725, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 365, Train Loss: 0.0005382, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 366, Train Loss: 0.0005304, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 367, Train Loss: 0.0004523, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 368, Train Loss: 0.0004160, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 369, Train Loss: 0.0003961, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 370, Train Loss: 0.0004181, Train Acc: 1.0000000, Test Acc: 0.8000000\n",
            "Epoch: 371, Train Loss: 0.0004203, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 372, Train Loss: 0.0004107, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 373, Train Loss: 0.0004002, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 374, Train Loss: 0.0004000, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 375, Train Loss: 0.0004052, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 376, Train Loss: 0.0004038, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 377, Train Loss: 0.0004175, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 378, Train Loss: 0.0003879, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 379, Train Loss: 0.0003801, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 380, Train Loss: 0.0003946, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 381, Train Loss: 0.0004049, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 382, Train Loss: 0.0004125, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 383, Train Loss: 0.0004638, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 384, Train Loss: 0.0004415, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 385, Train Loss: 0.0004097, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 386, Train Loss: 0.0003827, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 387, Train Loss: 0.0003746, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 388, Train Loss: 0.0003911, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 389, Train Loss: 0.0003985, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 390, Train Loss: 0.0003918, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 391, Train Loss: 0.0003710, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 392, Train Loss: 0.0003793, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 393, Train Loss: 0.0004039, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 394, Train Loss: 0.0004023, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 395, Train Loss: 0.0003777, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 396, Train Loss: 0.0003959, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 397, Train Loss: 0.0003956, Train Acc: 1.0000000, Test Acc: 0.8000000\n",
            "Epoch: 398, Train Loss: 0.0004490, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 399, Train Loss: 0.0004076, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 400, Train Loss: 0.0003920, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 401, Train Loss: 0.0003886, Train Acc: 1.0000000, Test Acc: 0.8000000\n",
            "Epoch: 402, Train Loss: 0.0003732, Train Acc: 1.0000000, Test Acc: 0.8000000\n",
            "Epoch: 403, Train Loss: 0.0004288, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 404, Train Loss: 0.0004124, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 405, Train Loss: 0.0004705, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 406, Train Loss: 0.0004769, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 407, Train Loss: 0.0004172, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 408, Train Loss: 0.0003990, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 409, Train Loss: 0.0003898, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 410, Train Loss: 0.0003764, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 411, Train Loss: 0.0003717, Train Acc: 1.0000000, Test Acc: 0.8000000\n",
            "Epoch: 412, Train Loss: 0.0003706, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 413, Train Loss: 0.0003637, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 414, Train Loss: 0.0003847, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 415, Train Loss: 0.0003985, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 416, Train Loss: 0.0003806, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 417, Train Loss: 0.0003733, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 418, Train Loss: 0.0003752, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 419, Train Loss: 0.0003761, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 420, Train Loss: 0.0003672, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 421, Train Loss: 0.0004033, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 422, Train Loss: 0.0003966, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 423, Train Loss: 0.0003874, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 424, Train Loss: 0.0003628, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 425, Train Loss: 0.0003724, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 426, Train Loss: 0.0003680, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 427, Train Loss: 0.0003655, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 428, Train Loss: 0.0003805, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 429, Train Loss: 0.0003874, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 430, Train Loss: 0.0003748, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 431, Train Loss: 0.0003683, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 432, Train Loss: 0.0003655, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 433, Train Loss: 0.0003742, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 434, Train Loss: 0.0003794, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 435, Train Loss: 0.0003735, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 436, Train Loss: 0.0004442, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 437, Train Loss: 0.0003883, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 438, Train Loss: 0.0003745, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 439, Train Loss: 0.0003643, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 440, Train Loss: 0.0003796, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 441, Train Loss: 0.0003651, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 442, Train Loss: 0.0003829, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 443, Train Loss: 0.0003592, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 444, Train Loss: 0.0003723, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 445, Train Loss: 0.0003718, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 446, Train Loss: 0.0003704, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 447, Train Loss: 0.0003795, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 448, Train Loss: 0.0003891, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 449, Train Loss: 0.0003870, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 450, Train Loss: 0.0003561, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 451, Train Loss: 0.0003740, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 452, Train Loss: 0.0003633, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 453, Train Loss: 0.0003579, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 454, Train Loss: 0.0003520, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 455, Train Loss: 0.0003447, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 456, Train Loss: 0.0003838, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 457, Train Loss: 0.0003570, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 458, Train Loss: 0.0003470, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 459, Train Loss: 0.0003725, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 460, Train Loss: 0.0003575, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 461, Train Loss: 0.0003866, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 462, Train Loss: 0.0003732, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 463, Train Loss: 0.0003529, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 464, Train Loss: 0.0003462, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 465, Train Loss: 0.0003692, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 466, Train Loss: 0.0003539, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 467, Train Loss: 0.0003544, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 468, Train Loss: 0.0003501, Train Acc: 1.0000000, Test Acc: 0.8000000\n",
            "Epoch: 469, Train Loss: 0.0003505, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 470, Train Loss: 0.0003437, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 471, Train Loss: 0.0003538, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 472, Train Loss: 0.0003478, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 473, Train Loss: 0.0004013, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 474, Train Loss: 0.0003635, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 475, Train Loss: 0.0003567, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 476, Train Loss: 0.0003601, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 477, Train Loss: 0.0003521, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 478, Train Loss: 0.0003569, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 479, Train Loss: 0.0003357, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 480, Train Loss: 0.0003437, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 481, Train Loss: 0.0003513, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 482, Train Loss: 0.0003326, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 483, Train Loss: 0.0003601, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 484, Train Loss: 0.0003466, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 485, Train Loss: 0.0003611, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 486, Train Loss: 0.0003583, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 487, Train Loss: 0.0004232, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 488, Train Loss: 0.0003972, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 489, Train Loss: 0.0003706, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 490, Train Loss: 0.0003761, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 491, Train Loss: 0.0003530, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 492, Train Loss: 0.0003522, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 493, Train Loss: 0.0003714, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 494, Train Loss: 0.0003547, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 495, Train Loss: 0.0003496, Train Acc: 1.0000000, Test Acc: 0.8000000\n",
            "Epoch: 496, Train Loss: 0.0003467, Train Acc: 1.0000000, Test Acc: 0.8000000\n",
            "Epoch: 497, Train Loss: 0.0003441, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 498, Train Loss: 0.0003549, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 499, Train Loss: 0.0003440, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 500, Train Loss: 0.0003364, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "--------\n",
            "Best Test Acc:   0.7583333333333333_0.041666666666666685, Epoch: 102\n",
            "Best Train Loss: 0.00039238835829396353_4.870453105670295e-05\n",
            "Best Train Acc:  1.0_0.0\n",
            "FOLD2, ENZYMES, 1668698769/_rhoE0.5P18I1_GELU_X_6_256_0_0.001_50_0.6_B64S0W8, BT=0.7583333333333333_0.041666666666666685, BE=102, ID=\n",
            "train_set : test_set = %d : %d 540 60\n",
            "-------- FOLD2 DATASET=ENZYMES, COMMIT_ID=\n",
            "#Params: 1334534\n",
            "Epoch: 001, Train Loss: 5.1963864, Train Acc: 0.2296296, Test Acc: 0.2333333\n",
            "Epoch: 002, Train Loss: 2.1895888, Train Acc: 0.3037037, Test Acc: 0.1833333\n",
            "Epoch: 003, Train Loss: 2.0371513, Train Acc: 0.2351852, Test Acc: 0.2000000\n",
            "Epoch: 004, Train Loss: 1.6472408, Train Acc: 0.3148148, Test Acc: 0.2833333\n",
            "Epoch: 005, Train Loss: 1.7574915, Train Acc: 0.3166667, Test Acc: 0.2666667\n",
            "Epoch: 006, Train Loss: 1.5669858, Train Acc: 0.3518519, Test Acc: 0.3000000\n",
            "Epoch: 007, Train Loss: 1.4293172, Train Acc: 0.4388889, Test Acc: 0.3166667\n",
            "Epoch: 008, Train Loss: 1.4194629, Train Acc: 0.4166667, Test Acc: 0.2833333\n",
            "Epoch: 009, Train Loss: 1.4132272, Train Acc: 0.4444444, Test Acc: 0.3000000\n",
            "Epoch: 010, Train Loss: 1.3129518, Train Acc: 0.5074074, Test Acc: 0.3500000\n",
            "Epoch: 011, Train Loss: 1.2784772, Train Acc: 0.5296296, Test Acc: 0.3666667\n",
            "Epoch: 012, Train Loss: 1.2377579, Train Acc: 0.5462963, Test Acc: 0.3666667\n",
            "Epoch: 013, Train Loss: 1.1699658, Train Acc: 0.5703704, Test Acc: 0.3500000\n",
            "Epoch: 014, Train Loss: 1.2754739, Train Acc: 0.5666667, Test Acc: 0.4000000\n",
            "Epoch: 015, Train Loss: 1.1660384, Train Acc: 0.5907407, Test Acc: 0.3500000\n",
            "Epoch: 016, Train Loss: 1.0074754, Train Acc: 0.6777778, Test Acc: 0.4000000\n",
            "Epoch: 017, Train Loss: 1.0931104, Train Acc: 0.5629630, Test Acc: 0.4000000\n",
            "Epoch: 018, Train Loss: 0.9163674, Train Acc: 0.7277778, Test Acc: 0.4333333\n",
            "Epoch: 019, Train Loss: 1.0916277, Train Acc: 0.5888889, Test Acc: 0.4166667\n",
            "Epoch: 020, Train Loss: 0.9081871, Train Acc: 0.7037037, Test Acc: 0.4666667\n",
            "Epoch: 021, Train Loss: 0.9455454, Train Acc: 0.6462963, Test Acc: 0.3500000\n",
            "Epoch: 022, Train Loss: 0.7651801, Train Acc: 0.7703704, Test Acc: 0.5000000\n",
            "Epoch: 023, Train Loss: 0.8113362, Train Acc: 0.7203704, Test Acc: 0.4500000\n",
            "Epoch: 024, Train Loss: 0.7679141, Train Acc: 0.7185185, Test Acc: 0.4666667\n",
            "Epoch: 025, Train Loss: 0.7976242, Train Acc: 0.7333333, Test Acc: 0.4333333\n",
            "Epoch: 026, Train Loss: 0.6507440, Train Acc: 0.8203704, Test Acc: 0.5333333\n",
            "Epoch: 027, Train Loss: 0.5620228, Train Acc: 0.8388889, Test Acc: 0.4833333\n",
            "Epoch: 028, Train Loss: 1.1636444, Train Acc: 0.6148148, Test Acc: 0.3000000\n",
            "Epoch: 029, Train Loss: 0.6806770, Train Acc: 0.7648148, Test Acc: 0.4333333\n",
            "Epoch: 030, Train Loss: 0.9743186, Train Acc: 0.6388889, Test Acc: 0.4166667\n",
            "Epoch: 031, Train Loss: 0.5296118, Train Acc: 0.8462963, Test Acc: 0.5166667\n",
            "Epoch: 032, Train Loss: 0.4569533, Train Acc: 0.8629630, Test Acc: 0.5166667\n",
            "Epoch: 033, Train Loss: 0.6158986, Train Acc: 0.7888889, Test Acc: 0.5000000\n",
            "Epoch: 034, Train Loss: 0.4407075, Train Acc: 0.8592593, Test Acc: 0.4666667\n",
            "Epoch: 035, Train Loss: 0.3835233, Train Acc: 0.9074074, Test Acc: 0.5000000\n",
            "Epoch: 036, Train Loss: 0.4499428, Train Acc: 0.8555556, Test Acc: 0.4333333\n",
            "Epoch: 037, Train Loss: 0.6568178, Train Acc: 0.7296296, Test Acc: 0.3833333\n",
            "Epoch: 038, Train Loss: 0.3643485, Train Acc: 0.8888889, Test Acc: 0.5333333\n",
            "Epoch: 039, Train Loss: 0.4684782, Train Acc: 0.8481481, Test Acc: 0.4666667\n",
            "Epoch: 040, Train Loss: 0.3578366, Train Acc: 0.8814815, Test Acc: 0.5333333\n",
            "Epoch: 041, Train Loss: 0.1955451, Train Acc: 0.9666667, Test Acc: 0.5166667\n",
            "Epoch: 042, Train Loss: 0.1808316, Train Acc: 0.9685185, Test Acc: 0.5333333\n",
            "Epoch: 043, Train Loss: 0.4730021, Train Acc: 0.8259259, Test Acc: 0.4333333\n",
            "Epoch: 044, Train Loss: 0.1319396, Train Acc: 0.9796296, Test Acc: 0.5500000\n",
            "Epoch: 045, Train Loss: 0.1228993, Train Acc: 0.9796296, Test Acc: 0.5166667\n",
            "Epoch: 046, Train Loss: 0.1681098, Train Acc: 0.9518519, Test Acc: 0.5500000\n",
            "Epoch: 047, Train Loss: 0.0734110, Train Acc: 0.9981481, Test Acc: 0.5833333\n",
            "Epoch: 048, Train Loss: 0.0955899, Train Acc: 0.9759259, Test Acc: 0.6000000\n",
            "Epoch: 049, Train Loss: 0.2000178, Train Acc: 0.9388889, Test Acc: 0.5333333\n",
            "Epoch: 050, Train Loss: 0.0766242, Train Acc: 0.9888889, Test Acc: 0.5666667\n",
            "Epoch: 051, Train Loss: 0.0365662, Train Acc: 1.0000000, Test Acc: 0.5166667\n",
            "Epoch: 052, Train Loss: 0.0251385, Train Acc: 1.0000000, Test Acc: 0.5666667\n",
            "Epoch: 053, Train Loss: 0.0210674, Train Acc: 1.0000000, Test Acc: 0.5666667\n",
            "Epoch: 054, Train Loss: 0.0190831, Train Acc: 1.0000000, Test Acc: 0.5833333\n",
            "Epoch: 055, Train Loss: 0.0167597, Train Acc: 1.0000000, Test Acc: 0.5833333\n",
            "Epoch: 056, Train Loss: 0.0160979, Train Acc: 1.0000000, Test Acc: 0.6000000\n",
            "Epoch: 057, Train Loss: 0.0157122, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 058, Train Loss: 0.0129078, Train Acc: 1.0000000, Test Acc: 0.6000000\n",
            "Epoch: 059, Train Loss: 0.0120848, Train Acc: 1.0000000, Test Acc: 0.5500000\n",
            "Epoch: 060, Train Loss: 0.0108836, Train Acc: 1.0000000, Test Acc: 0.5833333\n",
            "Epoch: 061, Train Loss: 0.0101971, Train Acc: 1.0000000, Test Acc: 0.5166667\n",
            "Epoch: 062, Train Loss: 0.0098997, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 063, Train Loss: 0.0092099, Train Acc: 1.0000000, Test Acc: 0.5500000\n",
            "Epoch: 064, Train Loss: 0.0079787, Train Acc: 1.0000000, Test Acc: 0.5500000\n",
            "Epoch: 065, Train Loss: 0.0078314, Train Acc: 1.0000000, Test Acc: 0.5666667\n",
            "Epoch: 066, Train Loss: 0.0074264, Train Acc: 1.0000000, Test Acc: 0.5500000\n",
            "Epoch: 067, Train Loss: 0.0139206, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 068, Train Loss: 0.0097336, Train Acc: 1.0000000, Test Acc: 0.5500000\n",
            "Epoch: 069, Train Loss: 0.0066463, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 070, Train Loss: 0.0071696, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 071, Train Loss: 0.0064708, Train Acc: 1.0000000, Test Acc: 0.5666667\n",
            "Epoch: 072, Train Loss: 0.0063571, Train Acc: 1.0000000, Test Acc: 0.5833333\n",
            "Epoch: 073, Train Loss: 0.0058068, Train Acc: 1.0000000, Test Acc: 0.5500000\n",
            "Epoch: 074, Train Loss: 0.0050812, Train Acc: 1.0000000, Test Acc: 0.5500000\n",
            "Epoch: 075, Train Loss: 0.0051308, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 076, Train Loss: 0.0049155, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 077, Train Loss: 0.0049017, Train Acc: 1.0000000, Test Acc: 0.5500000\n",
            "Epoch: 078, Train Loss: 0.0044016, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 079, Train Loss: 0.0047107, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 080, Train Loss: 0.0042789, Train Acc: 1.0000000, Test Acc: 0.5500000\n",
            "Epoch: 081, Train Loss: 0.0045521, Train Acc: 1.0000000, Test Acc: 0.5500000\n",
            "Epoch: 082, Train Loss: 0.0046994, Train Acc: 1.0000000, Test Acc: 0.5500000\n",
            "Epoch: 083, Train Loss: 0.0038711, Train Acc: 1.0000000, Test Acc: 0.5500000\n",
            "Epoch: 084, Train Loss: 0.0042113, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 085, Train Loss: 0.0035725, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 086, Train Loss: 0.0036127, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 087, Train Loss: 0.0033666, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 088, Train Loss: 0.0031488, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 089, Train Loss: 0.0031250, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 090, Train Loss: 0.0030299, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 091, Train Loss: 0.0028364, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 092, Train Loss: 0.0028487, Train Acc: 1.0000000, Test Acc: 0.5500000\n",
            "Epoch: 093, Train Loss: 0.0027145, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 094, Train Loss: 0.0027541, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 095, Train Loss: 0.0027417, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 096, Train Loss: 0.0027909, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 097, Train Loss: 0.0027331, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 098, Train Loss: 0.0027103, Train Acc: 1.0000000, Test Acc: 0.5500000\n",
            "Epoch: 099, Train Loss: 0.0023797, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 100, Train Loss: 0.0022462, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 101, Train Loss: 0.0022424, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 102, Train Loss: 0.0022136, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 103, Train Loss: 0.0022812, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 104, Train Loss: 0.0021367, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 105, Train Loss: 0.0033131, Train Acc: 1.0000000, Test Acc: 0.5666667\n",
            "Epoch: 106, Train Loss: 0.0023985, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 107, Train Loss: 0.0022405, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 108, Train Loss: 0.0022153, Train Acc: 1.0000000, Test Acc: 0.5500000\n",
            "Epoch: 109, Train Loss: 0.0019170, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 110, Train Loss: 0.0019559, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 111, Train Loss: 0.0019436, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 112, Train Loss: 0.0019013, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 113, Train Loss: 0.0018810, Train Acc: 1.0000000, Test Acc: 0.5500000\n",
            "Epoch: 114, Train Loss: 0.0020085, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 115, Train Loss: 0.0019578, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 116, Train Loss: 0.0019695, Train Acc: 1.0000000, Test Acc: 0.5500000\n",
            "Epoch: 117, Train Loss: 0.0020321, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 118, Train Loss: 0.0019783, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 119, Train Loss: 0.0017718, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 120, Train Loss: 0.0017416, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 121, Train Loss: 0.0017143, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 122, Train Loss: 0.0016476, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 123, Train Loss: 0.0015185, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 124, Train Loss: 0.0016164, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 125, Train Loss: 0.0017800, Train Acc: 1.0000000, Test Acc: 0.5500000\n",
            "Epoch: 126, Train Loss: 0.0023569, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 127, Train Loss: 0.0021059, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 128, Train Loss: 0.0019495, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 129, Train Loss: 0.0016859, Train Acc: 1.0000000, Test Acc: 0.5166667\n",
            "Epoch: 130, Train Loss: 0.0018479, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 131, Train Loss: 0.0015374, Train Acc: 1.0000000, Test Acc: 0.5500000\n",
            "Epoch: 132, Train Loss: 0.0014225, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 133, Train Loss: 0.0013541, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 134, Train Loss: 0.0013467, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 135, Train Loss: 0.0012864, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 136, Train Loss: 0.0012730, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 137, Train Loss: 0.0012669, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 138, Train Loss: 0.0012326, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 139, Train Loss: 0.0012379, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 140, Train Loss: 0.0012833, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 141, Train Loss: 0.0013764, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 142, Train Loss: 0.0012588, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 143, Train Loss: 0.0012839, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 144, Train Loss: 0.0012640, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 145, Train Loss: 0.0013420, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 146, Train Loss: 0.0011109, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 147, Train Loss: 0.0011358, Train Acc: 1.0000000, Test Acc: 0.5500000\n",
            "Epoch: 148, Train Loss: 0.0013081, Train Acc: 1.0000000, Test Acc: 0.5500000\n",
            "Epoch: 149, Train Loss: 0.0012090, Train Acc: 1.0000000, Test Acc: 0.5500000\n",
            "Epoch: 150, Train Loss: 0.0011175, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 151, Train Loss: 0.0012009, Train Acc: 1.0000000, Test Acc: 0.5500000\n",
            "Epoch: 152, Train Loss: 0.0011381, Train Acc: 1.0000000, Test Acc: 0.5500000\n",
            "Epoch: 153, Train Loss: 0.0011245, Train Acc: 1.0000000, Test Acc: 0.5500000\n",
            "Epoch: 154, Train Loss: 0.0011201, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 155, Train Loss: 0.0010411, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 156, Train Loss: 0.0009783, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 157, Train Loss: 0.0009725, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 158, Train Loss: 0.0009821, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 159, Train Loss: 0.0009647, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 160, Train Loss: 0.0010566, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 161, Train Loss: 0.0010581, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 162, Train Loss: 0.0011283, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 163, Train Loss: 0.0010529, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 164, Train Loss: 0.0010579, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 165, Train Loss: 0.0011888, Train Acc: 1.0000000, Test Acc: 0.5666667\n",
            "Epoch: 166, Train Loss: 0.0010795, Train Acc: 1.0000000, Test Acc: 0.5666667\n",
            "Epoch: 167, Train Loss: 0.0010051, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 168, Train Loss: 0.0009538, Train Acc: 1.0000000, Test Acc: 0.5500000\n",
            "Epoch: 169, Train Loss: 0.0009243, Train Acc: 1.0000000, Test Acc: 0.5500000\n",
            "Epoch: 170, Train Loss: 0.0010220, Train Acc: 1.0000000, Test Acc: 0.5500000\n",
            "Epoch: 171, Train Loss: 0.0009837, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 172, Train Loss: 0.0009164, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 173, Train Loss: 0.0010077, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 174, Train Loss: 0.0009092, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 175, Train Loss: 0.0009179, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 176, Train Loss: 0.0009091, Train Acc: 1.0000000, Test Acc: 0.5666667\n",
            "Epoch: 177, Train Loss: 0.0008672, Train Acc: 1.0000000, Test Acc: 0.5500000\n",
            "Epoch: 178, Train Loss: 0.0008960, Train Acc: 1.0000000, Test Acc: 0.5500000\n",
            "Epoch: 179, Train Loss: 0.0008589, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 180, Train Loss: 0.0008184, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 181, Train Loss: 0.0008283, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 182, Train Loss: 0.0007835, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 183, Train Loss: 0.0007778, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 184, Train Loss: 0.0008427, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 185, Train Loss: 0.0008936, Train Acc: 1.0000000, Test Acc: 0.5500000\n",
            "Epoch: 186, Train Loss: 0.0007943, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 187, Train Loss: 0.0007926, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 188, Train Loss: 0.0008391, Train Acc: 1.0000000, Test Acc: 0.5500000\n",
            "Epoch: 189, Train Loss: 0.0008339, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 190, Train Loss: 0.0007867, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 191, Train Loss: 0.0008495, Train Acc: 1.0000000, Test Acc: 0.5500000\n",
            "Epoch: 192, Train Loss: 0.0008486, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 193, Train Loss: 0.0009181, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 194, Train Loss: 0.0009008, Train Acc: 1.0000000, Test Acc: 0.5500000\n",
            "Epoch: 195, Train Loss: 0.0008407, Train Acc: 1.0000000, Test Acc: 0.5500000\n",
            "Epoch: 196, Train Loss: 0.0007526, Train Acc: 1.0000000, Test Acc: 0.5500000\n",
            "Epoch: 197, Train Loss: 0.0007349, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 198, Train Loss: 0.0008006, Train Acc: 1.0000000, Test Acc: 0.5500000\n",
            "Epoch: 199, Train Loss: 0.0007983, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 200, Train Loss: 0.0007434, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 201, Train Loss: 0.0007396, Train Acc: 1.0000000, Test Acc: 0.5500000\n",
            "Epoch: 202, Train Loss: 0.0007221, Train Acc: 1.0000000, Test Acc: 0.5500000\n",
            "Epoch: 203, Train Loss: 0.0007143, Train Acc: 1.0000000, Test Acc: 0.5666667\n",
            "Epoch: 204, Train Loss: 0.0008740, Train Acc: 1.0000000, Test Acc: 0.5500000\n",
            "Epoch: 205, Train Loss: 0.0007487, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 206, Train Loss: 0.0007367, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 207, Train Loss: 0.0007990, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 208, Train Loss: 0.0007676, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 209, Train Loss: 0.0007618, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 210, Train Loss: 0.0007312, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 211, Train Loss: 0.0007090, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 212, Train Loss: 0.0006669, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 213, Train Loss: 0.0006718, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 214, Train Loss: 0.0006620, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 215, Train Loss: 0.0006665, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 216, Train Loss: 0.0006651, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 217, Train Loss: 0.0006517, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 218, Train Loss: 0.0006364, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 219, Train Loss: 0.0007066, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 220, Train Loss: 0.0006355, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 221, Train Loss: 0.0006329, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 222, Train Loss: 0.0006876, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 223, Train Loss: 0.0006882, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 224, Train Loss: 0.0006618, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 225, Train Loss: 0.0006823, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 226, Train Loss: 0.0006315, Train Acc: 1.0000000, Test Acc: 0.5500000\n",
            "Epoch: 227, Train Loss: 0.0006305, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 228, Train Loss: 0.0006017, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 229, Train Loss: 0.0006196, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 230, Train Loss: 0.0006337, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 231, Train Loss: 0.0006179, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 232, Train Loss: 0.0005808, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 233, Train Loss: 0.0006017, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 234, Train Loss: 0.0006044, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 235, Train Loss: 0.0006059, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 236, Train Loss: 0.0006102, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 237, Train Loss: 0.0005936, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 238, Train Loss: 0.0005836, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 239, Train Loss: 0.0005967, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 240, Train Loss: 0.0005774, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 241, Train Loss: 0.0006161, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 242, Train Loss: 0.0005709, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 243, Train Loss: 0.0005749, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 244, Train Loss: 0.0005811, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 245, Train Loss: 0.0006308, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 246, Train Loss: 0.0006045, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 247, Train Loss: 0.0005639, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 248, Train Loss: 0.0005815, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 249, Train Loss: 0.0005463, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 250, Train Loss: 0.0005472, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 251, Train Loss: 0.0005889, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 252, Train Loss: 0.0005543, Train Acc: 1.0000000, Test Acc: 0.5500000\n",
            "Epoch: 253, Train Loss: 0.0005575, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 254, Train Loss: 0.0005629, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 255, Train Loss: 0.0005335, Train Acc: 1.0000000, Test Acc: 0.5500000\n",
            "Epoch: 256, Train Loss: 0.0005247, Train Acc: 1.0000000, Test Acc: 0.5500000\n",
            "Epoch: 257, Train Loss: 0.0005745, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 258, Train Loss: 0.0006896, Train Acc: 1.0000000, Test Acc: 0.5500000\n",
            "Epoch: 259, Train Loss: 0.0007622, Train Acc: 1.0000000, Test Acc: 0.5500000\n",
            "Epoch: 260, Train Loss: 0.0007332, Train Acc: 1.0000000, Test Acc: 0.5500000\n",
            "Epoch: 261, Train Loss: 0.0006661, Train Acc: 1.0000000, Test Acc: 0.5500000\n",
            "Epoch: 262, Train Loss: 0.0006141, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 263, Train Loss: 0.0005598, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 264, Train Loss: 0.0005498, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 265, Train Loss: 0.0005283, Train Acc: 1.0000000, Test Acc: 0.5500000\n",
            "Epoch: 266, Train Loss: 0.0005044, Train Acc: 1.0000000, Test Acc: 0.5500000\n",
            "Epoch: 267, Train Loss: 0.0004946, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 268, Train Loss: 0.0004949, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 269, Train Loss: 0.0005456, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 270, Train Loss: 0.0005149, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 271, Train Loss: 0.0004851, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 272, Train Loss: 0.0005035, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 273, Train Loss: 0.0005215, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 274, Train Loss: 0.0004960, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 275, Train Loss: 0.0004878, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 276, Train Loss: 0.0005807, Train Acc: 1.0000000, Test Acc: 0.5500000\n",
            "Epoch: 277, Train Loss: 0.0005476, Train Acc: 1.0000000, Test Acc: 0.5500000\n",
            "Epoch: 278, Train Loss: 0.0005146, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 279, Train Loss: 0.0005291, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 280, Train Loss: 0.0005838, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 281, Train Loss: 0.0005277, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 282, Train Loss: 0.0005157, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 283, Train Loss: 0.0005152, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 284, Train Loss: 0.0004924, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 285, Train Loss: 0.0005045, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 286, Train Loss: 0.0004936, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 287, Train Loss: 0.0004887, Train Acc: 1.0000000, Test Acc: 0.5500000\n",
            "Epoch: 288, Train Loss: 0.0004812, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 289, Train Loss: 0.0005057, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 290, Train Loss: 0.0005007, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 291, Train Loss: 0.0005089, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 292, Train Loss: 0.0005097, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 293, Train Loss: 0.0005040, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 294, Train Loss: 0.0004973, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 295, Train Loss: 0.0005148, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 296, Train Loss: 0.0004722, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 297, Train Loss: 0.0004704, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 298, Train Loss: 0.0004726, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 299, Train Loss: 0.0004734, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 300, Train Loss: 0.0004706, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 301, Train Loss: 0.0006350, Train Acc: 1.0000000, Test Acc: 0.5500000\n",
            "Epoch: 302, Train Loss: 0.0006495, Train Acc: 1.0000000, Test Acc: 0.5500000\n",
            "Epoch: 303, Train Loss: 0.0006079, Train Acc: 1.0000000, Test Acc: 0.5500000\n",
            "Epoch: 304, Train Loss: 0.0006490, Train Acc: 1.0000000, Test Acc: 0.5500000\n",
            "Epoch: 305, Train Loss: 0.0005533, Train Acc: 1.0000000, Test Acc: 0.5500000\n",
            "Epoch: 306, Train Loss: 0.0005184, Train Acc: 1.0000000, Test Acc: 0.5500000\n",
            "Epoch: 307, Train Loss: 0.0005003, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 308, Train Loss: 0.0005058, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 309, Train Loss: 0.0004820, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 310, Train Loss: 0.0004802, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 311, Train Loss: 0.0004604, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 312, Train Loss: 0.0004613, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 313, Train Loss: 0.0004553, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 314, Train Loss: 0.0004636, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 315, Train Loss: 0.0004597, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 316, Train Loss: 0.0004629, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 317, Train Loss: 0.0004610, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 318, Train Loss: 0.0004785, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 319, Train Loss: 0.0004824, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 320, Train Loss: 0.0004553, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 321, Train Loss: 0.0005108, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 322, Train Loss: 0.0004567, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 323, Train Loss: 0.0004421, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 324, Train Loss: 0.0004539, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 325, Train Loss: 0.0005215, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 326, Train Loss: 0.0004540, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 327, Train Loss: 0.0004380, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 328, Train Loss: 0.0004739, Train Acc: 1.0000000, Test Acc: 0.5500000\n",
            "Epoch: 329, Train Loss: 0.0004552, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 330, Train Loss: 0.0004431, Train Acc: 1.0000000, Test Acc: 0.5500000\n",
            "Epoch: 331, Train Loss: 0.0004552, Train Acc: 1.0000000, Test Acc: 0.5500000\n",
            "Epoch: 332, Train Loss: 0.0004494, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 333, Train Loss: 0.0004497, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 334, Train Loss: 0.0004375, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 335, Train Loss: 0.0004435, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 336, Train Loss: 0.0004408, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 337, Train Loss: 0.0004469, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 338, Train Loss: 0.0004194, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 339, Train Loss: 0.0004044, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 340, Train Loss: 0.0004891, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 341, Train Loss: 0.0004359, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 342, Train Loss: 0.0005017, Train Acc: 1.0000000, Test Acc: 0.5500000\n",
            "Epoch: 343, Train Loss: 0.0004499, Train Acc: 1.0000000, Test Acc: 0.5500000\n",
            "Epoch: 344, Train Loss: 0.0004276, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 345, Train Loss: 0.0004510, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 346, Train Loss: 0.0004372, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 347, Train Loss: 0.0004322, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 348, Train Loss: 0.0004180, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 349, Train Loss: 0.0004297, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 350, Train Loss: 0.0004129, Train Acc: 1.0000000, Test Acc: 0.5500000\n",
            "Epoch: 351, Train Loss: 0.0004278, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 352, Train Loss: 0.0004402, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 353, Train Loss: 0.0004208, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 354, Train Loss: 0.0004406, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 355, Train Loss: 0.0004231, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 356, Train Loss: 0.0004283, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 357, Train Loss: 0.0004107, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 358, Train Loss: 0.0003996, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 359, Train Loss: 0.0003959, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 360, Train Loss: 0.0004015, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 361, Train Loss: 0.0004081, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 362, Train Loss: 0.0004278, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 363, Train Loss: 0.0004452, Train Acc: 1.0000000, Test Acc: 0.5500000\n",
            "Epoch: 364, Train Loss: 0.0004008, Train Acc: 1.0000000, Test Acc: 0.5500000\n",
            "Epoch: 365, Train Loss: 0.0004089, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 366, Train Loss: 0.0004185, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 367, Train Loss: 0.0004103, Train Acc: 1.0000000, Test Acc: 0.5500000\n",
            "Epoch: 368, Train Loss: 0.0004080, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 369, Train Loss: 0.0004018, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 370, Train Loss: 0.0004002, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 371, Train Loss: 0.0003981, Train Acc: 1.0000000, Test Acc: 0.5500000\n",
            "Epoch: 372, Train Loss: 0.0004019, Train Acc: 1.0000000, Test Acc: 0.5500000\n",
            "Epoch: 373, Train Loss: 0.0003967, Train Acc: 1.0000000, Test Acc: 0.5500000\n",
            "Epoch: 374, Train Loss: 0.0004027, Train Acc: 1.0000000, Test Acc: 0.5500000\n",
            "Epoch: 375, Train Loss: 0.0004081, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 376, Train Loss: 0.0003839, Train Acc: 1.0000000, Test Acc: 0.5500000\n",
            "Epoch: 377, Train Loss: 0.0003936, Train Acc: 1.0000000, Test Acc: 0.5500000\n",
            "Epoch: 378, Train Loss: 0.0004571, Train Acc: 1.0000000, Test Acc: 0.5500000\n",
            "Epoch: 379, Train Loss: 0.0004024, Train Acc: 1.0000000, Test Acc: 0.5500000\n",
            "Epoch: 380, Train Loss: 0.0003911, Train Acc: 1.0000000, Test Acc: 0.5500000\n",
            "Epoch: 381, Train Loss: 0.0003885, Train Acc: 1.0000000, Test Acc: 0.5500000\n",
            "Epoch: 382, Train Loss: 0.0003780, Train Acc: 1.0000000, Test Acc: 0.5500000\n",
            "Epoch: 383, Train Loss: 0.0003862, Train Acc: 1.0000000, Test Acc: 0.5500000\n",
            "Epoch: 384, Train Loss: 0.0003842, Train Acc: 1.0000000, Test Acc: 0.5500000\n",
            "Epoch: 385, Train Loss: 0.0003982, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 386, Train Loss: 0.0004303, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 387, Train Loss: 0.0004100, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 388, Train Loss: 0.0004009, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 389, Train Loss: 0.0003899, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 390, Train Loss: 0.0003974, Train Acc: 1.0000000, Test Acc: 0.5500000\n",
            "Epoch: 391, Train Loss: 0.0003798, Train Acc: 1.0000000, Test Acc: 0.5500000\n",
            "Epoch: 392, Train Loss: 0.0003690, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 393, Train Loss: 0.0003966, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 394, Train Loss: 0.0003900, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 395, Train Loss: 0.0003794, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 396, Train Loss: 0.0004017, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 397, Train Loss: 0.0003902, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 398, Train Loss: 0.0003936, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 399, Train Loss: 0.0003933, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 400, Train Loss: 0.0003880, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 401, Train Loss: 0.0003770, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 402, Train Loss: 0.0003982, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 403, Train Loss: 0.0003822, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 404, Train Loss: 0.0003822, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 405, Train Loss: 0.0003817, Train Acc: 1.0000000, Test Acc: 0.5500000\n",
            "Epoch: 406, Train Loss: 0.0003846, Train Acc: 1.0000000, Test Acc: 0.5500000\n",
            "Epoch: 407, Train Loss: 0.0003769, Train Acc: 1.0000000, Test Acc: 0.5500000\n",
            "Epoch: 408, Train Loss: 0.0003708, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 409, Train Loss: 0.0003995, Train Acc: 1.0000000, Test Acc: 0.5500000\n",
            "Epoch: 410, Train Loss: 0.0003844, Train Acc: 1.0000000, Test Acc: 0.5500000\n",
            "Epoch: 411, Train Loss: 0.0003746, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 412, Train Loss: 0.0003628, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 413, Train Loss: 0.0003733, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 414, Train Loss: 0.0003740, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 415, Train Loss: 0.0004150, Train Acc: 1.0000000, Test Acc: 0.5500000\n",
            "Epoch: 416, Train Loss: 0.0003754, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 417, Train Loss: 0.0003882, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 418, Train Loss: 0.0003811, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 419, Train Loss: 0.0003861, Train Acc: 1.0000000, Test Acc: 0.5500000\n",
            "Epoch: 420, Train Loss: 0.0003970, Train Acc: 1.0000000, Test Acc: 0.5500000\n",
            "Epoch: 421, Train Loss: 0.0003881, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 422, Train Loss: 0.0003717, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 423, Train Loss: 0.0003781, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 424, Train Loss: 0.0003754, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 425, Train Loss: 0.0003708, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 426, Train Loss: 0.0003701, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 427, Train Loss: 0.0003782, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 428, Train Loss: 0.0003586, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 429, Train Loss: 0.0003704, Train Acc: 1.0000000, Test Acc: 0.5500000\n",
            "Epoch: 430, Train Loss: 0.0003628, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 431, Train Loss: 0.0003633, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 432, Train Loss: 0.0003530, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 433, Train Loss: 0.0003889, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 434, Train Loss: 0.0003867, Train Acc: 1.0000000, Test Acc: 0.5500000\n",
            "Epoch: 435, Train Loss: 0.0003672, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 436, Train Loss: 0.0003665, Train Acc: 1.0000000, Test Acc: 0.5500000\n",
            "Epoch: 437, Train Loss: 0.0003795, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 438, Train Loss: 0.0003557, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 439, Train Loss: 0.0003443, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 440, Train Loss: 0.0004445, Train Acc: 1.0000000, Test Acc: 0.5500000\n",
            "Epoch: 441, Train Loss: 0.0004208, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 442, Train Loss: 0.0003800, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 443, Train Loss: 0.0003967, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 444, Train Loss: 0.0003737, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 445, Train Loss: 0.0003645, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 446, Train Loss: 0.0003562, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 447, Train Loss: 0.0003695, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 448, Train Loss: 0.0003787, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 449, Train Loss: 0.0003667, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 450, Train Loss: 0.0003629, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 451, Train Loss: 0.0003597, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 452, Train Loss: 0.0003619, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 453, Train Loss: 0.0003481, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 454, Train Loss: 0.0003572, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 455, Train Loss: 0.0003607, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 456, Train Loss: 0.0003388, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 457, Train Loss: 0.0003566, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 458, Train Loss: 0.0003398, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 459, Train Loss: 0.0003316, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 460, Train Loss: 0.0003850, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 461, Train Loss: 0.0003659, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 462, Train Loss: 0.0003594, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 463, Train Loss: 0.0003601, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 464, Train Loss: 0.0003592, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 465, Train Loss: 0.0003411, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 466, Train Loss: 0.0003412, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 467, Train Loss: 0.0003418, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 468, Train Loss: 0.0003462, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 469, Train Loss: 0.0003525, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 470, Train Loss: 0.0003486, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 471, Train Loss: 0.0003935, Train Acc: 1.0000000, Test Acc: 0.5500000\n",
            "Epoch: 472, Train Loss: 0.0003637, Train Acc: 1.0000000, Test Acc: 0.5500000\n",
            "Epoch: 473, Train Loss: 0.0003598, Train Acc: 1.0000000, Test Acc: 0.5500000\n",
            "Epoch: 474, Train Loss: 0.0003632, Train Acc: 1.0000000, Test Acc: 0.5500000\n",
            "Epoch: 475, Train Loss: 0.0003827, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 476, Train Loss: 0.0003536, Train Acc: 1.0000000, Test Acc: 0.5500000\n",
            "Epoch: 477, Train Loss: 0.0003510, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 478, Train Loss: 0.0003437, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 479, Train Loss: 0.0003521, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 480, Train Loss: 0.0003509, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 481, Train Loss: 0.0003907, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 482, Train Loss: 0.0003535, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 483, Train Loss: 0.0003374, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 484, Train Loss: 0.0003644, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 485, Train Loss: 0.0003603, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 486, Train Loss: 0.0003634, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 487, Train Loss: 0.0003741, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 488, Train Loss: 0.0003705, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 489, Train Loss: 0.0003808, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 490, Train Loss: 0.0003854, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 491, Train Loss: 0.0003586, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 492, Train Loss: 0.0003671, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 493, Train Loss: 0.0003486, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 494, Train Loss: 0.0003602, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 495, Train Loss: 0.0003450, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 496, Train Loss: 0.0003472, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 497, Train Loss: 0.0003420, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 498, Train Loss: 0.0003457, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 499, Train Loss: 0.0003502, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "Epoch: 500, Train Loss: 0.0003456, Train Acc: 1.0000000, Test Acc: 0.5333333\n",
            "--------\n",
            "Best Test Acc:   0.6888888888888888_0.09060836905722273, Epoch: 202\n",
            "Best Train Loss: 0.00037854335710613265_4.432594063018823e-05\n",
            "Best Train Acc:  1.0_0.0\n",
            "FOLD3, ENZYMES, 1668698769/_rhoE0.5P18I1_GELU_X_6_256_0_0.001_50_0.6_B64S0W8, BT=0.6888888888888888_0.09060836905722273, BE=202, ID=\n",
            "train_set : test_set = %d : %d 540 60\n",
            "-------- FOLD3 DATASET=ENZYMES, COMMIT_ID=\n",
            "#Params: 1334534\n",
            "Epoch: 001, Train Loss: 5.7308419, Train Acc: 0.1851852, Test Acc: 0.2166667\n",
            "Epoch: 002, Train Loss: 2.9483535, Train Acc: 0.2481481, Test Acc: 0.2666667\n",
            "Epoch: 003, Train Loss: 2.0032263, Train Acc: 0.2666667, Test Acc: 0.2833333\n",
            "Epoch: 004, Train Loss: 1.6429635, Train Acc: 0.3518519, Test Acc: 0.3833333\n",
            "Epoch: 005, Train Loss: 1.6196313, Train Acc: 0.3425926, Test Acc: 0.3333333\n",
            "Epoch: 006, Train Loss: 1.5103304, Train Acc: 0.3925926, Test Acc: 0.4333333\n",
            "Epoch: 007, Train Loss: 1.5431793, Train Acc: 0.3870370, Test Acc: 0.4000000\n",
            "Epoch: 008, Train Loss: 1.5714812, Train Acc: 0.3944444, Test Acc: 0.3666667\n",
            "Epoch: 009, Train Loss: 1.3759778, Train Acc: 0.4888889, Test Acc: 0.4500000\n",
            "Epoch: 010, Train Loss: 1.3858880, Train Acc: 0.4907407, Test Acc: 0.4500000\n",
            "Epoch: 011, Train Loss: 1.4105604, Train Acc: 0.5203704, Test Acc: 0.4666667\n",
            "Epoch: 012, Train Loss: 1.3403472, Train Acc: 0.4888889, Test Acc: 0.4500000\n",
            "Epoch: 013, Train Loss: 1.3589739, Train Acc: 0.4685185, Test Acc: 0.3333333\n",
            "Epoch: 014, Train Loss: 1.3646482, Train Acc: 0.4907407, Test Acc: 0.4666667\n",
            "Epoch: 015, Train Loss: 1.2242459, Train Acc: 0.5314815, Test Acc: 0.5000000\n",
            "Epoch: 016, Train Loss: 1.1520354, Train Acc: 0.5870370, Test Acc: 0.5666667\n",
            "Epoch: 017, Train Loss: 1.2447073, Train Acc: 0.5111111, Test Acc: 0.4833333\n",
            "Epoch: 018, Train Loss: 1.1167353, Train Acc: 0.6000000, Test Acc: 0.5000000\n",
            "Epoch: 019, Train Loss: 0.9940042, Train Acc: 0.6685185, Test Acc: 0.5666667\n",
            "Epoch: 020, Train Loss: 0.9362368, Train Acc: 0.6833333, Test Acc: 0.5833333\n",
            "Epoch: 021, Train Loss: 0.9466291, Train Acc: 0.6648148, Test Acc: 0.5500000\n",
            "Epoch: 022, Train Loss: 1.0976757, Train Acc: 0.5759259, Test Acc: 0.4666667\n",
            "Epoch: 023, Train Loss: 0.8057676, Train Acc: 0.7444444, Test Acc: 0.6333333\n",
            "Epoch: 024, Train Loss: 0.9425253, Train Acc: 0.6333333, Test Acc: 0.5000000\n",
            "Epoch: 025, Train Loss: 0.9411295, Train Acc: 0.6333333, Test Acc: 0.5333333\n",
            "Epoch: 026, Train Loss: 0.8800290, Train Acc: 0.7074074, Test Acc: 0.6333333\n",
            "Epoch: 027, Train Loss: 0.7616218, Train Acc: 0.7166667, Test Acc: 0.5666667\n",
            "Epoch: 028, Train Loss: 0.6202899, Train Acc: 0.8407407, Test Acc: 0.6500000\n",
            "Epoch: 029, Train Loss: 0.8786270, Train Acc: 0.6833333, Test Acc: 0.5000000\n",
            "Epoch: 030, Train Loss: 2.4164292, Train Acc: 0.3592593, Test Acc: 0.3333333\n",
            "Epoch: 031, Train Loss: 0.8839606, Train Acc: 0.6314815, Test Acc: 0.5666667\n",
            "Epoch: 032, Train Loss: 0.9597652, Train Acc: 0.6444444, Test Acc: 0.4833333\n",
            "Epoch: 033, Train Loss: 0.7388262, Train Acc: 0.7537037, Test Acc: 0.5833333\n",
            "Epoch: 034, Train Loss: 0.4738970, Train Acc: 0.8777778, Test Acc: 0.6833333\n",
            "Epoch: 035, Train Loss: 0.5938612, Train Acc: 0.8129630, Test Acc: 0.6666667\n",
            "Epoch: 036, Train Loss: 0.5069067, Train Acc: 0.8425926, Test Acc: 0.6166667\n",
            "Epoch: 037, Train Loss: 0.6592514, Train Acc: 0.7462963, Test Acc: 0.6166667\n",
            "Epoch: 038, Train Loss: 0.7836245, Train Acc: 0.6666667, Test Acc: 0.5333333\n",
            "Epoch: 039, Train Loss: 0.9266753, Train Acc: 0.6666667, Test Acc: 0.5666667\n",
            "Epoch: 040, Train Loss: 0.4214281, Train Acc: 0.8796296, Test Acc: 0.6500000\n",
            "Epoch: 041, Train Loss: 0.3350738, Train Acc: 0.9203704, Test Acc: 0.6500000\n",
            "Epoch: 042, Train Loss: 0.4337101, Train Acc: 0.8425926, Test Acc: 0.6000000\n",
            "Epoch: 043, Train Loss: 0.4328429, Train Acc: 0.8351852, Test Acc: 0.6000000\n",
            "Epoch: 044, Train Loss: 0.4797264, Train Acc: 0.8055556, Test Acc: 0.5833333\n",
            "Epoch: 045, Train Loss: 0.2755366, Train Acc: 0.9166667, Test Acc: 0.7166667\n",
            "Epoch: 046, Train Loss: 0.2221113, Train Acc: 0.9314815, Test Acc: 0.6666667\n",
            "Epoch: 047, Train Loss: 0.4408555, Train Acc: 0.8333333, Test Acc: 0.6333333\n",
            "Epoch: 048, Train Loss: 1.0927714, Train Acc: 0.6351852, Test Acc: 0.5000000\n",
            "Epoch: 049, Train Loss: 0.1665714, Train Acc: 0.9500000, Test Acc: 0.7500000\n",
            "Epoch: 050, Train Loss: 0.2524982, Train Acc: 0.9185185, Test Acc: 0.6833333\n",
            "Epoch: 051, Train Loss: 0.1599078, Train Acc: 0.9537037, Test Acc: 0.7166667\n",
            "Epoch: 052, Train Loss: 0.0816411, Train Acc: 0.9925926, Test Acc: 0.7000000\n",
            "Epoch: 053, Train Loss: 0.0808111, Train Acc: 0.9925926, Test Acc: 0.7666667\n",
            "Epoch: 054, Train Loss: 0.0821432, Train Acc: 0.9851852, Test Acc: 0.7166667\n",
            "Epoch: 055, Train Loss: 0.0547297, Train Acc: 0.9981481, Test Acc: 0.7166667\n",
            "Epoch: 056, Train Loss: 0.0419573, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 057, Train Loss: 0.0318017, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 058, Train Loss: 0.0274815, Train Acc: 1.0000000, Test Acc: 0.7500000\n",
            "Epoch: 059, Train Loss: 0.0246076, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 060, Train Loss: 0.0248836, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 061, Train Loss: 0.0203007, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 062, Train Loss: 0.0190229, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 063, Train Loss: 0.0174413, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 064, Train Loss: 0.0157915, Train Acc: 1.0000000, Test Acc: 0.7500000\n",
            "Epoch: 065, Train Loss: 0.0147634, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 066, Train Loss: 0.0161825, Train Acc: 1.0000000, Test Acc: 0.7500000\n",
            "Epoch: 067, Train Loss: 0.0169494, Train Acc: 1.0000000, Test Acc: 0.7500000\n",
            "Epoch: 068, Train Loss: 0.0128418, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 069, Train Loss: 0.0122431, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 070, Train Loss: 0.0125667, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 071, Train Loss: 0.0108857, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 072, Train Loss: 0.0105932, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 073, Train Loss: 0.0097822, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 074, Train Loss: 0.0096159, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 075, Train Loss: 0.0088836, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 076, Train Loss: 0.0082717, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 077, Train Loss: 0.0079907, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 078, Train Loss: 0.0085100, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 079, Train Loss: 0.0102375, Train Acc: 0.9981481, Test Acc: 0.7666667\n",
            "Epoch: 080, Train Loss: 0.0214852, Train Acc: 0.9944444, Test Acc: 0.7500000\n",
            "Epoch: 081, Train Loss: 0.0202065, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 082, Train Loss: 0.0098135, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 083, Train Loss: 0.0094369, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 084, Train Loss: 0.0107247, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 085, Train Loss: 0.0090530, Train Acc: 1.0000000, Test Acc: 0.7500000\n",
            "Epoch: 086, Train Loss: 0.0089343, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 087, Train Loss: 0.0066885, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 088, Train Loss: 0.0065618, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 089, Train Loss: 0.0054520, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 090, Train Loss: 0.0048474, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 091, Train Loss: 0.0056154, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 092, Train Loss: 0.0047662, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 093, Train Loss: 0.0048973, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 094, Train Loss: 0.0045149, Train Acc: 1.0000000, Test Acc: 0.7500000\n",
            "Epoch: 095, Train Loss: 0.0051291, Train Acc: 1.0000000, Test Acc: 0.7500000\n",
            "Epoch: 096, Train Loss: 0.0048664, Train Acc: 1.0000000, Test Acc: 0.7500000\n",
            "Epoch: 097, Train Loss: 0.0057914, Train Acc: 1.0000000, Test Acc: 0.7500000\n",
            "Epoch: 098, Train Loss: 0.0038162, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 099, Train Loss: 0.0036232, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 100, Train Loss: 0.0035328, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 101, Train Loss: 0.0034391, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 102, Train Loss: 0.0031742, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 103, Train Loss: 0.0029903, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 104, Train Loss: 0.0031818, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 105, Train Loss: 0.0028773, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 106, Train Loss: 0.0028086, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 107, Train Loss: 0.0030219, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 108, Train Loss: 0.0028830, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 109, Train Loss: 0.0038921, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 110, Train Loss: 0.0033105, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 111, Train Loss: 0.0034065, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 112, Train Loss: 0.0035547, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 113, Train Loss: 0.0026573, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 114, Train Loss: 0.0026476, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 115, Train Loss: 0.0025993, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 116, Train Loss: 0.0024446, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 117, Train Loss: 0.0024127, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 118, Train Loss: 0.0024894, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 119, Train Loss: 0.0023209, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 120, Train Loss: 0.0023281, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 121, Train Loss: 0.0022014, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 122, Train Loss: 0.0022350, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 123, Train Loss: 0.0024217, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 124, Train Loss: 0.0021752, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 125, Train Loss: 0.0021662, Train Acc: 1.0000000, Test Acc: 0.7500000\n",
            "Epoch: 126, Train Loss: 0.0020033, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 127, Train Loss: 0.0027043, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 128, Train Loss: 0.0021045, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 129, Train Loss: 0.0021993, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 130, Train Loss: 0.0021153, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 131, Train Loss: 0.0023361, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 132, Train Loss: 0.0021006, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 133, Train Loss: 0.0022758, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 134, Train Loss: 0.0018202, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 135, Train Loss: 0.0021548, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 136, Train Loss: 0.0022727, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 137, Train Loss: 0.0022868, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 138, Train Loss: 0.0021845, Train Acc: 1.0000000, Test Acc: 0.7500000\n",
            "Epoch: 139, Train Loss: 0.0019646, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 140, Train Loss: 0.0018762, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 141, Train Loss: 0.0016919, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 142, Train Loss: 0.0016759, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 143, Train Loss: 0.0015905, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 144, Train Loss: 0.0015478, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 145, Train Loss: 0.0015301, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 146, Train Loss: 0.0014466, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 147, Train Loss: 0.0014544, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 148, Train Loss: 0.0016464, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 149, Train Loss: 0.0020832, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 150, Train Loss: 0.0017748, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 151, Train Loss: 0.0015343, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 152, Train Loss: 0.0014244, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 153, Train Loss: 0.0016205, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 154, Train Loss: 0.0015226, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 155, Train Loss: 0.0015887, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 156, Train Loss: 0.0013493, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 157, Train Loss: 0.0012298, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 158, Train Loss: 0.0013218, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 159, Train Loss: 0.0013053, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 160, Train Loss: 0.0012168, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 161, Train Loss: 0.0011805, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 162, Train Loss: 0.0011443, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 163, Train Loss: 0.0011873, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 164, Train Loss: 0.0011626, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 165, Train Loss: 0.0011555, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 166, Train Loss: 0.0011965, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 167, Train Loss: 0.0011469, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 168, Train Loss: 0.0011290, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 169, Train Loss: 0.0011096, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 170, Train Loss: 0.0010960, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 171, Train Loss: 0.0010837, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 172, Train Loss: 0.0010811, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 173, Train Loss: 0.0011415, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 174, Train Loss: 0.0010742, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 175, Train Loss: 0.0010480, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 176, Train Loss: 0.0010402, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 177, Train Loss: 0.0010227, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 178, Train Loss: 0.0010593, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 179, Train Loss: 0.0011103, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 180, Train Loss: 0.0010380, Train Acc: 1.0000000, Test Acc: 0.7500000\n",
            "Epoch: 181, Train Loss: 0.0010003, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 182, Train Loss: 0.0009641, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 183, Train Loss: 0.0009551, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 184, Train Loss: 0.0009712, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 185, Train Loss: 0.0009958, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 186, Train Loss: 0.0009266, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 187, Train Loss: 0.0009462, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 188, Train Loss: 0.0009062, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 189, Train Loss: 0.0008824, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 190, Train Loss: 0.0013974, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 191, Train Loss: 0.0012445, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 192, Train Loss: 0.0011281, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 193, Train Loss: 0.0009757, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 194, Train Loss: 0.0009469, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 195, Train Loss: 0.0009206, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 196, Train Loss: 0.0008769, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 197, Train Loss: 0.0008664, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 198, Train Loss: 0.0008544, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 199, Train Loss: 0.0008174, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 200, Train Loss: 0.0008247, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 201, Train Loss: 0.0008284, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 202, Train Loss: 0.0007819, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 203, Train Loss: 0.0007705, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 204, Train Loss: 0.0007768, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 205, Train Loss: 0.0008055, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 206, Train Loss: 0.0008123, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 207, Train Loss: 0.0008019, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 208, Train Loss: 0.0007707, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 209, Train Loss: 0.0007598, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 210, Train Loss: 0.0008533, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 211, Train Loss: 0.0008307, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 212, Train Loss: 0.0008193, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 213, Train Loss: 0.0007897, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 214, Train Loss: 0.0008777, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 215, Train Loss: 0.0008295, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 216, Train Loss: 0.0007988, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 217, Train Loss: 0.0008075, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 218, Train Loss: 0.0008710, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 219, Train Loss: 0.0019324, Train Acc: 1.0000000, Test Acc: 0.7500000\n",
            "Epoch: 220, Train Loss: 0.0010773, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 221, Train Loss: 0.0009997, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 222, Train Loss: 0.0009239, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 223, Train Loss: 0.0008267, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 224, Train Loss: 0.0008749, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 225, Train Loss: 0.0008206, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 226, Train Loss: 0.0008109, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 227, Train Loss: 0.0007529, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 228, Train Loss: 0.0007192, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 229, Train Loss: 0.0007220, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 230, Train Loss: 0.0007179, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 231, Train Loss: 0.0007355, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 232, Train Loss: 0.0007087, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 233, Train Loss: 0.0008142, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 234, Train Loss: 0.0007397, Train Acc: 1.0000000, Test Acc: 0.7500000\n",
            "Epoch: 235, Train Loss: 0.0007427, Train Acc: 1.0000000, Test Acc: 0.7500000\n",
            "Epoch: 236, Train Loss: 0.0007390, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 237, Train Loss: 0.0006989, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 238, Train Loss: 0.0007003, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 239, Train Loss: 0.0006832, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 240, Train Loss: 0.0006541, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 241, Train Loss: 0.0006785, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 242, Train Loss: 0.0006711, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 243, Train Loss: 0.0006513, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 244, Train Loss: 0.0006313, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 245, Train Loss: 0.0006370, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 246, Train Loss: 0.0006503, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 247, Train Loss: 0.0006261, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 248, Train Loss: 0.0006217, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 249, Train Loss: 0.0006137, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 250, Train Loss: 0.0006383, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 251, Train Loss: 0.0006493, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 252, Train Loss: 0.0006569, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 253, Train Loss: 0.0006262, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 254, Train Loss: 0.0006386, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 255, Train Loss: 0.0006067, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 256, Train Loss: 0.0006125, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 257, Train Loss: 0.0005928, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 258, Train Loss: 0.0005953, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 259, Train Loss: 0.0006106, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 260, Train Loss: 0.0006264, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 261, Train Loss: 0.0006173, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 262, Train Loss: 0.0006114, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 263, Train Loss: 0.0006194, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 264, Train Loss: 0.0005996, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 265, Train Loss: 0.0006097, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 266, Train Loss: 0.0006193, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 267, Train Loss: 0.0005760, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 268, Train Loss: 0.0005992, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 269, Train Loss: 0.0006237, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 270, Train Loss: 0.0005696, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 271, Train Loss: 0.0005725, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 272, Train Loss: 0.0006290, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 273, Train Loss: 0.0005761, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 274, Train Loss: 0.0005854, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 275, Train Loss: 0.0005812, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 276, Train Loss: 0.0005860, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 277, Train Loss: 0.0005921, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 278, Train Loss: 0.0005819, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 279, Train Loss: 0.0007768, Train Acc: 1.0000000, Test Acc: 0.7500000\n",
            "Epoch: 280, Train Loss: 0.0006607, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 281, Train Loss: 0.0005754, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 282, Train Loss: 0.0005557, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 283, Train Loss: 0.0005574, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 284, Train Loss: 0.0005405, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 285, Train Loss: 0.0005714, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 286, Train Loss: 0.0005874, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 287, Train Loss: 0.0005827, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 288, Train Loss: 0.0005897, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 289, Train Loss: 0.0005587, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 290, Train Loss: 0.0005577, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 291, Train Loss: 0.0005338, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 292, Train Loss: 0.0005504, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 293, Train Loss: 0.0005272, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 294, Train Loss: 0.0005376, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 295, Train Loss: 0.0005543, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 296, Train Loss: 0.0005550, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 297, Train Loss: 0.0005590, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 298, Train Loss: 0.0005123, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 299, Train Loss: 0.0005090, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 300, Train Loss: 0.0005107, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 301, Train Loss: 0.0005184, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 302, Train Loss: 0.0005032, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 303, Train Loss: 0.0005179, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 304, Train Loss: 0.0005179, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 305, Train Loss: 0.0005247, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 306, Train Loss: 0.0005409, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 307, Train Loss: 0.0005267, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 308, Train Loss: 0.0004928, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 309, Train Loss: 0.0004972, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 310, Train Loss: 0.0005000, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 311, Train Loss: 0.0004904, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 312, Train Loss: 0.0005072, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 313, Train Loss: 0.0005142, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 314, Train Loss: 0.0005035, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 315, Train Loss: 0.0004946, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 316, Train Loss: 0.0004958, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 317, Train Loss: 0.0004846, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 318, Train Loss: 0.0005158, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 319, Train Loss: 0.0005245, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 320, Train Loss: 0.0005039, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 321, Train Loss: 0.0005030, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 322, Train Loss: 0.0004991, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 323, Train Loss: 0.0004972, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 324, Train Loss: 0.0004751, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 325, Train Loss: 0.0004958, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 326, Train Loss: 0.0004843, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 327, Train Loss: 0.0004712, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 328, Train Loss: 0.0005121, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 329, Train Loss: 0.0004775, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 330, Train Loss: 0.0004997, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 331, Train Loss: 0.0005218, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 332, Train Loss: 0.0005356, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 333, Train Loss: 0.0005022, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 334, Train Loss: 0.0004887, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 335, Train Loss: 0.0004808, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 336, Train Loss: 0.0004649, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 337, Train Loss: 0.0004898, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 338, Train Loss: 0.0004745, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 339, Train Loss: 0.0004698, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 340, Train Loss: 0.0004940, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 341, Train Loss: 0.0004671, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 342, Train Loss: 0.0004734, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 343, Train Loss: 0.0004891, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 344, Train Loss: 0.0005659, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 345, Train Loss: 0.0005002, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 346, Train Loss: 0.0006717, Train Acc: 1.0000000, Test Acc: 0.7500000\n",
            "Epoch: 347, Train Loss: 0.0006557, Train Acc: 1.0000000, Test Acc: 0.7500000\n",
            "Epoch: 348, Train Loss: 0.0005843, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 349, Train Loss: 0.0005375, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 350, Train Loss: 0.0005130, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 351, Train Loss: 0.0005405, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 352, Train Loss: 0.0004901, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 353, Train Loss: 0.0004705, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 354, Train Loss: 0.0004698, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 355, Train Loss: 0.0004642, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 356, Train Loss: 0.0004864, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 357, Train Loss: 0.0004791, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 358, Train Loss: 0.0005103, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 359, Train Loss: 0.0005221, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 360, Train Loss: 0.0004741, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 361, Train Loss: 0.0004720, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 362, Train Loss: 0.0004524, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 363, Train Loss: 0.0004925, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 364, Train Loss: 0.0004670, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 365, Train Loss: 0.0004550, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 366, Train Loss: 0.0004579, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 367, Train Loss: 0.0004595, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 368, Train Loss: 0.0004445, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 369, Train Loss: 0.0004592, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 370, Train Loss: 0.0004912, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 371, Train Loss: 0.0004664, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 372, Train Loss: 0.0004565, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 373, Train Loss: 0.0004578, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 374, Train Loss: 0.0004550, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 375, Train Loss: 0.0004367, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 376, Train Loss: 0.0004259, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 377, Train Loss: 0.0004510, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 378, Train Loss: 0.0004490, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 379, Train Loss: 0.0004465, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 380, Train Loss: 0.0004239, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 381, Train Loss: 0.0004264, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 382, Train Loss: 0.0004351, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 383, Train Loss: 0.0004264, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 384, Train Loss: 0.0004503, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 385, Train Loss: 0.0004224, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 386, Train Loss: 0.0004281, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 387, Train Loss: 0.0004333, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 388, Train Loss: 0.0004383, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 389, Train Loss: 0.0004576, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 390, Train Loss: 0.0004400, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 391, Train Loss: 0.0004246, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 392, Train Loss: 0.0004505, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 393, Train Loss: 0.0004415, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 394, Train Loss: 0.0005049, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 395, Train Loss: 0.0004498, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 396, Train Loss: 0.0004286, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 397, Train Loss: 0.0004222, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 398, Train Loss: 0.0004238, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 399, Train Loss: 0.0004086, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 400, Train Loss: 0.0004262, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 401, Train Loss: 0.0004290, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 402, Train Loss: 0.0004348, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 403, Train Loss: 0.0004171, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 404, Train Loss: 0.0004358, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 405, Train Loss: 0.0004341, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 406, Train Loss: 0.0004207, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 407, Train Loss: 0.0004189, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 408, Train Loss: 0.0004771, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 409, Train Loss: 0.0004382, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 410, Train Loss: 0.0004547, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 411, Train Loss: 0.0004407, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 412, Train Loss: 0.0004217, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 413, Train Loss: 0.0004067, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 414, Train Loss: 0.0004216, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 415, Train Loss: 0.0004062, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 416, Train Loss: 0.0004129, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 417, Train Loss: 0.0004114, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 418, Train Loss: 0.0004017, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 419, Train Loss: 0.0004053, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 420, Train Loss: 0.0004967, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 421, Train Loss: 0.0004938, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 422, Train Loss: 0.0005019, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 423, Train Loss: 0.0004328, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 424, Train Loss: 0.0004154, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 425, Train Loss: 0.0004219, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 426, Train Loss: 0.0004037, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 427, Train Loss: 0.0004038, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 428, Train Loss: 0.0004085, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 429, Train Loss: 0.0004062, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 430, Train Loss: 0.0004239, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 431, Train Loss: 0.0004013, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 432, Train Loss: 0.0004098, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 433, Train Loss: 0.0004231, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 434, Train Loss: 0.0004315, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 435, Train Loss: 0.0004162, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 436, Train Loss: 0.0003955, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 437, Train Loss: 0.0003898, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 438, Train Loss: 0.0003846, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 439, Train Loss: 0.0004178, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 440, Train Loss: 0.0004360, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 441, Train Loss: 0.0004200, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 442, Train Loss: 0.0004120, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 443, Train Loss: 0.0003880, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 444, Train Loss: 0.0003868, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 445, Train Loss: 0.0003900, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 446, Train Loss: 0.0004024, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 447, Train Loss: 0.0004084, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 448, Train Loss: 0.0004265, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 449, Train Loss: 0.0004283, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 450, Train Loss: 0.0004271, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 451, Train Loss: 0.0004340, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 452, Train Loss: 0.0004009, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 453, Train Loss: 0.0004077, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 454, Train Loss: 0.0003954, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 455, Train Loss: 0.0003979, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 456, Train Loss: 0.0004130, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 457, Train Loss: 0.0003936, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 458, Train Loss: 0.0003967, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 459, Train Loss: 0.0003849, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 460, Train Loss: 0.0004025, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 461, Train Loss: 0.0003914, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 462, Train Loss: 0.0003973, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 463, Train Loss: 0.0003830, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 464, Train Loss: 0.0003815, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 465, Train Loss: 0.0003777, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 466, Train Loss: 0.0003772, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 467, Train Loss: 0.0003740, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 468, Train Loss: 0.0003790, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 469, Train Loss: 0.0003657, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 470, Train Loss: 0.0003861, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 471, Train Loss: 0.0003785, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 472, Train Loss: 0.0003686, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 473, Train Loss: 0.0003823, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 474, Train Loss: 0.0003769, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 475, Train Loss: 0.0003817, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 476, Train Loss: 0.0003827, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 477, Train Loss: 0.0003876, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 478, Train Loss: 0.0003829, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 479, Train Loss: 0.0003728, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 480, Train Loss: 0.0003602, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 481, Train Loss: 0.0003795, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 482, Train Loss: 0.0003792, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 483, Train Loss: 0.0004050, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 484, Train Loss: 0.0004026, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 485, Train Loss: 0.0004561, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 486, Train Loss: 0.0004128, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 487, Train Loss: 0.0004182, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 488, Train Loss: 0.0003925, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 489, Train Loss: 0.0003803, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 490, Train Loss: 0.0003923, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 491, Train Loss: 0.0003976, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 492, Train Loss: 0.0004678, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 493, Train Loss: 0.0003965, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 494, Train Loss: 0.0003967, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 495, Train Loss: 0.0003709, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 496, Train Loss: 0.0004151, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 497, Train Loss: 0.0004109, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 498, Train Loss: 0.0004204, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 499, Train Loss: 0.0003923, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 500, Train Loss: 0.0004002, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "--------\n",
            "Best Test Acc:   0.7083333333333333_0.09895285072531597, Epoch: 97\n",
            "Best Train Loss: 0.0003739550491715178_3.9201396259212224e-05\n",
            "Best Train Acc:  1.0_0.0\n",
            "FOLD4, ENZYMES, 1668698769/_rhoE0.5P18I1_GELU_X_6_256_0_0.001_50_0.6_B64S0W8, BT=0.7083333333333333_0.09895285072531597, BE=97, ID=\n",
            "train_set : test_set = %d : %d 540 60\n",
            "-------- FOLD4 DATASET=ENZYMES, COMMIT_ID=\n",
            "#Params: 1334534\n",
            "Epoch: 001, Train Loss: 7.3084084, Train Acc: 0.2129630, Test Acc: 0.2000000\n",
            "Epoch: 002, Train Loss: 2.7348726, Train Acc: 0.2111111, Test Acc: 0.1833333\n",
            "Epoch: 003, Train Loss: 1.7380634, Train Acc: 0.3000000, Test Acc: 0.2166667\n",
            "Epoch: 004, Train Loss: 1.7327287, Train Acc: 0.2870370, Test Acc: 0.2333333\n",
            "Epoch: 005, Train Loss: 1.6781827, Train Acc: 0.3037037, Test Acc: 0.2666667\n",
            "Epoch: 006, Train Loss: 1.5007073, Train Acc: 0.4000000, Test Acc: 0.4000000\n",
            "Epoch: 007, Train Loss: 1.5289369, Train Acc: 0.3574074, Test Acc: 0.2333333\n",
            "Epoch: 008, Train Loss: 1.4644933, Train Acc: 0.4185185, Test Acc: 0.3333333\n",
            "Epoch: 009, Train Loss: 1.5616377, Train Acc: 0.4092593, Test Acc: 0.3333333\n",
            "Epoch: 010, Train Loss: 1.3352865, Train Acc: 0.5277778, Test Acc: 0.4166667\n",
            "Epoch: 011, Train Loss: 1.4048910, Train Acc: 0.4333333, Test Acc: 0.3166667\n",
            "Epoch: 012, Train Loss: 1.4444150, Train Acc: 0.4000000, Test Acc: 0.3166667\n",
            "Epoch: 013, Train Loss: 1.5621049, Train Acc: 0.3907407, Test Acc: 0.3000000\n",
            "Epoch: 014, Train Loss: 1.4362195, Train Acc: 0.4833333, Test Acc: 0.4666667\n",
            "Epoch: 015, Train Loss: 1.2853045, Train Acc: 0.5111111, Test Acc: 0.3833333\n",
            "Epoch: 016, Train Loss: 1.4303609, Train Acc: 0.4444444, Test Acc: 0.3833333\n",
            "Epoch: 017, Train Loss: 1.4193361, Train Acc: 0.4388889, Test Acc: 0.4000000\n",
            "Epoch: 018, Train Loss: 1.4461334, Train Acc: 0.4000000, Test Acc: 0.2833333\n",
            "Epoch: 019, Train Loss: 1.1915454, Train Acc: 0.5333333, Test Acc: 0.3833333\n",
            "Epoch: 020, Train Loss: 1.2528680, Train Acc: 0.5462963, Test Acc: 0.4166667\n",
            "Epoch: 021, Train Loss: 1.6405484, Train Acc: 0.4148148, Test Acc: 0.3000000\n",
            "Epoch: 022, Train Loss: 1.2188134, Train Acc: 0.5685185, Test Acc: 0.3833333\n",
            "Epoch: 023, Train Loss: 1.3253098, Train Acc: 0.5037037, Test Acc: 0.3833333\n",
            "Epoch: 024, Train Loss: 1.1410496, Train Acc: 0.5296296, Test Acc: 0.4500000\n",
            "Epoch: 025, Train Loss: 0.9279171, Train Acc: 0.6851852, Test Acc: 0.5166667\n",
            "Epoch: 026, Train Loss: 0.8861841, Train Acc: 0.6907407, Test Acc: 0.4666667\n",
            "Epoch: 027, Train Loss: 0.8726473, Train Acc: 0.7351852, Test Acc: 0.5500000\n",
            "Epoch: 028, Train Loss: 0.7424263, Train Acc: 0.7592593, Test Acc: 0.6000000\n",
            "Epoch: 029, Train Loss: 0.7774584, Train Acc: 0.7500000, Test Acc: 0.5500000\n",
            "Epoch: 030, Train Loss: 0.9073711, Train Acc: 0.6925926, Test Acc: 0.5166667\n",
            "Epoch: 031, Train Loss: 0.6973980, Train Acc: 0.7388889, Test Acc: 0.5000000\n",
            "Epoch: 032, Train Loss: 0.7246993, Train Acc: 0.7555556, Test Acc: 0.6333333\n",
            "Epoch: 033, Train Loss: 0.8255903, Train Acc: 0.6722222, Test Acc: 0.4666667\n",
            "Epoch: 034, Train Loss: 0.7260255, Train Acc: 0.7574074, Test Acc: 0.5333333\n",
            "Epoch: 035, Train Loss: 0.5268790, Train Acc: 0.8462963, Test Acc: 0.6333333\n",
            "Epoch: 036, Train Loss: 0.4698572, Train Acc: 0.8592593, Test Acc: 0.6000000\n",
            "Epoch: 037, Train Loss: 0.4488454, Train Acc: 0.8851852, Test Acc: 0.6000000\n",
            "Epoch: 038, Train Loss: 0.6203724, Train Acc: 0.7444444, Test Acc: 0.4833333\n",
            "Epoch: 039, Train Loss: 0.5266867, Train Acc: 0.8481481, Test Acc: 0.6333333\n",
            "Epoch: 040, Train Loss: 0.5976001, Train Acc: 0.7685185, Test Acc: 0.5166667\n",
            "Epoch: 041, Train Loss: 0.4478453, Train Acc: 0.8481481, Test Acc: 0.5833333\n",
            "Epoch: 042, Train Loss: 0.4821801, Train Acc: 0.8425926, Test Acc: 0.6166667\n",
            "Epoch: 043, Train Loss: 0.7592885, Train Acc: 0.7444444, Test Acc: 0.5666667\n",
            "Epoch: 044, Train Loss: 0.3974753, Train Acc: 0.8685185, Test Acc: 0.6166667\n",
            "Epoch: 045, Train Loss: 0.5273212, Train Acc: 0.8148148, Test Acc: 0.5833333\n",
            "Epoch: 046, Train Loss: 0.3834328, Train Acc: 0.8759259, Test Acc: 0.5666667\n",
            "Epoch: 047, Train Loss: 0.1822705, Train Acc: 0.9777778, Test Acc: 0.6500000\n",
            "Epoch: 048, Train Loss: 0.3150318, Train Acc: 0.9111111, Test Acc: 0.6333333\n",
            "Epoch: 049, Train Loss: 0.2157798, Train Acc: 0.9444444, Test Acc: 0.6333333\n",
            "Epoch: 050, Train Loss: 0.1970382, Train Acc: 0.9555556, Test Acc: 0.6333333\n",
            "Epoch: 051, Train Loss: 0.1054568, Train Acc: 0.9907407, Test Acc: 0.6500000\n",
            "Epoch: 052, Train Loss: 0.0972323, Train Acc: 0.9833333, Test Acc: 0.6500000\n",
            "Epoch: 053, Train Loss: 0.0580409, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 054, Train Loss: 0.0529493, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 055, Train Loss: 0.0504616, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 056, Train Loss: 0.0392223, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 057, Train Loss: 0.0360341, Train Acc: 1.0000000, Test Acc: 0.6500000\n",
            "Epoch: 058, Train Loss: 0.0484815, Train Acc: 0.9981481, Test Acc: 0.6500000\n",
            "Epoch: 059, Train Loss: 0.0465258, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 060, Train Loss: 0.0419915, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 061, Train Loss: 0.0266047, Train Acc: 1.0000000, Test Acc: 0.6500000\n",
            "Epoch: 062, Train Loss: 0.0252358, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 063, Train Loss: 0.0212040, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 064, Train Loss: 0.0199878, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 065, Train Loss: 0.0226326, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 066, Train Loss: 0.0221095, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 067, Train Loss: 0.0177988, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 068, Train Loss: 0.0186147, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 069, Train Loss: 0.0155294, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 070, Train Loss: 0.0271670, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 071, Train Loss: 0.0144585, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 072, Train Loss: 0.0154541, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 073, Train Loss: 0.0231373, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 074, Train Loss: 0.0200729, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 075, Train Loss: 0.0133209, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 076, Train Loss: 0.0167727, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 077, Train Loss: 0.0120405, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 078, Train Loss: 0.0111821, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 079, Train Loss: 0.0101890, Train Acc: 1.0000000, Test Acc: 0.6500000\n",
            "Epoch: 080, Train Loss: 0.0118723, Train Acc: 1.0000000, Test Acc: 0.6500000\n",
            "Epoch: 081, Train Loss: 0.0100328, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 082, Train Loss: 0.0090386, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 083, Train Loss: 0.0090252, Train Acc: 1.0000000, Test Acc: 0.6500000\n",
            "Epoch: 084, Train Loss: 0.0105194, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 085, Train Loss: 0.0081269, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 086, Train Loss: 0.0317724, Train Acc: 0.9944444, Test Acc: 0.6500000\n",
            "Epoch: 087, Train Loss: 0.0597069, Train Acc: 0.9907407, Test Acc: 0.6666667\n",
            "Epoch: 088, Train Loss: 0.0832797, Train Acc: 0.9888889, Test Acc: 0.6166667\n",
            "Epoch: 089, Train Loss: 0.3933583, Train Acc: 0.8666667, Test Acc: 0.6166667\n",
            "Epoch: 090, Train Loss: 0.0829322, Train Acc: 0.9851852, Test Acc: 0.6666667\n",
            "Epoch: 091, Train Loss: 0.1082716, Train Acc: 0.9814815, Test Acc: 0.6166667\n",
            "Epoch: 092, Train Loss: 0.1507463, Train Acc: 0.9555556, Test Acc: 0.6666667\n",
            "Epoch: 093, Train Loss: 0.1415298, Train Acc: 0.9666667, Test Acc: 0.6666667\n",
            "Epoch: 094, Train Loss: 0.1159621, Train Acc: 0.9722222, Test Acc: 0.6500000\n",
            "Epoch: 095, Train Loss: 0.1880958, Train Acc: 0.9462963, Test Acc: 0.6500000\n",
            "Epoch: 096, Train Loss: 0.1348448, Train Acc: 0.9592593, Test Acc: 0.6833333\n",
            "Epoch: 097, Train Loss: 0.0555834, Train Acc: 0.9944444, Test Acc: 0.6666667\n",
            "Epoch: 098, Train Loss: 0.0347214, Train Acc: 0.9981481, Test Acc: 0.6833333\n",
            "Epoch: 099, Train Loss: 0.0214472, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 100, Train Loss: 0.0116938, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 101, Train Loss: 0.0089728, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 102, Train Loss: 0.0070169, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 103, Train Loss: 0.0064662, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 104, Train Loss: 0.0064189, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 105, Train Loss: 0.0056867, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 106, Train Loss: 0.0056445, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 107, Train Loss: 0.0049917, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 108, Train Loss: 0.0049346, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 109, Train Loss: 0.0045544, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 110, Train Loss: 0.0045662, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 111, Train Loss: 0.0042013, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 112, Train Loss: 0.0040899, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 113, Train Loss: 0.0041998, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 114, Train Loss: 0.0040263, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 115, Train Loss: 0.0040600, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 116, Train Loss: 0.0039168, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 117, Train Loss: 0.0038299, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 118, Train Loss: 0.0037551, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 119, Train Loss: 0.0035712, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 120, Train Loss: 0.0031761, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 121, Train Loss: 0.0031903, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 122, Train Loss: 0.0030967, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 123, Train Loss: 0.0033391, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 124, Train Loss: 0.0031318, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 125, Train Loss: 0.0031247, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 126, Train Loss: 0.0029561, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 127, Train Loss: 0.0030459, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 128, Train Loss: 0.0026867, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 129, Train Loss: 0.0028673, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 130, Train Loss: 0.0029199, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 131, Train Loss: 0.0027485, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 132, Train Loss: 0.0025314, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 133, Train Loss: 0.0024884, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 134, Train Loss: 0.0024362, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 135, Train Loss: 0.0022949, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 136, Train Loss: 0.0022661, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 137, Train Loss: 0.0023065, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 138, Train Loss: 0.0022004, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 139, Train Loss: 0.0023676, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 140, Train Loss: 0.0024619, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 141, Train Loss: 0.0020839, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 142, Train Loss: 0.0019414, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 143, Train Loss: 0.0019206, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 144, Train Loss: 0.0018868, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 145, Train Loss: 0.0018932, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 146, Train Loss: 0.0018090, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 147, Train Loss: 0.0017953, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 148, Train Loss: 0.0018527, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 149, Train Loss: 0.0019342, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 150, Train Loss: 0.0018422, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 151, Train Loss: 0.0021262, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 152, Train Loss: 0.0019069, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 153, Train Loss: 0.0017921, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 154, Train Loss: 0.0017095, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 155, Train Loss: 0.0016778, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 156, Train Loss: 0.0016267, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 157, Train Loss: 0.0016582, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 158, Train Loss: 0.0016197, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 159, Train Loss: 0.0015493, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 160, Train Loss: 0.0016015, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 161, Train Loss: 0.0016281, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 162, Train Loss: 0.0016127, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 163, Train Loss: 0.0015963, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 164, Train Loss: 0.0014863, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 165, Train Loss: 0.0015160, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 166, Train Loss: 0.0014638, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 167, Train Loss: 0.0014101, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 168, Train Loss: 0.0014669, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 169, Train Loss: 0.0014003, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 170, Train Loss: 0.0013886, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 171, Train Loss: 0.0013479, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 172, Train Loss: 0.0013808, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 173, Train Loss: 0.0013600, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 174, Train Loss: 0.0013011, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 175, Train Loss: 0.0012797, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 176, Train Loss: 0.0012611, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 177, Train Loss: 0.0013166, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 178, Train Loss: 0.0013377, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 179, Train Loss: 0.0014448, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 180, Train Loss: 0.0013520, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 181, Train Loss: 0.0012257, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 182, Train Loss: 0.0014352, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 183, Train Loss: 0.0013581, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 184, Train Loss: 0.0012370, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 185, Train Loss: 0.0012647, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 186, Train Loss: 0.0012617, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 187, Train Loss: 0.0012064, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 188, Train Loss: 0.0012058, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 189, Train Loss: 0.0011923, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 190, Train Loss: 0.0011734, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 191, Train Loss: 0.0011663, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 192, Train Loss: 0.0011262, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 193, Train Loss: 0.0011405, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 194, Train Loss: 0.0011339, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 195, Train Loss: 0.0011667, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 196, Train Loss: 0.0011434, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 197, Train Loss: 0.0011175, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 198, Train Loss: 0.0012353, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 199, Train Loss: 0.0029238, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 200, Train Loss: 0.0015883, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 201, Train Loss: 0.0013412, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 202, Train Loss: 0.0012304, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 203, Train Loss: 0.0011741, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 204, Train Loss: 0.0011099, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 205, Train Loss: 0.0011305, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 206, Train Loss: 0.0011085, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 207, Train Loss: 0.0010764, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 208, Train Loss: 0.0010647, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 209, Train Loss: 0.0010222, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 210, Train Loss: 0.0010210, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 211, Train Loss: 0.0010337, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 212, Train Loss: 0.0010284, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 213, Train Loss: 0.0010070, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 214, Train Loss: 0.0010278, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 215, Train Loss: 0.0009845, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 216, Train Loss: 0.0009756, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 217, Train Loss: 0.0009706, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 218, Train Loss: 0.0009406, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 219, Train Loss: 0.0009440, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 220, Train Loss: 0.0009596, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 221, Train Loss: 0.0009703, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 222, Train Loss: 0.0009707, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 223, Train Loss: 0.0009472, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 224, Train Loss: 0.0009442, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 225, Train Loss: 0.0009516, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 226, Train Loss: 0.0022493, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 227, Train Loss: 0.0014989, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 228, Train Loss: 0.0012227, Train Acc: 1.0000000, Test Acc: 0.6500000\n",
            "Epoch: 229, Train Loss: 0.0013008, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 230, Train Loss: 0.0011882, Train Acc: 1.0000000, Test Acc: 0.6500000\n",
            "Epoch: 231, Train Loss: 0.0010141, Train Acc: 1.0000000, Test Acc: 0.6500000\n",
            "Epoch: 232, Train Loss: 0.0009413, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 233, Train Loss: 0.0009813, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 234, Train Loss: 0.0009566, Train Acc: 1.0000000, Test Acc: 0.6500000\n",
            "Epoch: 235, Train Loss: 0.0010230, Train Acc: 1.0000000, Test Acc: 0.6500000\n",
            "Epoch: 236, Train Loss: 0.0009667, Train Acc: 1.0000000, Test Acc: 0.6500000\n",
            "Epoch: 237, Train Loss: 0.0009180, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 238, Train Loss: 0.0009109, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 239, Train Loss: 0.0008844, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 240, Train Loss: 0.0008917, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 241, Train Loss: 0.0010097, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 242, Train Loss: 0.0009302, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 243, Train Loss: 0.0008828, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 244, Train Loss: 0.0008746, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 245, Train Loss: 0.0008350, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 246, Train Loss: 0.0008217, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 247, Train Loss: 0.0008317, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 248, Train Loss: 0.0008524, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 249, Train Loss: 0.0008347, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 250, Train Loss: 0.0008259, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 251, Train Loss: 0.0007842, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 252, Train Loss: 0.0007553, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 253, Train Loss: 0.0007592, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 254, Train Loss: 0.0007785, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 255, Train Loss: 0.0007918, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 256, Train Loss: 0.0007964, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 257, Train Loss: 0.0007747, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 258, Train Loss: 0.0008243, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 259, Train Loss: 0.0008158, Train Acc: 1.0000000, Test Acc: 0.6500000\n",
            "Epoch: 260, Train Loss: 0.0008215, Train Acc: 1.0000000, Test Acc: 0.6500000\n",
            "Epoch: 261, Train Loss: 0.0007983, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 262, Train Loss: 0.0008038, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 263, Train Loss: 0.0007859, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 264, Train Loss: 0.0007880, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 265, Train Loss: 0.0008096, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 266, Train Loss: 0.0008077, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 267, Train Loss: 0.0007818, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 268, Train Loss: 0.0007628, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 269, Train Loss: 0.0008076, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 270, Train Loss: 0.0007571, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 271, Train Loss: 0.0007383, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 272, Train Loss: 0.0007429, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 273, Train Loss: 0.0007229, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 274, Train Loss: 0.0007368, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 275, Train Loss: 0.0007479, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 276, Train Loss: 0.0007039, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 277, Train Loss: 0.0007589, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 278, Train Loss: 0.0008962, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 279, Train Loss: 0.0007852, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 280, Train Loss: 0.0008123, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 281, Train Loss: 0.0007577, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 282, Train Loss: 0.0007494, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 283, Train Loss: 0.0007457, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 284, Train Loss: 0.0007457, Train Acc: 1.0000000, Test Acc: 0.6500000\n",
            "Epoch: 285, Train Loss: 0.0007269, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 286, Train Loss: 0.0006993, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 287, Train Loss: 0.0007381, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 288, Train Loss: 0.0007149, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 289, Train Loss: 0.0007022, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 290, Train Loss: 0.0007213, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 291, Train Loss: 0.0007783, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 292, Train Loss: 0.0007191, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 293, Train Loss: 0.0007327, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 294, Train Loss: 0.0007130, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 295, Train Loss: 0.0006911, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 296, Train Loss: 0.0007390, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 297, Train Loss: 0.0007418, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 298, Train Loss: 0.0007240, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 299, Train Loss: 0.0008243, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 300, Train Loss: 0.0007352, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 301, Train Loss: 0.0007000, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 302, Train Loss: 0.0006781, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 303, Train Loss: 0.0006692, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 304, Train Loss: 0.0006871, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 305, Train Loss: 0.0006530, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 306, Train Loss: 0.0006509, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 307, Train Loss: 0.0006760, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 308, Train Loss: 0.0006669, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 309, Train Loss: 0.0006399, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 310, Train Loss: 0.0006496, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 311, Train Loss: 0.0007269, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 312, Train Loss: 0.0007003, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 313, Train Loss: 0.0006731, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 314, Train Loss: 0.0006752, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 315, Train Loss: 0.0006559, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 316, Train Loss: 0.0006593, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 317, Train Loss: 0.0006514, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 318, Train Loss: 0.0006572, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 319, Train Loss: 0.0006897, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 320, Train Loss: 0.0006501, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 321, Train Loss: 0.0006443, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 322, Train Loss: 0.0006293, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 323, Train Loss: 0.0006414, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 324, Train Loss: 0.0006251, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 325, Train Loss: 0.0006282, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 326, Train Loss: 0.0006155, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 327, Train Loss: 0.0006179, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 328, Train Loss: 0.0006168, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 329, Train Loss: 0.0006351, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 330, Train Loss: 0.0006290, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 331, Train Loss: 0.0006196, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 332, Train Loss: 0.0006474, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 333, Train Loss: 0.0006379, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 334, Train Loss: 0.0007221, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 335, Train Loss: 0.0006517, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 336, Train Loss: 0.0006049, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 337, Train Loss: 0.0006160, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 338, Train Loss: 0.0006053, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 339, Train Loss: 0.0006076, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 340, Train Loss: 0.0006126, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 341, Train Loss: 0.0006403, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 342, Train Loss: 0.0006422, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 343, Train Loss: 0.0006084, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 344, Train Loss: 0.0005982, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 345, Train Loss: 0.0006273, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 346, Train Loss: 0.0006029, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 347, Train Loss: 0.0005772, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 348, Train Loss: 0.0006544, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 349, Train Loss: 0.0006003, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 350, Train Loss: 0.0005760, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 351, Train Loss: 0.0005929, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 352, Train Loss: 0.0006213, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 353, Train Loss: 0.0005964, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 354, Train Loss: 0.0005699, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 355, Train Loss: 0.0005783, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 356, Train Loss: 0.0006033, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 357, Train Loss: 0.0005815, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 358, Train Loss: 0.0005789, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 359, Train Loss: 0.0005659, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 360, Train Loss: 0.0005607, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 361, Train Loss: 0.0005699, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 362, Train Loss: 0.0005622, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 363, Train Loss: 0.0006014, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 364, Train Loss: 0.0006321, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 365, Train Loss: 0.0006535, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 366, Train Loss: 0.0006001, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 367, Train Loss: 0.0005615, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 368, Train Loss: 0.0005697, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 369, Train Loss: 0.0005698, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 370, Train Loss: 0.0006050, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 371, Train Loss: 0.0006074, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 372, Train Loss: 0.0005910, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 373, Train Loss: 0.0005963, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 374, Train Loss: 0.0005797, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 375, Train Loss: 0.0005871, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 376, Train Loss: 0.0005781, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 377, Train Loss: 0.0005574, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 378, Train Loss: 0.0005571, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 379, Train Loss: 0.0005581, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 380, Train Loss: 0.0005611, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 381, Train Loss: 0.0005600, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 382, Train Loss: 0.0005397, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 383, Train Loss: 0.0005485, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 384, Train Loss: 0.0005437, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 385, Train Loss: 0.0005316, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 386, Train Loss: 0.0005478, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 387, Train Loss: 0.0005512, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 388, Train Loss: 0.0005396, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 389, Train Loss: 0.0005417, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 390, Train Loss: 0.0005733, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 391, Train Loss: 0.0005672, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 392, Train Loss: 0.0005809, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 393, Train Loss: 0.0005848, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 394, Train Loss: 0.0005644, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 395, Train Loss: 0.0005439, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 396, Train Loss: 0.0005579, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 397, Train Loss: 0.0005605, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 398, Train Loss: 0.0005536, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 399, Train Loss: 0.0005260, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 400, Train Loss: 0.0005379, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 401, Train Loss: 0.0005516, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 402, Train Loss: 0.0005575, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 403, Train Loss: 0.0005564, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 404, Train Loss: 0.0006184, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 405, Train Loss: 0.0005476, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 406, Train Loss: 0.0005367, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 407, Train Loss: 0.0005255, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 408, Train Loss: 0.0005584, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 409, Train Loss: 0.0005618, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 410, Train Loss: 0.0005893, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 411, Train Loss: 0.0005248, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 412, Train Loss: 0.0005194, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 413, Train Loss: 0.0005223, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 414, Train Loss: 0.0005137, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 415, Train Loss: 0.0005560, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 416, Train Loss: 0.0005414, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 417, Train Loss: 0.0005186, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 418, Train Loss: 0.0005025, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 419, Train Loss: 0.0005612, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 420, Train Loss: 0.0005399, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 421, Train Loss: 0.0005295, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 422, Train Loss: 0.0005161, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 423, Train Loss: 0.0005132, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 424, Train Loss: 0.0005242, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 425, Train Loss: 0.0006414, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 426, Train Loss: 0.0005537, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 427, Train Loss: 0.0006228, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 428, Train Loss: 0.0005377, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 429, Train Loss: 0.0005291, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 430, Train Loss: 0.0005047, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 431, Train Loss: 0.0005448, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 432, Train Loss: 0.0005152, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 433, Train Loss: 0.0005069, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 434, Train Loss: 0.0005263, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 435, Train Loss: 0.0005173, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 436, Train Loss: 0.0005016, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 437, Train Loss: 0.0005260, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 438, Train Loss: 0.0005384, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 439, Train Loss: 0.0005065, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 440, Train Loss: 0.0005014, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 441, Train Loss: 0.0005048, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 442, Train Loss: 0.0005045, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 443, Train Loss: 0.0004971, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 444, Train Loss: 0.0005360, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 445, Train Loss: 0.0005104, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 446, Train Loss: 0.0005364, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 447, Train Loss: 0.0005089, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 448, Train Loss: 0.0005127, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 449, Train Loss: 0.0005001, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 450, Train Loss: 0.0005171, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 451, Train Loss: 0.0005479, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 452, Train Loss: 0.0005009, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 453, Train Loss: 0.0005258, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 454, Train Loss: 0.0005127, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 455, Train Loss: 0.0005448, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 456, Train Loss: 0.0005105, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 457, Train Loss: 0.0005458, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 458, Train Loss: 0.0005547, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 459, Train Loss: 0.0005720, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 460, Train Loss: 0.0006167, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 461, Train Loss: 0.0005431, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 462, Train Loss: 0.0005079, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 463, Train Loss: 0.0005243, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 464, Train Loss: 0.0005194, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 465, Train Loss: 0.0005149, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 466, Train Loss: 0.0005230, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 467, Train Loss: 0.0005066, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 468, Train Loss: 0.0004933, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 469, Train Loss: 0.0005778, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 470, Train Loss: 0.0005201, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 471, Train Loss: 0.0005001, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 472, Train Loss: 0.0004838, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 473, Train Loss: 0.0005310, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 474, Train Loss: 0.0005000, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 475, Train Loss: 0.0005059, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 476, Train Loss: 0.0004984, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 477, Train Loss: 0.0005158, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 478, Train Loss: 0.0004889, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 479, Train Loss: 0.0005200, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 480, Train Loss: 0.0004872, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 481, Train Loss: 0.0004997, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 482, Train Loss: 0.0005009, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 483, Train Loss: 0.0005459, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 484, Train Loss: 0.0004961, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 485, Train Loss: 0.0005016, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 486, Train Loss: 0.0005051, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 487, Train Loss: 0.0004858, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 488, Train Loss: 0.0004829, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 489, Train Loss: 0.0004972, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 490, Train Loss: 0.0005235, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 491, Train Loss: 0.0005115, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 492, Train Loss: 0.0004867, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 493, Train Loss: 0.0004803, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 494, Train Loss: 0.0004777, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 495, Train Loss: 0.0005460, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 496, Train Loss: 0.0005082, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 497, Train Loss: 0.0004866, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 498, Train Loss: 0.0005185, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 499, Train Loss: 0.0005263, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 500, Train Loss: 0.0005141, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "--------\n",
            "Best Test Acc:   0.7033333333333334_0.08906926143924924, Epoch: 97\n",
            "Best Train Loss: 0.00039659494669952737_5.726831074208954e-05\n",
            "Best Train Acc:  1.0_0.0\n",
            "FOLD5, ENZYMES, 1668698769/_rhoE0.5P18I1_GELU_X_6_256_0_0.001_50_0.6_B64S0W8, BT=0.7033333333333334_0.08906926143924924, BE=97, ID=\n",
            "train_set : test_set = %d : %d 540 60\n",
            "-------- FOLD5 DATASET=ENZYMES, COMMIT_ID=\n",
            "#Params: 1334534\n",
            "Epoch: 001, Train Loss: 3.1771303, Train Acc: 0.2148148, Test Acc: 0.2666667\n",
            "Epoch: 002, Train Loss: 2.4806058, Train Acc: 0.2351852, Test Acc: 0.2000000\n",
            "Epoch: 003, Train Loss: 1.8124258, Train Acc: 0.2685185, Test Acc: 0.3166667\n",
            "Epoch: 004, Train Loss: 1.7151808, Train Acc: 0.2666667, Test Acc: 0.3000000\n",
            "Epoch: 005, Train Loss: 1.5683551, Train Acc: 0.3685185, Test Acc: 0.3666667\n",
            "Epoch: 006, Train Loss: 1.5746920, Train Acc: 0.3111111, Test Acc: 0.2833333\n",
            "Epoch: 007, Train Loss: 1.4816530, Train Acc: 0.3851852, Test Acc: 0.3166667\n",
            "Epoch: 008, Train Loss: 1.5737558, Train Acc: 0.4018519, Test Acc: 0.3833333\n",
            "Epoch: 009, Train Loss: 1.4571385, Train Acc: 0.4074074, Test Acc: 0.3166667\n",
            "Epoch: 010, Train Loss: 1.3922341, Train Acc: 0.4111111, Test Acc: 0.3833333\n",
            "Epoch: 011, Train Loss: 1.3207440, Train Acc: 0.5277778, Test Acc: 0.4333333\n",
            "Epoch: 012, Train Loss: 1.4258452, Train Acc: 0.3703704, Test Acc: 0.3666667\n",
            "Epoch: 013, Train Loss: 1.1957700, Train Acc: 0.5740741, Test Acc: 0.4833333\n",
            "Epoch: 014, Train Loss: 1.3158122, Train Acc: 0.5500000, Test Acc: 0.4500000\n",
            "Epoch: 015, Train Loss: 1.2940033, Train Acc: 0.5648148, Test Acc: 0.4500000\n",
            "Epoch: 016, Train Loss: 1.3412436, Train Acc: 0.4574074, Test Acc: 0.3000000\n",
            "Epoch: 017, Train Loss: 1.0894748, Train Acc: 0.6462963, Test Acc: 0.4833333\n",
            "Epoch: 018, Train Loss: 1.2258334, Train Acc: 0.5388889, Test Acc: 0.4000000\n",
            "Epoch: 019, Train Loss: 1.5561812, Train Acc: 0.4592593, Test Acc: 0.3833333\n",
            "Epoch: 020, Train Loss: 1.0228807, Train Acc: 0.6444444, Test Acc: 0.4500000\n",
            "Epoch: 021, Train Loss: 0.9910220, Train Acc: 0.6555556, Test Acc: 0.4333333\n",
            "Epoch: 022, Train Loss: 1.1190322, Train Acc: 0.5129630, Test Acc: 0.3500000\n",
            "Epoch: 023, Train Loss: 0.8977836, Train Acc: 0.6833333, Test Acc: 0.5000000\n",
            "Epoch: 024, Train Loss: 0.7950397, Train Acc: 0.7833333, Test Acc: 0.5000000\n",
            "Epoch: 025, Train Loss: 0.9674666, Train Acc: 0.6185185, Test Acc: 0.4666667\n",
            "Epoch: 026, Train Loss: 0.8119603, Train Acc: 0.7129630, Test Acc: 0.5000000\n",
            "Epoch: 027, Train Loss: 0.6331223, Train Acc: 0.8240741, Test Acc: 0.6000000\n",
            "Epoch: 028, Train Loss: 0.7876472, Train Acc: 0.7333333, Test Acc: 0.4000000\n",
            "Epoch: 029, Train Loss: 0.6600553, Train Acc: 0.7648148, Test Acc: 0.4666667\n",
            "Epoch: 030, Train Loss: 0.6742545, Train Acc: 0.7740741, Test Acc: 0.5166667\n",
            "Epoch: 031, Train Loss: 0.5953909, Train Acc: 0.8000000, Test Acc: 0.5333333\n",
            "Epoch: 032, Train Loss: 0.4310084, Train Acc: 0.9018519, Test Acc: 0.6000000\n",
            "Epoch: 033, Train Loss: 0.7946672, Train Acc: 0.7518519, Test Acc: 0.5166667\n",
            "Epoch: 034, Train Loss: 0.5505524, Train Acc: 0.8074074, Test Acc: 0.5000000\n",
            "Epoch: 035, Train Loss: 0.6274854, Train Acc: 0.7703704, Test Acc: 0.5500000\n",
            "Epoch: 036, Train Loss: 0.4687389, Train Acc: 0.8703704, Test Acc: 0.5333333\n",
            "Epoch: 037, Train Loss: 0.2964242, Train Acc: 0.9259259, Test Acc: 0.5833333\n",
            "Epoch: 038, Train Loss: 0.2881171, Train Acc: 0.9462963, Test Acc: 0.6333333\n",
            "Epoch: 039, Train Loss: 0.5115109, Train Acc: 0.8148148, Test Acc: 0.5166667\n",
            "Epoch: 040, Train Loss: 0.2703948, Train Acc: 0.9370370, Test Acc: 0.6500000\n",
            "Epoch: 041, Train Loss: 0.3532329, Train Acc: 0.8981481, Test Acc: 0.6000000\n",
            "Epoch: 042, Train Loss: 0.3091281, Train Acc: 0.8981481, Test Acc: 0.5666667\n",
            "Epoch: 043, Train Loss: 0.1967396, Train Acc: 0.9555556, Test Acc: 0.5666667\n",
            "Epoch: 044, Train Loss: 0.1620203, Train Acc: 0.9814815, Test Acc: 0.6166667\n",
            "Epoch: 045, Train Loss: 0.2132430, Train Acc: 0.9444444, Test Acc: 0.5833333\n",
            "Epoch: 046, Train Loss: 0.3416630, Train Acc: 0.9037037, Test Acc: 0.6166667\n",
            "Epoch: 047, Train Loss: 0.2796555, Train Acc: 0.9018519, Test Acc: 0.5666667\n",
            "Epoch: 048, Train Loss: 0.1485301, Train Acc: 0.9648148, Test Acc: 0.5833333\n",
            "Epoch: 049, Train Loss: 0.1036626, Train Acc: 0.9888889, Test Acc: 0.6166667\n",
            "Epoch: 050, Train Loss: 0.1563078, Train Acc: 0.9611111, Test Acc: 0.7000000\n",
            "Epoch: 051, Train Loss: 0.0570460, Train Acc: 0.9981481, Test Acc: 0.6500000\n",
            "Epoch: 052, Train Loss: 0.0525107, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 053, Train Loss: 0.0617246, Train Acc: 1.0000000, Test Acc: 0.6333333\n",
            "Epoch: 054, Train Loss: 0.0248051, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 055, Train Loss: 0.0240223, Train Acc: 1.0000000, Test Acc: 0.6500000\n",
            "Epoch: 056, Train Loss: 0.0192477, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 057, Train Loss: 0.0181552, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 058, Train Loss: 0.0157400, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 059, Train Loss: 0.0145567, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 060, Train Loss: 0.0139722, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 061, Train Loss: 0.0133724, Train Acc: 1.0000000, Test Acc: 0.6500000\n",
            "Epoch: 062, Train Loss: 0.0128686, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 063, Train Loss: 0.0115473, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 064, Train Loss: 0.0125653, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 065, Train Loss: 0.0097435, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 066, Train Loss: 0.0101341, Train Acc: 1.0000000, Test Acc: 0.6500000\n",
            "Epoch: 067, Train Loss: 0.0084242, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 068, Train Loss: 0.0084140, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 069, Train Loss: 0.0078188, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 070, Train Loss: 0.0071578, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 071, Train Loss: 0.0072532, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 072, Train Loss: 0.0066054, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 073, Train Loss: 0.0061847, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 074, Train Loss: 0.0067216, Train Acc: 1.0000000, Test Acc: 0.6500000\n",
            "Epoch: 075, Train Loss: 0.0056702, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 076, Train Loss: 0.0067184, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 077, Train Loss: 0.0076857, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 078, Train Loss: 0.0094923, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 079, Train Loss: 0.0066031, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 080, Train Loss: 0.0075286, Train Acc: 1.0000000, Test Acc: 0.6500000\n",
            "Epoch: 081, Train Loss: 0.0051106, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 082, Train Loss: 0.0047726, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 083, Train Loss: 0.0044104, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 084, Train Loss: 0.0041990, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 085, Train Loss: 0.0042515, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 086, Train Loss: 0.0037167, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 087, Train Loss: 0.0039208, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 088, Train Loss: 0.0035965, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 089, Train Loss: 0.0034509, Train Acc: 1.0000000, Test Acc: 0.6500000\n",
            "Epoch: 090, Train Loss: 0.0037904, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 091, Train Loss: 0.0032533, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 092, Train Loss: 0.0037811, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 093, Train Loss: 0.0032151, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 094, Train Loss: 0.0033377, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 095, Train Loss: 0.0029266, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 096, Train Loss: 0.0028735, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 097, Train Loss: 0.0027173, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 098, Train Loss: 0.0028740, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 099, Train Loss: 0.0029412, Train Acc: 1.0000000, Test Acc: 0.6500000\n",
            "Epoch: 100, Train Loss: 0.0027986, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 101, Train Loss: 0.0025122, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 102, Train Loss: 0.0022396, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 103, Train Loss: 0.0021205, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 104, Train Loss: 0.0022688, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 105, Train Loss: 0.0022845, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 106, Train Loss: 0.0021096, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 107, Train Loss: 0.0021593, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 108, Train Loss: 0.0020363, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 109, Train Loss: 0.0020523, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 110, Train Loss: 0.0019215, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 111, Train Loss: 0.0023262, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 112, Train Loss: 0.0020848, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 113, Train Loss: 0.0019705, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 114, Train Loss: 0.0020199, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 115, Train Loss: 0.0019084, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 116, Train Loss: 0.0019226, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 117, Train Loss: 0.0018916, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 118, Train Loss: 0.0018191, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 119, Train Loss: 0.0018082, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 120, Train Loss: 0.0017017, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 121, Train Loss: 0.0016751, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 122, Train Loss: 0.0022086, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 123, Train Loss: 0.0016706, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 124, Train Loss: 0.0017378, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 125, Train Loss: 0.0017294, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 126, Train Loss: 0.0015534, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 127, Train Loss: 0.0015940, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 128, Train Loss: 0.0016238, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 129, Train Loss: 0.0015487, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 130, Train Loss: 0.0015835, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 131, Train Loss: 0.0014179, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 132, Train Loss: 0.0056456, Train Acc: 1.0000000, Test Acc: 0.6333333\n",
            "Epoch: 133, Train Loss: 0.0022919, Train Acc: 1.0000000, Test Acc: 0.6166667\n",
            "Epoch: 134, Train Loss: 0.0021282, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 135, Train Loss: 0.0019019, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 136, Train Loss: 0.0015307, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 137, Train Loss: 0.0014813, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 138, Train Loss: 0.0014744, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 139, Train Loss: 0.0013275, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 140, Train Loss: 0.0013793, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 141, Train Loss: 0.0013804, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 142, Train Loss: 0.0013934, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 143, Train Loss: 0.0013528, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 144, Train Loss: 0.0013007, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 145, Train Loss: 0.0013441, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 146, Train Loss: 0.0013683, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 147, Train Loss: 0.0012965, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 148, Train Loss: 0.0013307, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 149, Train Loss: 0.0012658, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 150, Train Loss: 0.0011211, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 151, Train Loss: 0.0010783, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 152, Train Loss: 0.0011105, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 153, Train Loss: 0.0011207, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 154, Train Loss: 0.0011223, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 155, Train Loss: 0.0011004, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 156, Train Loss: 0.0010592, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 157, Train Loss: 0.0011710, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 158, Train Loss: 0.0010515, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 159, Train Loss: 0.0010604, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 160, Train Loss: 0.0010313, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 161, Train Loss: 0.0009973, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 162, Train Loss: 0.0009989, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 163, Train Loss: 0.0010087, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 164, Train Loss: 0.0010351, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 165, Train Loss: 0.0010145, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 166, Train Loss: 0.0010193, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 167, Train Loss: 0.0009415, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 168, Train Loss: 0.0009818, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 169, Train Loss: 0.0009570, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 170, Train Loss: 0.0009442, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 171, Train Loss: 0.0009455, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 172, Train Loss: 0.0009762, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 173, Train Loss: 0.0009032, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 174, Train Loss: 0.0008994, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 175, Train Loss: 0.0008521, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 176, Train Loss: 0.0008509, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 177, Train Loss: 0.0008466, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 178, Train Loss: 0.0008512, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 179, Train Loss: 0.0009141, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 180, Train Loss: 0.0009280, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 181, Train Loss: 0.0009490, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 182, Train Loss: 0.0008938, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 183, Train Loss: 0.0008259, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 184, Train Loss: 0.0008476, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 185, Train Loss: 0.0008197, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 186, Train Loss: 0.0008173, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 187, Train Loss: 0.0008185, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 188, Train Loss: 0.0008354, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 189, Train Loss: 0.0008308, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 190, Train Loss: 0.0008896, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 191, Train Loss: 0.0008514, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 192, Train Loss: 0.0007890, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 193, Train Loss: 0.0007442, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 194, Train Loss: 0.0007582, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 195, Train Loss: 0.0007950, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 196, Train Loss: 0.0007774, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 197, Train Loss: 0.0007524, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 198, Train Loss: 0.0007652, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 199, Train Loss: 0.0007533, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 200, Train Loss: 0.0007425, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 201, Train Loss: 0.0007472, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 202, Train Loss: 0.0007292, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 203, Train Loss: 0.0007438, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 204, Train Loss: 0.0007613, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 205, Train Loss: 0.0007572, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 206, Train Loss: 0.0007213, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 207, Train Loss: 0.0007223, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 208, Train Loss: 0.0007043, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 209, Train Loss: 0.0007208, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 210, Train Loss: 0.0007107, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 211, Train Loss: 0.0006833, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 212, Train Loss: 0.0007084, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 213, Train Loss: 0.0006945, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 214, Train Loss: 0.0006900, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 215, Train Loss: 0.0006668, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 216, Train Loss: 0.0006552, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 217, Train Loss: 0.0006531, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 218, Train Loss: 0.0006959, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 219, Train Loss: 0.0006802, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 220, Train Loss: 0.0006869, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 221, Train Loss: 0.0006470, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 222, Train Loss: 0.0006460, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 223, Train Loss: 0.0006503, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 224, Train Loss: 0.0006253, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 225, Train Loss: 0.0006180, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 226, Train Loss: 0.0006388, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 227, Train Loss: 0.0006084, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 228, Train Loss: 0.0006222, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 229, Train Loss: 0.0006555, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 230, Train Loss: 0.0006370, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 231, Train Loss: 0.0006258, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 232, Train Loss: 0.0006276, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 233, Train Loss: 0.0005935, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 234, Train Loss: 0.0005949, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 235, Train Loss: 0.0006026, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 236, Train Loss: 0.0005888, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 237, Train Loss: 0.0005938, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 238, Train Loss: 0.0005743, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 239, Train Loss: 0.0005737, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 240, Train Loss: 0.0006069, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 241, Train Loss: 0.0006417, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 242, Train Loss: 0.0006722, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 243, Train Loss: 0.0006141, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 244, Train Loss: 0.0006207, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 245, Train Loss: 0.0006035, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 246, Train Loss: 0.0005908, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 247, Train Loss: 0.0005842, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 248, Train Loss: 0.0005559, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 249, Train Loss: 0.0006262, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 250, Train Loss: 0.0006074, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 251, Train Loss: 0.0005596, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 252, Train Loss: 0.0005643, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 253, Train Loss: 0.0005627, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 254, Train Loss: 0.0005833, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 255, Train Loss: 0.0005843, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 256, Train Loss: 0.0005715, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 257, Train Loss: 0.0005712, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 258, Train Loss: 0.0006010, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 259, Train Loss: 0.0005703, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 260, Train Loss: 0.0005513, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 261, Train Loss: 0.0005403, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 262, Train Loss: 0.0005154, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 263, Train Loss: 0.0005230, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 264, Train Loss: 0.0005128, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 265, Train Loss: 0.0006128, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 266, Train Loss: 0.0005875, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 267, Train Loss: 0.0006058, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 268, Train Loss: 0.0005626, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 269, Train Loss: 0.0005331, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 270, Train Loss: 0.0005232, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 271, Train Loss: 0.0005245, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 272, Train Loss: 0.0005303, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 273, Train Loss: 0.0005284, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 274, Train Loss: 0.0005157, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 275, Train Loss: 0.0005196, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 276, Train Loss: 0.0005169, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 277, Train Loss: 0.0005629, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 278, Train Loss: 0.0005154, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 279, Train Loss: 0.0005172, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 280, Train Loss: 0.0005114, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 281, Train Loss: 0.0005257, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 282, Train Loss: 0.0004996, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 283, Train Loss: 0.0004872, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 284, Train Loss: 0.0004988, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 285, Train Loss: 0.0004979, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 286, Train Loss: 0.0004928, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 287, Train Loss: 0.0005028, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 288, Train Loss: 0.0005085, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 289, Train Loss: 0.0005172, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 290, Train Loss: 0.0004898, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 291, Train Loss: 0.0004868, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 292, Train Loss: 0.0005357, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 293, Train Loss: 0.0005289, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 294, Train Loss: 0.0004895, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 295, Train Loss: 0.0004967, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 296, Train Loss: 0.0004759, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 297, Train Loss: 0.0005180, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 298, Train Loss: 0.0004989, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 299, Train Loss: 0.0004842, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 300, Train Loss: 0.0004749, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 301, Train Loss: 0.0004635, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 302, Train Loss: 0.0004766, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 303, Train Loss: 0.0004785, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 304, Train Loss: 0.0004608, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 305, Train Loss: 0.0004608, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 306, Train Loss: 0.0004654, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 307, Train Loss: 0.0004518, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 308, Train Loss: 0.0004766, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 309, Train Loss: 0.0005030, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 310, Train Loss: 0.0004877, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 311, Train Loss: 0.0004753, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 312, Train Loss: 0.0004785, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 313, Train Loss: 0.0004663, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 314, Train Loss: 0.0004689, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 315, Train Loss: 0.0004729, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 316, Train Loss: 0.0004537, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 317, Train Loss: 0.0004733, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 318, Train Loss: 0.0004641, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 319, Train Loss: 0.0004619, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 320, Train Loss: 0.0004410, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 321, Train Loss: 0.0004508, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 322, Train Loss: 0.0004950, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 323, Train Loss: 0.0004773, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 324, Train Loss: 0.0004739, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 325, Train Loss: 0.0004639, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 326, Train Loss: 0.0004798, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 327, Train Loss: 0.0004952, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 328, Train Loss: 0.0004810, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 329, Train Loss: 0.0004524, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 330, Train Loss: 0.0004397, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 331, Train Loss: 0.0004368, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 332, Train Loss: 0.0004557, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 333, Train Loss: 0.0004691, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 334, Train Loss: 0.0004618, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 335, Train Loss: 0.0004551, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 336, Train Loss: 0.0004528, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 337, Train Loss: 0.0004520, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 338, Train Loss: 0.0004305, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 339, Train Loss: 0.0004277, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 340, Train Loss: 0.0004312, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 341, Train Loss: 0.0004443, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 342, Train Loss: 0.0004460, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 343, Train Loss: 0.0004288, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 344, Train Loss: 0.0004244, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 345, Train Loss: 0.0004633, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 346, Train Loss: 0.0008327, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 347, Train Loss: 0.0008082, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 348, Train Loss: 0.0006151, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 349, Train Loss: 0.0005262, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 350, Train Loss: 0.0005136, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 351, Train Loss: 0.0004983, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 352, Train Loss: 0.0004898, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 353, Train Loss: 0.0004714, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 354, Train Loss: 0.0004702, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 355, Train Loss: 0.0004789, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 356, Train Loss: 0.0005085, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 357, Train Loss: 0.0004567, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 358, Train Loss: 0.0004551, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 359, Train Loss: 0.0004458, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 360, Train Loss: 0.0004542, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 361, Train Loss: 0.0004504, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 362, Train Loss: 0.0004323, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 363, Train Loss: 0.0004404, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 364, Train Loss: 0.0004587, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 365, Train Loss: 0.0004619, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 366, Train Loss: 0.0004691, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 367, Train Loss: 0.0004498, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 368, Train Loss: 0.0004438, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 369, Train Loss: 0.0004459, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 370, Train Loss: 0.0004557, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 371, Train Loss: 0.0004390, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 372, Train Loss: 0.0004198, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 373, Train Loss: 0.0004238, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 374, Train Loss: 0.0004284, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 375, Train Loss: 0.0004253, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 376, Train Loss: 0.0004234, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 377, Train Loss: 0.0004372, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 378, Train Loss: 0.0004267, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 379, Train Loss: 0.0004186, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 380, Train Loss: 0.0004137, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 381, Train Loss: 0.0004114, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 382, Train Loss: 0.0004263, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 383, Train Loss: 0.0004363, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 384, Train Loss: 0.0004363, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 385, Train Loss: 0.0004310, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 386, Train Loss: 0.0004384, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 387, Train Loss: 0.0004360, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 388, Train Loss: 0.0004236, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 389, Train Loss: 0.0004164, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 390, Train Loss: 0.0004003, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 391, Train Loss: 0.0004109, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 392, Train Loss: 0.0004011, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 393, Train Loss: 0.0004081, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 394, Train Loss: 0.0004028, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 395, Train Loss: 0.0003972, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 396, Train Loss: 0.0003978, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 397, Train Loss: 0.0004281, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 398, Train Loss: 0.0004131, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 399, Train Loss: 0.0003910, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 400, Train Loss: 0.0003764, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 401, Train Loss: 0.0003831, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 402, Train Loss: 0.0003751, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 403, Train Loss: 0.0003757, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 404, Train Loss: 0.0003861, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 405, Train Loss: 0.0003829, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 406, Train Loss: 0.0003955, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 407, Train Loss: 0.0004024, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 408, Train Loss: 0.0003997, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 409, Train Loss: 0.0004126, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 410, Train Loss: 0.0003940, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 411, Train Loss: 0.0004140, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 412, Train Loss: 0.0003823, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 413, Train Loss: 0.0003891, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 414, Train Loss: 0.0004228, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 415, Train Loss: 0.0004049, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 416, Train Loss: 0.0003866, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 417, Train Loss: 0.0003885, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 418, Train Loss: 0.0003712, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 419, Train Loss: 0.0003930, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 420, Train Loss: 0.0003757, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 421, Train Loss: 0.0004046, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 422, Train Loss: 0.0003699, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 423, Train Loss: 0.0003683, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 424, Train Loss: 0.0003855, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 425, Train Loss: 0.0003869, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 426, Train Loss: 0.0003770, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 427, Train Loss: 0.0003816, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 428, Train Loss: 0.0004048, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 429, Train Loss: 0.0004078, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 430, Train Loss: 0.0003772, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 431, Train Loss: 0.0003801, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 432, Train Loss: 0.0003897, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 433, Train Loss: 0.0003781, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 434, Train Loss: 0.0003735, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 435, Train Loss: 0.0003880, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 436, Train Loss: 0.0003702, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 437, Train Loss: 0.0003786, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 438, Train Loss: 0.0003708, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 439, Train Loss: 0.0003753, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 440, Train Loss: 0.0003600, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 441, Train Loss: 0.0004123, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 442, Train Loss: 0.0004004, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 443, Train Loss: 0.0004047, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 444, Train Loss: 0.0003886, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 445, Train Loss: 0.0003776, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 446, Train Loss: 0.0003734, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 447, Train Loss: 0.0003647, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 448, Train Loss: 0.0003896, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 449, Train Loss: 0.0003742, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 450, Train Loss: 0.0003712, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 451, Train Loss: 0.0003684, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 452, Train Loss: 0.0003736, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 453, Train Loss: 0.0003737, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 454, Train Loss: 0.0003815, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 455, Train Loss: 0.0003614, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 456, Train Loss: 0.0003633, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 457, Train Loss: 0.0003584, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 458, Train Loss: 0.0003660, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 459, Train Loss: 0.0003693, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 460, Train Loss: 0.0003663, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 461, Train Loss: 0.0003570, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 462, Train Loss: 0.0004067, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 463, Train Loss: 0.0004004, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 464, Train Loss: 0.0003758, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 465, Train Loss: 0.0003922, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 466, Train Loss: 0.0003978, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 467, Train Loss: 0.0003910, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 468, Train Loss: 0.0004062, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 469, Train Loss: 0.0003839, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 470, Train Loss: 0.0003639, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 471, Train Loss: 0.0003663, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 472, Train Loss: 0.0003619, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 473, Train Loss: 0.0003553, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 474, Train Loss: 0.0003505, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 475, Train Loss: 0.0003566, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 476, Train Loss: 0.0003995, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 477, Train Loss: 0.0003651, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 478, Train Loss: 0.0003739, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 479, Train Loss: 0.0003520, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 480, Train Loss: 0.0003567, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 481, Train Loss: 0.0003597, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 482, Train Loss: 0.0003634, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 483, Train Loss: 0.0003735, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 484, Train Loss: 0.0003853, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 485, Train Loss: 0.0003631, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 486, Train Loss: 0.0003777, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 487, Train Loss: 0.0003588, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 488, Train Loss: 0.0003531, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 489, Train Loss: 0.0003565, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 490, Train Loss: 0.0003755, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 491, Train Loss: 0.0003609, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 492, Train Loss: 0.0003502, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 493, Train Loss: 0.0003484, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 494, Train Loss: 0.0003370, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 495, Train Loss: 0.0003595, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 496, Train Loss: 0.0003553, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 497, Train Loss: 0.0003573, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 498, Train Loss: 0.0003501, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 499, Train Loss: 0.0003550, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 500, Train Loss: 0.0003392, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "--------\n",
            "Best Test Acc:   0.6972222222222223_0.0824490115536435, Epoch: 97\n",
            "Best Train Loss: 0.00038995379979081193_5.4346790834658696e-05\n",
            "Best Train Acc:  1.0_0.0\n",
            "FOLD6, ENZYMES, 1668698769/_rhoE0.5P18I1_GELU_X_6_256_0_0.001_50_0.6_B64S0W8, BT=0.6972222222222223_0.0824490115536435, BE=97, ID=\n",
            "train_set : test_set = %d : %d 540 60\n",
            "-------- FOLD6 DATASET=ENZYMES, COMMIT_ID=\n",
            "#Params: 1334534\n",
            "Epoch: 001, Train Loss: 6.4078541, Train Acc: 0.2000000, Test Acc: 0.2166667\n",
            "Epoch: 002, Train Loss: 2.5528083, Train Acc: 0.2425926, Test Acc: 0.2166667\n",
            "Epoch: 003, Train Loss: 2.2022519, Train Acc: 0.2740741, Test Acc: 0.2000000\n",
            "Epoch: 004, Train Loss: 1.7517542, Train Acc: 0.2481481, Test Acc: 0.2166667\n",
            "Epoch: 005, Train Loss: 1.5488467, Train Acc: 0.3870370, Test Acc: 0.3500000\n",
            "Epoch: 006, Train Loss: 1.6077544, Train Acc: 0.3222222, Test Acc: 0.2166667\n",
            "Epoch: 007, Train Loss: 1.5025898, Train Acc: 0.3888889, Test Acc: 0.2666667\n",
            "Epoch: 008, Train Loss: 1.3967696, Train Acc: 0.4611111, Test Acc: 0.3500000\n",
            "Epoch: 009, Train Loss: 1.3720658, Train Acc: 0.4629630, Test Acc: 0.2833333\n",
            "Epoch: 010, Train Loss: 1.3638537, Train Acc: 0.4296296, Test Acc: 0.3166667\n",
            "Epoch: 011, Train Loss: 1.3541713, Train Acc: 0.4981481, Test Acc: 0.3500000\n",
            "Epoch: 012, Train Loss: 1.4561937, Train Acc: 0.4574074, Test Acc: 0.3833333\n",
            "Epoch: 013, Train Loss: 1.2338185, Train Acc: 0.5685185, Test Acc: 0.4166667\n",
            "Epoch: 014, Train Loss: 1.2203758, Train Acc: 0.5370370, Test Acc: 0.4333333\n",
            "Epoch: 015, Train Loss: 1.1241852, Train Acc: 0.5962963, Test Acc: 0.4166667\n",
            "Epoch: 016, Train Loss: 1.0974512, Train Acc: 0.6037037, Test Acc: 0.5000000\n",
            "Epoch: 017, Train Loss: 1.1193334, Train Acc: 0.5611111, Test Acc: 0.4166667\n",
            "Epoch: 018, Train Loss: 1.0140026, Train Acc: 0.6407407, Test Acc: 0.5333333\n",
            "Epoch: 019, Train Loss: 0.9589725, Train Acc: 0.6722222, Test Acc: 0.4500000\n",
            "Epoch: 020, Train Loss: 1.0819476, Train Acc: 0.5740741, Test Acc: 0.4666667\n",
            "Epoch: 021, Train Loss: 0.9906279, Train Acc: 0.6259259, Test Acc: 0.4666667\n",
            "Epoch: 022, Train Loss: 0.7784700, Train Acc: 0.7722222, Test Acc: 0.5833333\n",
            "Epoch: 023, Train Loss: 0.7117446, Train Acc: 0.8055556, Test Acc: 0.5333333\n",
            "Epoch: 024, Train Loss: 0.6933344, Train Acc: 0.8129630, Test Acc: 0.5666667\n",
            "Epoch: 025, Train Loss: 0.7537444, Train Acc: 0.7388889, Test Acc: 0.5000000\n",
            "Epoch: 026, Train Loss: 0.8533997, Train Acc: 0.6796296, Test Acc: 0.4333333\n",
            "Epoch: 027, Train Loss: 0.8295538, Train Acc: 0.7333333, Test Acc: 0.5333333\n",
            "Epoch: 028, Train Loss: 0.7066495, Train Acc: 0.7444444, Test Acc: 0.4500000\n",
            "Epoch: 029, Train Loss: 0.7311677, Train Acc: 0.7314815, Test Acc: 0.4500000\n",
            "Epoch: 030, Train Loss: 0.5587595, Train Acc: 0.8222222, Test Acc: 0.5833333\n",
            "Epoch: 031, Train Loss: 0.5386260, Train Acc: 0.8685185, Test Acc: 0.5833333\n",
            "Epoch: 032, Train Loss: 0.4945220, Train Acc: 0.8685185, Test Acc: 0.6000000\n",
            "Epoch: 033, Train Loss: 0.6129047, Train Acc: 0.7851852, Test Acc: 0.5166667\n",
            "Epoch: 034, Train Loss: 0.7658203, Train Acc: 0.7055556, Test Acc: 0.5000000\n",
            "Epoch: 035, Train Loss: 0.3065785, Train Acc: 0.9481481, Test Acc: 0.6333333\n",
            "Epoch: 036, Train Loss: 0.5888878, Train Acc: 0.8055556, Test Acc: 0.5833333\n",
            "Epoch: 037, Train Loss: 0.3209911, Train Acc: 0.9222222, Test Acc: 0.5666667\n",
            "Epoch: 038, Train Loss: 0.3615636, Train Acc: 0.8833333, Test Acc: 0.5833333\n",
            "Epoch: 039, Train Loss: 0.4182444, Train Acc: 0.8629630, Test Acc: 0.5333333\n",
            "Epoch: 040, Train Loss: 0.1917870, Train Acc: 0.9703704, Test Acc: 0.6500000\n",
            "Epoch: 041, Train Loss: 0.4477492, Train Acc: 0.8425926, Test Acc: 0.5500000\n",
            "Epoch: 042, Train Loss: 0.1475217, Train Acc: 0.9759259, Test Acc: 0.6833333\n",
            "Epoch: 043, Train Loss: 0.1124227, Train Acc: 0.9907407, Test Acc: 0.7333333\n",
            "Epoch: 044, Train Loss: 0.1072504, Train Acc: 0.9888889, Test Acc: 0.7000000\n",
            "Epoch: 045, Train Loss: 0.2186044, Train Acc: 0.9351852, Test Acc: 0.6666667\n",
            "Epoch: 046, Train Loss: 0.6722742, Train Acc: 0.8018519, Test Acc: 0.5500000\n",
            "Epoch: 047, Train Loss: 0.1660487, Train Acc: 0.9722222, Test Acc: 0.6666667\n",
            "Epoch: 048, Train Loss: 0.1768267, Train Acc: 0.9592593, Test Acc: 0.5833333\n",
            "Epoch: 049, Train Loss: 0.2992280, Train Acc: 0.8870370, Test Acc: 0.6166667\n",
            "Epoch: 050, Train Loss: 0.4434649, Train Acc: 0.8185185, Test Acc: 0.5833333\n",
            "Epoch: 051, Train Loss: 0.1103964, Train Acc: 0.9796296, Test Acc: 0.6333333\n",
            "Epoch: 052, Train Loss: 0.0632214, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 053, Train Loss: 0.0438644, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 054, Train Loss: 0.0327423, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 055, Train Loss: 0.0199548, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 056, Train Loss: 0.0233102, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 057, Train Loss: 0.0137743, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 058, Train Loss: 0.0116697, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 059, Train Loss: 0.0122555, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 060, Train Loss: 0.0101233, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 061, Train Loss: 0.0115581, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 062, Train Loss: 0.0092918, Train Acc: 1.0000000, Test Acc: 0.7500000\n",
            "Epoch: 063, Train Loss: 0.0105703, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 064, Train Loss: 0.0101590, Train Acc: 1.0000000, Test Acc: 0.7500000\n",
            "Epoch: 065, Train Loss: 0.0139655, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 066, Train Loss: 0.0077079, Train Acc: 1.0000000, Test Acc: 0.7500000\n",
            "Epoch: 067, Train Loss: 0.0072271, Train Acc: 1.0000000, Test Acc: 0.7500000\n",
            "Epoch: 068, Train Loss: 0.0065626, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 069, Train Loss: 0.0058147, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 070, Train Loss: 0.0055598, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 071, Train Loss: 0.0052543, Train Acc: 1.0000000, Test Acc: 0.7500000\n",
            "Epoch: 072, Train Loss: 0.0049556, Train Acc: 1.0000000, Test Acc: 0.7500000\n",
            "Epoch: 073, Train Loss: 0.0055360, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 074, Train Loss: 0.0051961, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 075, Train Loss: 0.0045789, Train Acc: 1.0000000, Test Acc: 0.7500000\n",
            "Epoch: 076, Train Loss: 0.0056271, Train Acc: 1.0000000, Test Acc: 0.7500000\n",
            "Epoch: 077, Train Loss: 0.0048984, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 078, Train Loss: 0.0044269, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 079, Train Loss: 0.0039957, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 080, Train Loss: 0.0037005, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 081, Train Loss: 0.0034432, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 082, Train Loss: 0.0032206, Train Acc: 1.0000000, Test Acc: 0.7500000\n",
            "Epoch: 083, Train Loss: 0.0032295, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 084, Train Loss: 0.0031715, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 085, Train Loss: 0.0029300, Train Acc: 1.0000000, Test Acc: 0.7500000\n",
            "Epoch: 086, Train Loss: 0.0032893, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 087, Train Loss: 0.0028268, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 088, Train Loss: 0.0028279, Train Acc: 1.0000000, Test Acc: 0.7500000\n",
            "Epoch: 089, Train Loss: 0.0026751, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 090, Train Loss: 0.0025831, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 091, Train Loss: 0.0025007, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 092, Train Loss: 0.0027397, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 093, Train Loss: 0.0026545, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 094, Train Loss: 0.0024697, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 095, Train Loss: 0.0026416, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 096, Train Loss: 0.0044981, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 097, Train Loss: 0.0025461, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 098, Train Loss: 0.0023303, Train Acc: 1.0000000, Test Acc: 0.7500000\n",
            "Epoch: 099, Train Loss: 0.0023119, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 100, Train Loss: 0.0023899, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 101, Train Loss: 0.0019219, Train Acc: 1.0000000, Test Acc: 0.7500000\n",
            "Epoch: 102, Train Loss: 0.0018873, Train Acc: 1.0000000, Test Acc: 0.7500000\n",
            "Epoch: 103, Train Loss: 0.0018213, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 104, Train Loss: 0.0017533, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 105, Train Loss: 0.0016384, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 106, Train Loss: 0.0016231, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 107, Train Loss: 0.0016213, Train Acc: 1.0000000, Test Acc: 0.7500000\n",
            "Epoch: 108, Train Loss: 0.0015625, Train Acc: 1.0000000, Test Acc: 0.7500000\n",
            "Epoch: 109, Train Loss: 0.0014834, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 110, Train Loss: 0.0015627, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 111, Train Loss: 0.0015909, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 112, Train Loss: 0.0014468, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 113, Train Loss: 0.0014599, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 114, Train Loss: 0.0014397, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 115, Train Loss: 0.0014718, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 116, Train Loss: 0.0014475, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 117, Train Loss: 0.0014141, Train Acc: 1.0000000, Test Acc: 0.7500000\n",
            "Epoch: 118, Train Loss: 0.0014576, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 119, Train Loss: 0.0013916, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 120, Train Loss: 0.0012987, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 121, Train Loss: 0.0013916, Train Acc: 1.0000000, Test Acc: 0.7500000\n",
            "Epoch: 122, Train Loss: 0.0012363, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 123, Train Loss: 0.0012471, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 124, Train Loss: 0.0012915, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 125, Train Loss: 0.0012046, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 126, Train Loss: 0.0011762, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 127, Train Loss: 0.0011155, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 128, Train Loss: 0.0011545, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 129, Train Loss: 0.0011581, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 130, Train Loss: 0.0011042, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 131, Train Loss: 0.0010384, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 132, Train Loss: 0.0011499, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 133, Train Loss: 0.0011941, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 134, Train Loss: 0.0010850, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 135, Train Loss: 0.0010551, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 136, Train Loss: 0.0010417, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 137, Train Loss: 0.0009751, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 138, Train Loss: 0.0009934, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 139, Train Loss: 0.0010208, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 140, Train Loss: 0.0009759, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 141, Train Loss: 0.0011167, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 142, Train Loss: 0.0010868, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 143, Train Loss: 0.0009451, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 144, Train Loss: 0.0009098, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 145, Train Loss: 0.0008980, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 146, Train Loss: 0.0008980, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 147, Train Loss: 0.0008405, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 148, Train Loss: 0.0008838, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 149, Train Loss: 0.0008863, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 150, Train Loss: 0.0008237, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 151, Train Loss: 0.0008598, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 152, Train Loss: 0.0007997, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 153, Train Loss: 0.0007754, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 154, Train Loss: 0.0007951, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 155, Train Loss: 0.0008357, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 156, Train Loss: 0.0008261, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 157, Train Loss: 0.0007798, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 158, Train Loss: 0.0008144, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 159, Train Loss: 0.0007923, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 160, Train Loss: 0.0007549, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 161, Train Loss: 0.0007637, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 162, Train Loss: 0.0007598, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 163, Train Loss: 0.0008074, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 164, Train Loss: 0.0007778, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 165, Train Loss: 0.0007692, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 166, Train Loss: 0.0008062, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 167, Train Loss: 0.0007462, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 168, Train Loss: 0.0006975, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 169, Train Loss: 0.0007132, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 170, Train Loss: 0.0007500, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 171, Train Loss: 0.0007302, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 172, Train Loss: 0.0007086, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 173, Train Loss: 0.0006750, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 174, Train Loss: 0.0006700, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 175, Train Loss: 0.0006785, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 176, Train Loss: 0.0007652, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 177, Train Loss: 0.0006845, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 178, Train Loss: 0.0006636, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 179, Train Loss: 0.0006352, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 180, Train Loss: 0.0006459, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 181, Train Loss: 0.0006486, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 182, Train Loss: 0.0006176, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 183, Train Loss: 0.0006923, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 184, Train Loss: 0.0006398, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 185, Train Loss: 0.0006456, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 186, Train Loss: 0.0007084, Train Acc: 1.0000000, Test Acc: 0.7500000\n",
            "Epoch: 187, Train Loss: 0.0007044, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 188, Train Loss: 0.0007702, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 189, Train Loss: 0.0009564, Train Acc: 1.0000000, Test Acc: 0.7500000\n",
            "Epoch: 190, Train Loss: 0.0008128, Train Acc: 1.0000000, Test Acc: 0.7500000\n",
            "Epoch: 191, Train Loss: 0.0006896, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 192, Train Loss: 0.0006269, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 193, Train Loss: 0.0007606, Train Acc: 1.0000000, Test Acc: 0.7500000\n",
            "Epoch: 194, Train Loss: 0.0006183, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 195, Train Loss: 0.0006206, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 196, Train Loss: 0.0006367, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 197, Train Loss: 0.0006643, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 198, Train Loss: 0.0006691, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 199, Train Loss: 0.0006537, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 200, Train Loss: 0.0007361, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 201, Train Loss: 0.0006428, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 202, Train Loss: 0.0005827, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 203, Train Loss: 0.0005606, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 204, Train Loss: 0.0005803, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 205, Train Loss: 0.0005380, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 206, Train Loss: 0.0005498, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 207, Train Loss: 0.0005439, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 208, Train Loss: 0.0005990, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 209, Train Loss: 0.0005285, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 210, Train Loss: 0.0005310, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 211, Train Loss: 0.0005182, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 212, Train Loss: 0.0005462, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 213, Train Loss: 0.0005397, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 214, Train Loss: 0.0005923, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 215, Train Loss: 0.0005862, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 216, Train Loss: 0.0005691, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 217, Train Loss: 0.0005373, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 218, Train Loss: 0.0005479, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 219, Train Loss: 0.0005236, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 220, Train Loss: 0.0004939, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 221, Train Loss: 0.0004861, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 222, Train Loss: 0.0005040, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 223, Train Loss: 0.0004786, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 224, Train Loss: 0.0004812, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 225, Train Loss: 0.0004980, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 226, Train Loss: 0.0005013, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 227, Train Loss: 0.0004910, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 228, Train Loss: 0.0005241, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 229, Train Loss: 0.0004714, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 230, Train Loss: 0.0004808, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 231, Train Loss: 0.0004783, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 232, Train Loss: 0.0004644, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 233, Train Loss: 0.0004922, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 234, Train Loss: 0.0004744, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 235, Train Loss: 0.0004644, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 236, Train Loss: 0.0004728, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 237, Train Loss: 0.0004865, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 238, Train Loss: 0.0005464, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 239, Train Loss: 0.0004836, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 240, Train Loss: 0.0004739, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 241, Train Loss: 0.0004585, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 242, Train Loss: 0.0004287, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 243, Train Loss: 0.0004701, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 244, Train Loss: 0.0004628, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 245, Train Loss: 0.0004592, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 246, Train Loss: 0.0004394, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 247, Train Loss: 0.0004394, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 248, Train Loss: 0.0004221, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 249, Train Loss: 0.0004243, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 250, Train Loss: 0.0004351, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 251, Train Loss: 0.0004246, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 252, Train Loss: 0.0004493, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 253, Train Loss: 0.0004237, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 254, Train Loss: 0.0004155, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 255, Train Loss: 0.0004175, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 256, Train Loss: 0.0004351, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 257, Train Loss: 0.0004258, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 258, Train Loss: 0.0004380, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 259, Train Loss: 0.0004228, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 260, Train Loss: 0.0004320, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 261, Train Loss: 0.0004155, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 262, Train Loss: 0.0004713, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 263, Train Loss: 0.0004276, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 264, Train Loss: 0.0004713, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 265, Train Loss: 0.0004235, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 266, Train Loss: 0.0004037, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 267, Train Loss: 0.0004063, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 268, Train Loss: 0.0003966, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 269, Train Loss: 0.0003951, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 270, Train Loss: 0.0003934, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 271, Train Loss: 0.0003884, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 272, Train Loss: 0.0003915, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 273, Train Loss: 0.0003777, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 274, Train Loss: 0.0003906, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 275, Train Loss: 0.0003963, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 276, Train Loss: 0.0003875, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 277, Train Loss: 0.0004096, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 278, Train Loss: 0.0003847, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 279, Train Loss: 0.0003771, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 280, Train Loss: 0.0003683, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 281, Train Loss: 0.0003840, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 282, Train Loss: 0.0003905, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 283, Train Loss: 0.0003792, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 284, Train Loss: 0.0003800, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 285, Train Loss: 0.0003935, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 286, Train Loss: 0.0003902, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 287, Train Loss: 0.0003809, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 288, Train Loss: 0.0003711, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 289, Train Loss: 0.0003858, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 290, Train Loss: 0.0003710, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 291, Train Loss: 0.0004403, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 292, Train Loss: 0.0004099, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 293, Train Loss: 0.0003868, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 294, Train Loss: 0.0003713, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 295, Train Loss: 0.0003590, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 296, Train Loss: 0.0003710, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 297, Train Loss: 0.0003777, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 298, Train Loss: 0.0003627, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 299, Train Loss: 0.0003754, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 300, Train Loss: 0.0003699, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 301, Train Loss: 0.0003884, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 302, Train Loss: 0.0003952, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 303, Train Loss: 0.0003749, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 304, Train Loss: 0.0003637, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 305, Train Loss: 0.0003617, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 306, Train Loss: 0.0004036, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 307, Train Loss: 0.0003704, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 308, Train Loss: 0.0003567, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 309, Train Loss: 0.0004292, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 310, Train Loss: 0.0003847, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 311, Train Loss: 0.0003507, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 312, Train Loss: 0.0004498, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 313, Train Loss: 0.0004230, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 314, Train Loss: 0.0003720, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 315, Train Loss: 0.0003472, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 316, Train Loss: 0.0003505, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 317, Train Loss: 0.0003495, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 318, Train Loss: 0.0003552, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 319, Train Loss: 0.0003542, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 320, Train Loss: 0.0003386, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 321, Train Loss: 0.0003408, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 322, Train Loss: 0.0003787, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 323, Train Loss: 0.0004224, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 324, Train Loss: 0.0003642, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 325, Train Loss: 0.0003518, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 326, Train Loss: 0.0004062, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 327, Train Loss: 0.0004606, Train Acc: 1.0000000, Test Acc: 0.7500000\n",
            "Epoch: 328, Train Loss: 0.0005077, Train Acc: 1.0000000, Test Acc: 0.7500000\n",
            "Epoch: 329, Train Loss: 0.0004354, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 330, Train Loss: 0.0003964, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 331, Train Loss: 0.0003813, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 332, Train Loss: 0.0003627, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 333, Train Loss: 0.0003646, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 334, Train Loss: 0.0003538, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 335, Train Loss: 0.0003762, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 336, Train Loss: 0.0003813, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 337, Train Loss: 0.0003739, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 338, Train Loss: 0.0003659, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 339, Train Loss: 0.0003499, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 340, Train Loss: 0.0003388, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 341, Train Loss: 0.0003458, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 342, Train Loss: 0.0003561, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 343, Train Loss: 0.0003549, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 344, Train Loss: 0.0003414, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 345, Train Loss: 0.0003488, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 346, Train Loss: 0.0003365, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 347, Train Loss: 0.0003316, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 348, Train Loss: 0.0003317, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 349, Train Loss: 0.0003169, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 350, Train Loss: 0.0003068, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 351, Train Loss: 0.0003137, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 352, Train Loss: 0.0003100, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 353, Train Loss: 0.0003179, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 354, Train Loss: 0.0003252, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 355, Train Loss: 0.0003322, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 356, Train Loss: 0.0003122, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 357, Train Loss: 0.0003177, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 358, Train Loss: 0.0003114, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 359, Train Loss: 0.0003893, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 360, Train Loss: 0.0003833, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 361, Train Loss: 0.0004276, Train Acc: 1.0000000, Test Acc: 0.7500000\n",
            "Epoch: 362, Train Loss: 0.0004730, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 363, Train Loss: 0.0004080, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 364, Train Loss: 0.0003705, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 365, Train Loss: 0.0003501, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 366, Train Loss: 0.0003269, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 367, Train Loss: 0.0003169, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 368, Train Loss: 0.0003374, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 369, Train Loss: 0.0003363, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 370, Train Loss: 0.0003330, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 371, Train Loss: 0.0003465, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 372, Train Loss: 0.0003287, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 373, Train Loss: 0.0003241, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 374, Train Loss: 0.0003200, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 375, Train Loss: 0.0003298, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 376, Train Loss: 0.0003119, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 377, Train Loss: 0.0003314, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 378, Train Loss: 0.0003147, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 379, Train Loss: 0.0003124, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 380, Train Loss: 0.0003164, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 381, Train Loss: 0.0003187, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 382, Train Loss: 0.0003273, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 383, Train Loss: 0.0003279, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 384, Train Loss: 0.0003107, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 385, Train Loss: 0.0003171, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 386, Train Loss: 0.0003042, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 387, Train Loss: 0.0003216, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 388, Train Loss: 0.0003296, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 389, Train Loss: 0.0003460, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 390, Train Loss: 0.0003173, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 391, Train Loss: 0.0003183, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 392, Train Loss: 0.0003395, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 393, Train Loss: 0.0003323, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 394, Train Loss: 0.0003282, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 395, Train Loss: 0.0003227, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 396, Train Loss: 0.0003093, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 397, Train Loss: 0.0003193, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 398, Train Loss: 0.0003275, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 399, Train Loss: 0.0003076, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 400, Train Loss: 0.0003011, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 401, Train Loss: 0.0003090, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 402, Train Loss: 0.0003129, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 403, Train Loss: 0.0003047, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 404, Train Loss: 0.0002890, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 405, Train Loss: 0.0003156, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 406, Train Loss: 0.0003113, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 407, Train Loss: 0.0003059, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 408, Train Loss: 0.0003098, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 409, Train Loss: 0.0003038, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 410, Train Loss: 0.0003152, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 411, Train Loss: 0.0002977, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 412, Train Loss: 0.0002919, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 413, Train Loss: 0.0002946, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 414, Train Loss: 0.0002998, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 415, Train Loss: 0.0003034, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 416, Train Loss: 0.0002986, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 417, Train Loss: 0.0003045, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 418, Train Loss: 0.0002952, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 419, Train Loss: 0.0002963, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 420, Train Loss: 0.0002863, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 421, Train Loss: 0.0003889, Train Acc: 1.0000000, Test Acc: 0.7500000\n",
            "Epoch: 422, Train Loss: 0.0003170, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 423, Train Loss: 0.0003050, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 424, Train Loss: 0.0003007, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 425, Train Loss: 0.0003471, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 426, Train Loss: 0.0003068, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 427, Train Loss: 0.0002931, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 428, Train Loss: 0.0002856, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 429, Train Loss: 0.0002922, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 430, Train Loss: 0.0002898, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 431, Train Loss: 0.0002900, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 432, Train Loss: 0.0002772, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 433, Train Loss: 0.0003044, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 434, Train Loss: 0.0003117, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 435, Train Loss: 0.0002895, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 436, Train Loss: 0.0002969, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 437, Train Loss: 0.0003029, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 438, Train Loss: 0.0002883, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 439, Train Loss: 0.0002850, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 440, Train Loss: 0.0002931, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 441, Train Loss: 0.0002973, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 442, Train Loss: 0.0003006, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 443, Train Loss: 0.0002978, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 444, Train Loss: 0.0003399, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 445, Train Loss: 0.0003198, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 446, Train Loss: 0.0003633, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 447, Train Loss: 0.0003222, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 448, Train Loss: 0.0003050, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 449, Train Loss: 0.0002907, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 450, Train Loss: 0.0002949, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 451, Train Loss: 0.0002843, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 452, Train Loss: 0.0002892, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 453, Train Loss: 0.0002824, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 454, Train Loss: 0.0002773, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 455, Train Loss: 0.0002776, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 456, Train Loss: 0.0003000, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 457, Train Loss: 0.0002915, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 458, Train Loss: 0.0003123, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 459, Train Loss: 0.0002966, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 460, Train Loss: 0.0002925, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 461, Train Loss: 0.0002832, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 462, Train Loss: 0.0002937, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 463, Train Loss: 0.0002858, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 464, Train Loss: 0.0002717, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 465, Train Loss: 0.0002790, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 466, Train Loss: 0.0002864, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 467, Train Loss: 0.0002868, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 468, Train Loss: 0.0002933, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 469, Train Loss: 0.0002941, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 470, Train Loss: 0.0002890, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 471, Train Loss: 0.0002721, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 472, Train Loss: 0.0002985, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 473, Train Loss: 0.0002886, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 474, Train Loss: 0.0002810, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 475, Train Loss: 0.0002773, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 476, Train Loss: 0.0002821, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 477, Train Loss: 0.0002698, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 478, Train Loss: 0.0002725, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 479, Train Loss: 0.0002730, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 480, Train Loss: 0.0002590, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 481, Train Loss: 0.0002763, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 482, Train Loss: 0.0002684, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 483, Train Loss: 0.0002652, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 484, Train Loss: 0.0002675, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 485, Train Loss: 0.0002838, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 486, Train Loss: 0.0002905, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 487, Train Loss: 0.0002852, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 488, Train Loss: 0.0002883, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 489, Train Loss: 0.0002795, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 490, Train Loss: 0.0002686, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 491, Train Loss: 0.0003052, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 492, Train Loss: 0.0002985, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 493, Train Loss: 0.0002748, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 494, Train Loss: 0.0002647, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 495, Train Loss: 0.0002641, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 496, Train Loss: 0.0002784, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 497, Train Loss: 0.0002682, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 498, Train Loss: 0.0002793, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 499, Train Loss: 0.0002768, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 500, Train Loss: 0.0002760, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "--------\n",
            "Best Test Acc:   0.7095238095238096_0.07656159614673107, Epoch: 286\n",
            "Best Train Loss: 0.00037124466861502105_6.805750593325036e-05\n",
            "Best Train Acc:  1.0_0.0\n",
            "FOLD7, ENZYMES, 1668698769/_rhoE0.5P18I1_GELU_X_6_256_0_0.001_50_0.6_B64S0W8, BT=0.7095238095238096_0.07656159614673107, BE=286, ID=\n",
            "train_set : test_set = %d : %d 540 60\n",
            "-------- FOLD7 DATASET=ENZYMES, COMMIT_ID=\n",
            "#Params: 1334534\n",
            "Epoch: 001, Train Loss: 5.6952950, Train Acc: 0.2203704, Test Acc: 0.2000000\n",
            "Epoch: 002, Train Loss: 2.9915257, Train Acc: 0.2074074, Test Acc: 0.2333333\n",
            "Epoch: 003, Train Loss: 2.2489710, Train Acc: 0.2685185, Test Acc: 0.2333333\n",
            "Epoch: 004, Train Loss: 1.7902154, Train Acc: 0.3259259, Test Acc: 0.3166667\n",
            "Epoch: 005, Train Loss: 1.5990769, Train Acc: 0.3240741, Test Acc: 0.2833333\n",
            "Epoch: 006, Train Loss: 1.5594888, Train Acc: 0.4037037, Test Acc: 0.3666667\n",
            "Epoch: 007, Train Loss: 1.4216691, Train Acc: 0.4851852, Test Acc: 0.3666667\n",
            "Epoch: 008, Train Loss: 1.6090348, Train Acc: 0.3925926, Test Acc: 0.2666667\n",
            "Epoch: 009, Train Loss: 1.4968522, Train Acc: 0.4685185, Test Acc: 0.3000000\n",
            "Epoch: 010, Train Loss: 1.3063107, Train Acc: 0.5462963, Test Acc: 0.4333333\n",
            "Epoch: 011, Train Loss: 1.2954415, Train Acc: 0.5037037, Test Acc: 0.3833333\n",
            "Epoch: 012, Train Loss: 1.3791781, Train Acc: 0.4851852, Test Acc: 0.3666667\n",
            "Epoch: 013, Train Loss: 1.3020470, Train Acc: 0.5037037, Test Acc: 0.3000000\n",
            "Epoch: 014, Train Loss: 1.2896519, Train Acc: 0.5407407, Test Acc: 0.3833333\n",
            "Epoch: 015, Train Loss: 1.1957932, Train Acc: 0.5574074, Test Acc: 0.4166667\n",
            "Epoch: 016, Train Loss: 1.1508195, Train Acc: 0.5851852, Test Acc: 0.4333333\n",
            "Epoch: 017, Train Loss: 1.1901355, Train Acc: 0.5537037, Test Acc: 0.4333333\n",
            "Epoch: 018, Train Loss: 1.0822835, Train Acc: 0.6351852, Test Acc: 0.4666667\n",
            "Epoch: 019, Train Loss: 1.0160105, Train Acc: 0.6555556, Test Acc: 0.4500000\n",
            "Epoch: 020, Train Loss: 0.9074108, Train Acc: 0.7277778, Test Acc: 0.5333333\n",
            "Epoch: 021, Train Loss: 0.9166991, Train Acc: 0.6777778, Test Acc: 0.5166667\n",
            "Epoch: 022, Train Loss: 0.9548534, Train Acc: 0.7222222, Test Acc: 0.5833333\n",
            "Epoch: 023, Train Loss: 0.8790700, Train Acc: 0.7222222, Test Acc: 0.5166667\n",
            "Epoch: 024, Train Loss: 0.8707038, Train Acc: 0.6851852, Test Acc: 0.4833333\n",
            "Epoch: 025, Train Loss: 0.7527678, Train Acc: 0.7648148, Test Acc: 0.5833333\n",
            "Epoch: 026, Train Loss: 0.7886577, Train Acc: 0.7574074, Test Acc: 0.5666667\n",
            "Epoch: 027, Train Loss: 0.8876664, Train Acc: 0.6425926, Test Acc: 0.5166667\n",
            "Epoch: 028, Train Loss: 0.7725314, Train Acc: 0.6907407, Test Acc: 0.5833333\n",
            "Epoch: 029, Train Loss: 0.6492080, Train Acc: 0.8018519, Test Acc: 0.6500000\n",
            "Epoch: 030, Train Loss: 0.6497838, Train Acc: 0.7925926, Test Acc: 0.6333333\n",
            "Epoch: 031, Train Loss: 0.7423852, Train Acc: 0.6962963, Test Acc: 0.5166667\n",
            "Epoch: 032, Train Loss: 0.5256362, Train Acc: 0.8537037, Test Acc: 0.6833333\n",
            "Epoch: 033, Train Loss: 0.4133198, Train Acc: 0.9037037, Test Acc: 0.7333333\n",
            "Epoch: 034, Train Loss: 0.5054228, Train Acc: 0.8407407, Test Acc: 0.6000000\n",
            "Epoch: 035, Train Loss: 0.7981307, Train Acc: 0.6870370, Test Acc: 0.4333333\n",
            "Epoch: 036, Train Loss: 0.4633486, Train Acc: 0.8388889, Test Acc: 0.6000000\n",
            "Epoch: 037, Train Loss: 0.3083554, Train Acc: 0.9351852, Test Acc: 0.6500000\n",
            "Epoch: 038, Train Loss: 0.4291310, Train Acc: 0.8574074, Test Acc: 0.6000000\n",
            "Epoch: 039, Train Loss: 0.4507570, Train Acc: 0.8111111, Test Acc: 0.6000000\n",
            "Epoch: 040, Train Loss: 0.2423552, Train Acc: 0.9500000, Test Acc: 0.7000000\n",
            "Epoch: 041, Train Loss: 0.4532319, Train Acc: 0.8462963, Test Acc: 0.6666667\n",
            "Epoch: 042, Train Loss: 0.4513222, Train Acc: 0.8296296, Test Acc: 0.6000000\n",
            "Epoch: 043, Train Loss: 0.2664311, Train Acc: 0.9185185, Test Acc: 0.6833333\n",
            "Epoch: 044, Train Loss: 0.3614382, Train Acc: 0.8648148, Test Acc: 0.6166667\n",
            "Epoch: 045, Train Loss: 0.2283942, Train Acc: 0.9277778, Test Acc: 0.6666667\n",
            "Epoch: 046, Train Loss: 0.1605409, Train Acc: 0.9759259, Test Acc: 0.8166667\n",
            "Epoch: 047, Train Loss: 0.3327059, Train Acc: 0.8740741, Test Acc: 0.7666667\n",
            "Epoch: 048, Train Loss: 0.1555863, Train Acc: 0.9722222, Test Acc: 0.7500000\n",
            "Epoch: 049, Train Loss: 0.0797841, Train Acc: 0.9944444, Test Acc: 0.7833333\n",
            "Epoch: 050, Train Loss: 0.0761976, Train Acc: 0.9962963, Test Acc: 0.7833333\n",
            "Epoch: 051, Train Loss: 0.0446438, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 052, Train Loss: 0.0355488, Train Acc: 0.9962963, Test Acc: 0.7666667\n",
            "Epoch: 053, Train Loss: 0.0276387, Train Acc: 0.9981481, Test Acc: 0.8000000\n",
            "Epoch: 054, Train Loss: 0.0233694, Train Acc: 1.0000000, Test Acc: 0.8000000\n",
            "Epoch: 055, Train Loss: 0.0234331, Train Acc: 0.9981481, Test Acc: 0.8166667\n",
            "Epoch: 056, Train Loss: 0.0178589, Train Acc: 1.0000000, Test Acc: 0.8000000\n",
            "Epoch: 057, Train Loss: 0.0157772, Train Acc: 1.0000000, Test Acc: 0.8000000\n",
            "Epoch: 058, Train Loss: 0.0139917, Train Acc: 1.0000000, Test Acc: 0.8000000\n",
            "Epoch: 059, Train Loss: 0.0146459, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 060, Train Loss: 0.0123308, Train Acc: 1.0000000, Test Acc: 0.8000000\n",
            "Epoch: 061, Train Loss: 0.0120684, Train Acc: 1.0000000, Test Acc: 0.8166667\n",
            "Epoch: 062, Train Loss: 0.0112983, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 063, Train Loss: 0.0186046, Train Acc: 1.0000000, Test Acc: 0.8333333\n",
            "Epoch: 064, Train Loss: 0.0190486, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 065, Train Loss: 0.0873717, Train Acc: 0.9814815, Test Acc: 0.8000000\n",
            "Epoch: 066, Train Loss: 0.0751662, Train Acc: 0.9796296, Test Acc: 0.7166667\n",
            "Epoch: 067, Train Loss: 0.0146097, Train Acc: 1.0000000, Test Acc: 0.8500000\n",
            "Epoch: 068, Train Loss: 0.0120444, Train Acc: 1.0000000, Test Acc: 0.8000000\n",
            "Epoch: 069, Train Loss: 0.0089687, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 070, Train Loss: 0.0095980, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 071, Train Loss: 0.0080849, Train Acc: 1.0000000, Test Acc: 0.8166667\n",
            "Epoch: 072, Train Loss: 0.0071659, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 073, Train Loss: 0.0068789, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 074, Train Loss: 0.0068720, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 075, Train Loss: 0.0058290, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 076, Train Loss: 0.0056066, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 077, Train Loss: 0.0058180, Train Acc: 1.0000000, Test Acc: 0.8000000\n",
            "Epoch: 078, Train Loss: 0.0051981, Train Acc: 1.0000000, Test Acc: 0.8000000\n",
            "Epoch: 079, Train Loss: 0.0053369, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 080, Train Loss: 0.0049608, Train Acc: 1.0000000, Test Acc: 0.8000000\n",
            "Epoch: 081, Train Loss: 0.0047291, Train Acc: 1.0000000, Test Acc: 0.8000000\n",
            "Epoch: 082, Train Loss: 0.0046512, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 083, Train Loss: 0.0043959, Train Acc: 1.0000000, Test Acc: 0.8000000\n",
            "Epoch: 084, Train Loss: 0.0041449, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 085, Train Loss: 0.0039121, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 086, Train Loss: 0.0057575, Train Acc: 1.0000000, Test Acc: 0.8166667\n",
            "Epoch: 087, Train Loss: 0.0077091, Train Acc: 1.0000000, Test Acc: 0.8000000\n",
            "Epoch: 088, Train Loss: 0.0125806, Train Acc: 1.0000000, Test Acc: 0.8000000\n",
            "Epoch: 089, Train Loss: 0.0044664, Train Acc: 1.0000000, Test Acc: 0.8000000\n",
            "Epoch: 090, Train Loss: 0.0076997, Train Acc: 0.9981481, Test Acc: 0.8000000\n",
            "Epoch: 091, Train Loss: 0.0040019, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 092, Train Loss: 0.0070920, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 093, Train Loss: 0.0039139, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 094, Train Loss: 0.0037344, Train Acc: 1.0000000, Test Acc: 0.8166667\n",
            "Epoch: 095, Train Loss: 0.0034852, Train Acc: 1.0000000, Test Acc: 0.8000000\n",
            "Epoch: 096, Train Loss: 0.0029817, Train Acc: 1.0000000, Test Acc: 0.8000000\n",
            "Epoch: 097, Train Loss: 0.0028177, Train Acc: 1.0000000, Test Acc: 0.8000000\n",
            "Epoch: 098, Train Loss: 0.0027784, Train Acc: 1.0000000, Test Acc: 0.8000000\n",
            "Epoch: 099, Train Loss: 0.0026985, Train Acc: 1.0000000, Test Acc: 0.8000000\n",
            "Epoch: 100, Train Loss: 0.0026581, Train Acc: 1.0000000, Test Acc: 0.8000000\n",
            "Epoch: 101, Train Loss: 0.0024781, Train Acc: 1.0000000, Test Acc: 0.8000000\n",
            "Epoch: 102, Train Loss: 0.0023459, Train Acc: 1.0000000, Test Acc: 0.8000000\n",
            "Epoch: 103, Train Loss: 0.0022485, Train Acc: 1.0000000, Test Acc: 0.8000000\n",
            "Epoch: 104, Train Loss: 0.0023369, Train Acc: 1.0000000, Test Acc: 0.8166667\n",
            "Epoch: 105, Train Loss: 0.0022822, Train Acc: 1.0000000, Test Acc: 0.8166667\n",
            "Epoch: 106, Train Loss: 0.0021953, Train Acc: 1.0000000, Test Acc: 0.8000000\n",
            "Epoch: 107, Train Loss: 0.0020953, Train Acc: 1.0000000, Test Acc: 0.8166667\n",
            "Epoch: 108, Train Loss: 0.0021827, Train Acc: 1.0000000, Test Acc: 0.8166667\n",
            "Epoch: 109, Train Loss: 0.0021652, Train Acc: 1.0000000, Test Acc: 0.8166667\n",
            "Epoch: 110, Train Loss: 0.0020367, Train Acc: 1.0000000, Test Acc: 0.8166667\n",
            "Epoch: 111, Train Loss: 0.0019984, Train Acc: 1.0000000, Test Acc: 0.8000000\n",
            "Epoch: 112, Train Loss: 0.0019964, Train Acc: 1.0000000, Test Acc: 0.8000000\n",
            "Epoch: 113, Train Loss: 0.0019160, Train Acc: 1.0000000, Test Acc: 0.8000000\n",
            "Epoch: 114, Train Loss: 0.0022917, Train Acc: 1.0000000, Test Acc: 0.8166667\n",
            "Epoch: 115, Train Loss: 0.0019911, Train Acc: 1.0000000, Test Acc: 0.8000000\n",
            "Epoch: 116, Train Loss: 0.0018846, Train Acc: 1.0000000, Test Acc: 0.8000000\n",
            "Epoch: 117, Train Loss: 0.0018206, Train Acc: 1.0000000, Test Acc: 0.8000000\n",
            "Epoch: 118, Train Loss: 0.0017866, Train Acc: 1.0000000, Test Acc: 0.8166667\n",
            "Epoch: 119, Train Loss: 0.0017498, Train Acc: 1.0000000, Test Acc: 0.8000000\n",
            "Epoch: 120, Train Loss: 0.0017295, Train Acc: 1.0000000, Test Acc: 0.8166667\n",
            "Epoch: 121, Train Loss: 0.0018111, Train Acc: 1.0000000, Test Acc: 0.8166667\n",
            "Epoch: 122, Train Loss: 0.0017188, Train Acc: 1.0000000, Test Acc: 0.8166667\n",
            "Epoch: 123, Train Loss: 0.0017241, Train Acc: 1.0000000, Test Acc: 0.8166667\n",
            "Epoch: 124, Train Loss: 0.0017014, Train Acc: 1.0000000, Test Acc: 0.8166667\n",
            "Epoch: 125, Train Loss: 0.0016163, Train Acc: 1.0000000, Test Acc: 0.8166667\n",
            "Epoch: 126, Train Loss: 0.0018061, Train Acc: 1.0000000, Test Acc: 0.8166667\n",
            "Epoch: 127, Train Loss: 0.0016671, Train Acc: 1.0000000, Test Acc: 0.8000000\n",
            "Epoch: 128, Train Loss: 0.0015947, Train Acc: 1.0000000, Test Acc: 0.8000000\n",
            "Epoch: 129, Train Loss: 0.0015580, Train Acc: 1.0000000, Test Acc: 0.8000000\n",
            "Epoch: 130, Train Loss: 0.0015522, Train Acc: 1.0000000, Test Acc: 0.8166667\n",
            "Epoch: 131, Train Loss: 0.0015327, Train Acc: 1.0000000, Test Acc: 0.8000000\n",
            "Epoch: 132, Train Loss: 0.0014393, Train Acc: 1.0000000, Test Acc: 0.8166667\n",
            "Epoch: 133, Train Loss: 0.0015281, Train Acc: 1.0000000, Test Acc: 0.8000000\n",
            "Epoch: 134, Train Loss: 0.0014400, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 135, Train Loss: 0.0015065, Train Acc: 1.0000000, Test Acc: 0.8000000\n",
            "Epoch: 136, Train Loss: 0.0014593, Train Acc: 1.0000000, Test Acc: 0.8000000\n",
            "Epoch: 137, Train Loss: 0.0013939, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 138, Train Loss: 0.0013636, Train Acc: 1.0000000, Test Acc: 0.8000000\n",
            "Epoch: 139, Train Loss: 0.0013611, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 140, Train Loss: 0.0013231, Train Acc: 1.0000000, Test Acc: 0.8000000\n",
            "Epoch: 141, Train Loss: 0.0013666, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 142, Train Loss: 0.0014286, Train Acc: 1.0000000, Test Acc: 0.8000000\n",
            "Epoch: 143, Train Loss: 0.0013505, Train Acc: 1.0000000, Test Acc: 0.8166667\n",
            "Epoch: 144, Train Loss: 0.0013147, Train Acc: 1.0000000, Test Acc: 0.8000000\n",
            "Epoch: 145, Train Loss: 0.0013183, Train Acc: 1.0000000, Test Acc: 0.8166667\n",
            "Epoch: 146, Train Loss: 0.0012771, Train Acc: 1.0000000, Test Acc: 0.8000000\n",
            "Epoch: 147, Train Loss: 0.0012320, Train Acc: 1.0000000, Test Acc: 0.8000000\n",
            "Epoch: 148, Train Loss: 0.0012710, Train Acc: 1.0000000, Test Acc: 0.8000000\n",
            "Epoch: 149, Train Loss: 0.0011809, Train Acc: 1.0000000, Test Acc: 0.8000000\n",
            "Epoch: 150, Train Loss: 0.0011781, Train Acc: 1.0000000, Test Acc: 0.8000000\n",
            "Epoch: 151, Train Loss: 0.0011667, Train Acc: 1.0000000, Test Acc: 0.8000000\n",
            "Epoch: 152, Train Loss: 0.0011524, Train Acc: 1.0000000, Test Acc: 0.8000000\n",
            "Epoch: 153, Train Loss: 0.0012555, Train Acc: 1.0000000, Test Acc: 0.8000000\n",
            "Epoch: 154, Train Loss: 0.0011228, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 155, Train Loss: 0.0011131, Train Acc: 1.0000000, Test Acc: 0.8166667\n",
            "Epoch: 156, Train Loss: 0.0011524, Train Acc: 1.0000000, Test Acc: 0.8000000\n",
            "Epoch: 157, Train Loss: 0.0011111, Train Acc: 1.0000000, Test Acc: 0.8000000\n",
            "Epoch: 158, Train Loss: 0.0011379, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 159, Train Loss: 0.0011001, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 160, Train Loss: 0.0012883, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 161, Train Loss: 0.0011830, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 162, Train Loss: 0.0010690, Train Acc: 1.0000000, Test Acc: 0.8000000\n",
            "Epoch: 163, Train Loss: 0.0011438, Train Acc: 1.0000000, Test Acc: 0.8166667\n",
            "Epoch: 164, Train Loss: 0.0010561, Train Acc: 1.0000000, Test Acc: 0.8166667\n",
            "Epoch: 165, Train Loss: 0.0010604, Train Acc: 1.0000000, Test Acc: 0.8166667\n",
            "Epoch: 166, Train Loss: 0.0010803, Train Acc: 1.0000000, Test Acc: 0.8000000\n",
            "Epoch: 167, Train Loss: 0.0012708, Train Acc: 1.0000000, Test Acc: 0.8000000\n",
            "Epoch: 168, Train Loss: 0.0010868, Train Acc: 1.0000000, Test Acc: 0.8000000\n",
            "Epoch: 169, Train Loss: 0.0010570, Train Acc: 1.0000000, Test Acc: 0.8000000\n",
            "Epoch: 170, Train Loss: 0.0010587, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 171, Train Loss: 0.0010325, Train Acc: 1.0000000, Test Acc: 0.8000000\n",
            "Epoch: 172, Train Loss: 0.0017281, Train Acc: 1.0000000, Test Acc: 0.8166667\n",
            "Epoch: 173, Train Loss: 0.0030808, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 174, Train Loss: 0.0487235, Train Acc: 0.9870370, Test Acc: 0.7500000\n",
            "Epoch: 175, Train Loss: 0.0068533, Train Acc: 1.0000000, Test Acc: 0.7500000\n",
            "Epoch: 176, Train Loss: 0.0038135, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 177, Train Loss: 0.0035723, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 178, Train Loss: 0.0021886, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 179, Train Loss: 0.0018550, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 180, Train Loss: 0.0016295, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 181, Train Loss: 0.0015408, Train Acc: 1.0000000, Test Acc: 0.8000000\n",
            "Epoch: 182, Train Loss: 0.0013999, Train Acc: 1.0000000, Test Acc: 0.8000000\n",
            "Epoch: 183, Train Loss: 0.0014639, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 184, Train Loss: 0.0013272, Train Acc: 1.0000000, Test Acc: 0.8000000\n",
            "Epoch: 185, Train Loss: 0.0012612, Train Acc: 1.0000000, Test Acc: 0.8000000\n",
            "Epoch: 186, Train Loss: 0.0012889, Train Acc: 1.0000000, Test Acc: 0.8000000\n",
            "Epoch: 187, Train Loss: 0.0012896, Train Acc: 1.0000000, Test Acc: 0.8000000\n",
            "Epoch: 188, Train Loss: 0.0011277, Train Acc: 1.0000000, Test Acc: 0.8000000\n",
            "Epoch: 189, Train Loss: 0.0010744, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 190, Train Loss: 0.0010894, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 191, Train Loss: 0.0010429, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 192, Train Loss: 0.0009765, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 193, Train Loss: 0.0009872, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 194, Train Loss: 0.0009834, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 195, Train Loss: 0.0010259, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 196, Train Loss: 0.0012642, Train Acc: 1.0000000, Test Acc: 0.8333333\n",
            "Epoch: 197, Train Loss: 0.0011322, Train Acc: 1.0000000, Test Acc: 0.8000000\n",
            "Epoch: 198, Train Loss: 0.0009718, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 199, Train Loss: 0.0009565, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 200, Train Loss: 0.0009390, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 201, Train Loss: 0.0009351, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 202, Train Loss: 0.0008846, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 203, Train Loss: 0.0008792, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 204, Train Loss: 0.0008811, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 205, Train Loss: 0.0008483, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 206, Train Loss: 0.0008488, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 207, Train Loss: 0.0008265, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 208, Train Loss: 0.0008015, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 209, Train Loss: 0.0007974, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 210, Train Loss: 0.0007826, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 211, Train Loss: 0.0007848, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 212, Train Loss: 0.0007608, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 213, Train Loss: 0.0008073, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 214, Train Loss: 0.0007996, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 215, Train Loss: 0.0007785, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 216, Train Loss: 0.0007620, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 217, Train Loss: 0.0009037, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 218, Train Loss: 0.0008185, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 219, Train Loss: 0.0007658, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 220, Train Loss: 0.0007273, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 221, Train Loss: 0.0008043, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 222, Train Loss: 0.0007674, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 223, Train Loss: 0.0007272, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 224, Train Loss: 0.0007483, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 225, Train Loss: 0.0007392, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 226, Train Loss: 0.0006943, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 227, Train Loss: 0.0007913, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 228, Train Loss: 0.0007421, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 229, Train Loss: 0.0006906, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 230, Train Loss: 0.0006850, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 231, Train Loss: 0.0006981, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 232, Train Loss: 0.0007303, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 233, Train Loss: 0.0006957, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 234, Train Loss: 0.0008315, Train Acc: 1.0000000, Test Acc: 0.8000000\n",
            "Epoch: 235, Train Loss: 0.0007645, Train Acc: 1.0000000, Test Acc: 0.8000000\n",
            "Epoch: 236, Train Loss: 0.0007121, Train Acc: 1.0000000, Test Acc: 0.8000000\n",
            "Epoch: 237, Train Loss: 0.0007024, Train Acc: 1.0000000, Test Acc: 0.8000000\n",
            "Epoch: 238, Train Loss: 0.0006934, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 239, Train Loss: 0.0007004, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 240, Train Loss: 0.0006752, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 241, Train Loss: 0.0006847, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 242, Train Loss: 0.0006642, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 243, Train Loss: 0.0007660, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 244, Train Loss: 0.0006823, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 245, Train Loss: 0.0006435, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 246, Train Loss: 0.0006359, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 247, Train Loss: 0.0006294, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 248, Train Loss: 0.0006511, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 249, Train Loss: 0.0006313, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 250, Train Loss: 0.0006185, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 251, Train Loss: 0.0005970, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 252, Train Loss: 0.0006174, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 253, Train Loss: 0.0005971, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 254, Train Loss: 0.0005941, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 255, Train Loss: 0.0006435, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 256, Train Loss: 0.0005988, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 257, Train Loss: 0.0005857, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 258, Train Loss: 0.0006123, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 259, Train Loss: 0.0006101, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 260, Train Loss: 0.0006088, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 261, Train Loss: 0.0006061, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 262, Train Loss: 0.0005761, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 263, Train Loss: 0.0005698, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 264, Train Loss: 0.0005647, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 265, Train Loss: 0.0005753, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 266, Train Loss: 0.0005665, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 267, Train Loss: 0.0006445, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 268, Train Loss: 0.0005797, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 269, Train Loss: 0.0005696, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 270, Train Loss: 0.0006085, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 271, Train Loss: 0.0005907, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 272, Train Loss: 0.0005594, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 273, Train Loss: 0.0005670, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 274, Train Loss: 0.0005494, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 275, Train Loss: 0.0005378, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 276, Train Loss: 0.0005623, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 277, Train Loss: 0.0005734, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 278, Train Loss: 0.0005976, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 279, Train Loss: 0.0005527, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 280, Train Loss: 0.0005438, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 281, Train Loss: 0.0005484, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 282, Train Loss: 0.0005439, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 283, Train Loss: 0.0005305, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 284, Train Loss: 0.0005037, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 285, Train Loss: 0.0005115, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 286, Train Loss: 0.0005122, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 287, Train Loss: 0.0005269, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 288, Train Loss: 0.0005185, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 289, Train Loss: 0.0005099, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 290, Train Loss: 0.0005124, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 291, Train Loss: 0.0005042, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 292, Train Loss: 0.0005193, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 293, Train Loss: 0.0005371, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 294, Train Loss: 0.0005324, Train Acc: 1.0000000, Test Acc: 0.8000000\n",
            "Epoch: 295, Train Loss: 0.0005085, Train Acc: 1.0000000, Test Acc: 0.8000000\n",
            "Epoch: 296, Train Loss: 0.0005062, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 297, Train Loss: 0.0004781, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 298, Train Loss: 0.0005320, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 299, Train Loss: 0.0005266, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 300, Train Loss: 0.0005287, Train Acc: 1.0000000, Test Acc: 0.8000000\n",
            "Epoch: 301, Train Loss: 0.0005449, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 302, Train Loss: 0.0005025, Train Acc: 1.0000000, Test Acc: 0.8000000\n",
            "Epoch: 303, Train Loss: 0.0004907, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 304, Train Loss: 0.0004745, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 305, Train Loss: 0.0005012, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 306, Train Loss: 0.0004810, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 307, Train Loss: 0.0004974, Train Acc: 1.0000000, Test Acc: 0.8000000\n",
            "Epoch: 308, Train Loss: 0.0005210, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 309, Train Loss: 0.0005036, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 310, Train Loss: 0.0006007, Train Acc: 1.0000000, Test Acc: 0.8000000\n",
            "Epoch: 311, Train Loss: 0.0005337, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 312, Train Loss: 0.0005086, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 313, Train Loss: 0.0004914, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 314, Train Loss: 0.0004901, Train Acc: 1.0000000, Test Acc: 0.8000000\n",
            "Epoch: 315, Train Loss: 0.0004867, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 316, Train Loss: 0.0005043, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 317, Train Loss: 0.0005167, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 318, Train Loss: 0.0005532, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 319, Train Loss: 0.0005394, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 320, Train Loss: 0.0005115, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 321, Train Loss: 0.0004871, Train Acc: 1.0000000, Test Acc: 0.8000000\n",
            "Epoch: 322, Train Loss: 0.0004672, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 323, Train Loss: 0.0004462, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 324, Train Loss: 0.0004629, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 325, Train Loss: 0.0004854, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 326, Train Loss: 0.0004672, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 327, Train Loss: 0.0004943, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 328, Train Loss: 0.0009442, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 329, Train Loss: 0.0007083, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 330, Train Loss: 0.0006119, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 331, Train Loss: 0.0005525, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 332, Train Loss: 0.0005593, Train Acc: 1.0000000, Test Acc: 0.8000000\n",
            "Epoch: 333, Train Loss: 0.0005560, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 334, Train Loss: 0.0005394, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 335, Train Loss: 0.0005257, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 336, Train Loss: 0.0004988, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 337, Train Loss: 0.0004812, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 338, Train Loss: 0.0004801, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 339, Train Loss: 0.0005068, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 340, Train Loss: 0.0004874, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 341, Train Loss: 0.0004770, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 342, Train Loss: 0.0004803, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 343, Train Loss: 0.0005533, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 344, Train Loss: 0.0005065, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 345, Train Loss: 0.0004947, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 346, Train Loss: 0.0004975, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 347, Train Loss: 0.0004928, Train Acc: 1.0000000, Test Acc: 0.8000000\n",
            "Epoch: 348, Train Loss: 0.0004893, Train Acc: 1.0000000, Test Acc: 0.8000000\n",
            "Epoch: 349, Train Loss: 0.0004840, Train Acc: 1.0000000, Test Acc: 0.8000000\n",
            "Epoch: 350, Train Loss: 0.0004816, Train Acc: 1.0000000, Test Acc: 0.8000000\n",
            "Epoch: 351, Train Loss: 0.0004648, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 352, Train Loss: 0.0004620, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 353, Train Loss: 0.0004733, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 354, Train Loss: 0.0004838, Train Acc: 1.0000000, Test Acc: 0.8000000\n",
            "Epoch: 355, Train Loss: 0.0004752, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 356, Train Loss: 0.0004888, Train Acc: 1.0000000, Test Acc: 0.8000000\n",
            "Epoch: 357, Train Loss: 0.0004625, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 358, Train Loss: 0.0004752, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 359, Train Loss: 0.0004753, Train Acc: 1.0000000, Test Acc: 0.8000000\n",
            "Epoch: 360, Train Loss: 0.0004479, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 361, Train Loss: 0.0005078, Train Acc: 1.0000000, Test Acc: 0.8000000\n",
            "Epoch: 362, Train Loss: 0.0004947, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 363, Train Loss: 0.0004506, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 364, Train Loss: 0.0004508, Train Acc: 1.0000000, Test Acc: 0.8000000\n",
            "Epoch: 365, Train Loss: 0.0004341, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 366, Train Loss: 0.0004466, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 367, Train Loss: 0.0004666, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 368, Train Loss: 0.0004547, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 369, Train Loss: 0.0004409, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 370, Train Loss: 0.0004382, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 371, Train Loss: 0.0004498, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 372, Train Loss: 0.0004504, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 373, Train Loss: 0.0004365, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 374, Train Loss: 0.0004707, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 375, Train Loss: 0.0004300, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 376, Train Loss: 0.0004378, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 377, Train Loss: 0.0004373, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 378, Train Loss: 0.0004301, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 379, Train Loss: 0.0004408, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 380, Train Loss: 0.0004458, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 381, Train Loss: 0.0004350, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 382, Train Loss: 0.0004325, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 383, Train Loss: 0.0004341, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 384, Train Loss: 0.0004317, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 385, Train Loss: 0.0004365, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 386, Train Loss: 0.0004425, Train Acc: 1.0000000, Test Acc: 0.8000000\n",
            "Epoch: 387, Train Loss: 0.0004316, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 388, Train Loss: 0.0004530, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 389, Train Loss: 0.0004351, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 390, Train Loss: 0.0004177, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 391, Train Loss: 0.0004397, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 392, Train Loss: 0.0004221, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 393, Train Loss: 0.0004320, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 394, Train Loss: 0.0004164, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 395, Train Loss: 0.0004324, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 396, Train Loss: 0.0004285, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 397, Train Loss: 0.0004271, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 398, Train Loss: 0.0004278, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 399, Train Loss: 0.0004383, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 400, Train Loss: 0.0004306, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 401, Train Loss: 0.0004208, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 402, Train Loss: 0.0004043, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 403, Train Loss: 0.0004123, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 404, Train Loss: 0.0004514, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 405, Train Loss: 0.0004341, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 406, Train Loss: 0.0004161, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 407, Train Loss: 0.0004102, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 408, Train Loss: 0.0004125, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 409, Train Loss: 0.0004073, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 410, Train Loss: 0.0003988, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 411, Train Loss: 0.0004009, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 412, Train Loss: 0.0003912, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 413, Train Loss: 0.0004016, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 414, Train Loss: 0.0004271, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 415, Train Loss: 0.0004198, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 416, Train Loss: 0.0004094, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 417, Train Loss: 0.0004129, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 418, Train Loss: 0.0003941, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 419, Train Loss: 0.0003908, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 420, Train Loss: 0.0004213, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 421, Train Loss: 0.0004352, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 422, Train Loss: 0.0003968, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 423, Train Loss: 0.0003926, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 424, Train Loss: 0.0004202, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 425, Train Loss: 0.0004015, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 426, Train Loss: 0.0004209, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 427, Train Loss: 0.0004359, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 428, Train Loss: 0.0004104, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 429, Train Loss: 0.0003852, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 430, Train Loss: 0.0003734, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 431, Train Loss: 0.0003923, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 432, Train Loss: 0.0003962, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 433, Train Loss: 0.0003809, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 434, Train Loss: 0.0003798, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 435, Train Loss: 0.0003910, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 436, Train Loss: 0.0003874, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 437, Train Loss: 0.0003857, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 438, Train Loss: 0.0003963, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 439, Train Loss: 0.0004214, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 440, Train Loss: 0.0004124, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 441, Train Loss: 0.0003969, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 442, Train Loss: 0.0003969, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 443, Train Loss: 0.0004094, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 444, Train Loss: 0.0004123, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 445, Train Loss: 0.0003988, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 446, Train Loss: 0.0003837, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 447, Train Loss: 0.0003899, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 448, Train Loss: 0.0003859, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 449, Train Loss: 0.0004052, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 450, Train Loss: 0.0003998, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 451, Train Loss: 0.0003955, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 452, Train Loss: 0.0004038, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 453, Train Loss: 0.0003996, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 454, Train Loss: 0.0003929, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 455, Train Loss: 0.0003897, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 456, Train Loss: 0.0004458, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 457, Train Loss: 0.0003977, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 458, Train Loss: 0.0003989, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 459, Train Loss: 0.0004147, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 460, Train Loss: 0.0003847, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 461, Train Loss: 0.0004016, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 462, Train Loss: 0.0003897, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 463, Train Loss: 0.0004506, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 464, Train Loss: 0.0004015, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 465, Train Loss: 0.0003896, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 466, Train Loss: 0.0004179, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 467, Train Loss: 0.0003940, Train Acc: 1.0000000, Test Acc: 0.8000000\n",
            "Epoch: 468, Train Loss: 0.0004137, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 469, Train Loss: 0.0003929, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 470, Train Loss: 0.0003805, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 471, Train Loss: 0.0003757, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 472, Train Loss: 0.0003765, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 473, Train Loss: 0.0003651, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 474, Train Loss: 0.0003678, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 475, Train Loss: 0.0003907, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 476, Train Loss: 0.0003728, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 477, Train Loss: 0.0003828, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 478, Train Loss: 0.0003766, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 479, Train Loss: 0.0004248, Train Acc: 1.0000000, Test Acc: 0.8000000\n",
            "Epoch: 480, Train Loss: 0.0003954, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 481, Train Loss: 0.0004051, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 482, Train Loss: 0.0003778, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 483, Train Loss: 0.0003920, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 484, Train Loss: 0.0003802, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 485, Train Loss: 0.0003788, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 486, Train Loss: 0.0003717, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 487, Train Loss: 0.0003904, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 488, Train Loss: 0.0003910, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 489, Train Loss: 0.0003846, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 490, Train Loss: 0.0003797, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 491, Train Loss: 0.0003777, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 492, Train Loss: 0.0003704, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 493, Train Loss: 0.0003821, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 494, Train Loss: 0.0003873, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 495, Train Loss: 0.0003711, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 496, Train Loss: 0.0003622, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 497, Train Loss: 0.0003675, Train Acc: 1.0000000, Test Acc: 0.7666667\n",
            "Epoch: 498, Train Loss: 0.0003731, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 499, Train Loss: 0.0003796, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "Epoch: 500, Train Loss: 0.0003876, Train Acc: 1.0000000, Test Acc: 0.7833333\n",
            "--------\n",
            "Best Test Acc:   0.7208333333333332_0.08965597829729173, Epoch: 129\n",
            "Best Train Loss: 0.00037425855282183164_6.415941540964229e-05\n",
            "Best Train Acc:  1.0_0.0\n",
            "FOLD8, ENZYMES, 1668698769/_rhoE0.5P18I1_GELU_X_6_256_0_0.001_50_0.6_B64S0W8, BT=0.7208333333333332_0.08965597829729173, BE=129, ID=\n",
            "train_set : test_set = %d : %d 540 60\n",
            "-------- FOLD8 DATASET=ENZYMES, COMMIT_ID=\n",
            "#Params: 1334534\n",
            "Epoch: 001, Train Loss: 4.8063758, Train Acc: 0.1925926, Test Acc: 0.2333333\n",
            "Epoch: 002, Train Loss: 3.9996383, Train Acc: 0.2148148, Test Acc: 0.2000000\n",
            "Epoch: 003, Train Loss: 1.6845951, Train Acc: 0.2777778, Test Acc: 0.3500000\n",
            "Epoch: 004, Train Loss: 1.7745615, Train Acc: 0.3203704, Test Acc: 0.3000000\n",
            "Epoch: 005, Train Loss: 1.5495474, Train Acc: 0.4370370, Test Acc: 0.4500000\n",
            "Epoch: 006, Train Loss: 1.6195827, Train Acc: 0.3333333, Test Acc: 0.3166667\n",
            "Epoch: 007, Train Loss: 1.4114525, Train Acc: 0.4907407, Test Acc: 0.4166667\n",
            "Epoch: 008, Train Loss: 1.5072694, Train Acc: 0.4111111, Test Acc: 0.3666667\n",
            "Epoch: 009, Train Loss: 1.7291836, Train Acc: 0.3685185, Test Acc: 0.3833333\n",
            "Epoch: 010, Train Loss: 1.5128363, Train Acc: 0.4518519, Test Acc: 0.4333333\n",
            "Epoch: 011, Train Loss: 1.4026816, Train Acc: 0.4333333, Test Acc: 0.3333333\n",
            "Epoch: 012, Train Loss: 1.4155451, Train Acc: 0.4074074, Test Acc: 0.3166667\n",
            "Epoch: 013, Train Loss: 1.1546912, Train Acc: 0.5796296, Test Acc: 0.5500000\n",
            "Epoch: 014, Train Loss: 1.2021631, Train Acc: 0.5888889, Test Acc: 0.4166667\n",
            "Epoch: 015, Train Loss: 1.0990111, Train Acc: 0.6222222, Test Acc: 0.4666667\n",
            "Epoch: 016, Train Loss: 1.0530723, Train Acc: 0.6203704, Test Acc: 0.5000000\n",
            "Epoch: 017, Train Loss: 1.2421564, Train Acc: 0.5074074, Test Acc: 0.4166667\n",
            "Epoch: 018, Train Loss: 0.9690243, Train Acc: 0.6444444, Test Acc: 0.5333333\n",
            "Epoch: 019, Train Loss: 0.8903916, Train Acc: 0.7425926, Test Acc: 0.5833333\n",
            "Epoch: 020, Train Loss: 1.0032382, Train Acc: 0.6240741, Test Acc: 0.5000000\n",
            "Epoch: 021, Train Loss: 0.8902811, Train Acc: 0.7351852, Test Acc: 0.5000000\n",
            "Epoch: 022, Train Loss: 0.9808436, Train Acc: 0.6296296, Test Acc: 0.5166667\n",
            "Epoch: 023, Train Loss: 1.0624790, Train Acc: 0.6481481, Test Acc: 0.4666667\n",
            "Epoch: 024, Train Loss: 0.8455038, Train Acc: 0.7185185, Test Acc: 0.5166667\n",
            "Epoch: 025, Train Loss: 0.8004132, Train Acc: 0.7277778, Test Acc: 0.5000000\n",
            "Epoch: 026, Train Loss: 1.0049123, Train Acc: 0.6648148, Test Acc: 0.4500000\n",
            "Epoch: 027, Train Loss: 0.6469262, Train Acc: 0.8000000, Test Acc: 0.6500000\n",
            "Epoch: 028, Train Loss: 0.9345770, Train Acc: 0.6870370, Test Acc: 0.5333333\n",
            "Epoch: 029, Train Loss: 0.7035321, Train Acc: 0.7333333, Test Acc: 0.5333333\n",
            "Epoch: 030, Train Loss: 0.7951459, Train Acc: 0.6944444, Test Acc: 0.5000000\n",
            "Epoch: 031, Train Loss: 0.5175801, Train Acc: 0.8407407, Test Acc: 0.6500000\n",
            "Epoch: 032, Train Loss: 0.6353182, Train Acc: 0.7481481, Test Acc: 0.5000000\n",
            "Epoch: 033, Train Loss: 0.5059509, Train Acc: 0.8259259, Test Acc: 0.5500000\n",
            "Epoch: 034, Train Loss: 0.5566134, Train Acc: 0.8203704, Test Acc: 0.5833333\n",
            "Epoch: 035, Train Loss: 0.4371094, Train Acc: 0.8518519, Test Acc: 0.5500000\n",
            "Epoch: 036, Train Loss: 0.5708555, Train Acc: 0.8018519, Test Acc: 0.5666667\n",
            "Epoch: 037, Train Loss: 0.6322713, Train Acc: 0.7833333, Test Acc: 0.5833333\n",
            "Epoch: 038, Train Loss: 0.5420944, Train Acc: 0.8166667, Test Acc: 0.6500000\n",
            "Epoch: 039, Train Loss: 0.4552114, Train Acc: 0.8666667, Test Acc: 0.5666667\n",
            "Epoch: 040, Train Loss: 0.3145018, Train Acc: 0.9351852, Test Acc: 0.6666667\n",
            "Epoch: 041, Train Loss: 0.4804643, Train Acc: 0.8518519, Test Acc: 0.6000000\n",
            "Epoch: 042, Train Loss: 0.2766775, Train Acc: 0.9518519, Test Acc: 0.6166667\n",
            "Epoch: 043, Train Loss: 0.2525918, Train Acc: 0.9277778, Test Acc: 0.6833333\n",
            "Epoch: 044, Train Loss: 0.1817685, Train Acc: 0.9629630, Test Acc: 0.6833333\n",
            "Epoch: 045, Train Loss: 0.2835236, Train Acc: 0.9203704, Test Acc: 0.6166667\n",
            "Epoch: 046, Train Loss: 0.1930122, Train Acc: 0.9444444, Test Acc: 0.6333333\n",
            "Epoch: 047, Train Loss: 0.1176664, Train Acc: 0.9833333, Test Acc: 0.6333333\n",
            "Epoch: 048, Train Loss: 0.1855627, Train Acc: 0.9407407, Test Acc: 0.6500000\n",
            "Epoch: 049, Train Loss: 0.1169381, Train Acc: 0.9722222, Test Acc: 0.7000000\n",
            "Epoch: 050, Train Loss: 0.1570912, Train Acc: 0.9574074, Test Acc: 0.6833333\n",
            "Epoch: 051, Train Loss: 0.0488458, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 052, Train Loss: 0.0377018, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 053, Train Loss: 0.0329179, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 054, Train Loss: 0.0265035, Train Acc: 0.9981481, Test Acc: 0.7166667\n",
            "Epoch: 055, Train Loss: 0.0247526, Train Acc: 0.9944444, Test Acc: 0.7333333\n",
            "Epoch: 056, Train Loss: 0.0240192, Train Acc: 0.9981481, Test Acc: 0.7000000\n",
            "Epoch: 057, Train Loss: 0.0191488, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 058, Train Loss: 0.0181974, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 059, Train Loss: 0.0224942, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 060, Train Loss: 0.0176266, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 061, Train Loss: 0.0149071, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 062, Train Loss: 0.0151365, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 063, Train Loss: 0.0150576, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 064, Train Loss: 0.0123607, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 065, Train Loss: 0.0126038, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 066, Train Loss: 0.0105813, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 067, Train Loss: 0.0087407, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 068, Train Loss: 0.0079165, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 069, Train Loss: 0.0087888, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 070, Train Loss: 0.0088710, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 071, Train Loss: 0.0084454, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 072, Train Loss: 0.0101342, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 073, Train Loss: 0.0069597, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 074, Train Loss: 0.0068521, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 075, Train Loss: 0.0066849, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 076, Train Loss: 0.0053181, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 077, Train Loss: 0.0053944, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 078, Train Loss: 0.0067258, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 079, Train Loss: 0.0054753, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 080, Train Loss: 0.0047815, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 081, Train Loss: 0.0047950, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 082, Train Loss: 0.0045425, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 083, Train Loss: 0.0046396, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 084, Train Loss: 0.0050894, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 085, Train Loss: 0.0159545, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 086, Train Loss: 0.0093102, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 087, Train Loss: 0.0050970, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 088, Train Loss: 0.0039052, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 089, Train Loss: 0.0034917, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 090, Train Loss: 0.0035035, Train Acc: 1.0000000, Test Acc: 0.7500000\n",
            "Epoch: 091, Train Loss: 0.0032232, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 092, Train Loss: 0.0032256, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 093, Train Loss: 0.0032370, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 094, Train Loss: 0.0034477, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 095, Train Loss: 0.0031018, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 096, Train Loss: 0.0028897, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 097, Train Loss: 0.0028329, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 098, Train Loss: 0.0026608, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 099, Train Loss: 0.0027852, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 100, Train Loss: 0.0032638, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 101, Train Loss: 0.0034059, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 102, Train Loss: 0.0028669, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 103, Train Loss: 0.0031935, Train Acc: 1.0000000, Test Acc: 0.7500000\n",
            "Epoch: 104, Train Loss: 0.0026020, Train Acc: 1.0000000, Test Acc: 0.7500000\n",
            "Epoch: 105, Train Loss: 0.0023824, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 106, Train Loss: 0.0022131, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 107, Train Loss: 0.0021377, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 108, Train Loss: 0.0027887, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 109, Train Loss: 0.0022487, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 110, Train Loss: 0.0023497, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 111, Train Loss: 0.0021702, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 112, Train Loss: 0.0020381, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 113, Train Loss: 0.0030544, Train Acc: 1.0000000, Test Acc: 0.7500000\n",
            "Epoch: 114, Train Loss: 0.0021419, Train Acc: 1.0000000, Test Acc: 0.7500000\n",
            "Epoch: 115, Train Loss: 0.0019781, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 116, Train Loss: 0.0020135, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 117, Train Loss: 0.0018198, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 118, Train Loss: 0.0017257, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 119, Train Loss: 0.0017333, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 120, Train Loss: 0.0017032, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 121, Train Loss: 0.0016525, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 122, Train Loss: 0.0015639, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 123, Train Loss: 0.0015244, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 124, Train Loss: 0.0015530, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 125, Train Loss: 0.0016922, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 126, Train Loss: 0.0016540, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 127, Train Loss: 0.0018850, Train Acc: 1.0000000, Test Acc: 0.7500000\n",
            "Epoch: 128, Train Loss: 0.0016099, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 129, Train Loss: 0.0015257, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 130, Train Loss: 0.0015379, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 131, Train Loss: 0.0015216, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 132, Train Loss: 0.0017878, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 133, Train Loss: 0.0014863, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 134, Train Loss: 0.0014655, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 135, Train Loss: 0.0013693, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 136, Train Loss: 0.0013253, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 137, Train Loss: 0.0013304, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 138, Train Loss: 0.0012739, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 139, Train Loss: 0.0012482, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 140, Train Loss: 0.0011770, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 141, Train Loss: 0.0011941, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 142, Train Loss: 0.0011657, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 143, Train Loss: 0.0012058, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 144, Train Loss: 0.0011961, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 145, Train Loss: 0.0012649, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 146, Train Loss: 0.0011548, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 147, Train Loss: 0.0011220, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 148, Train Loss: 0.0011015, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 149, Train Loss: 0.0010672, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 150, Train Loss: 0.0010713, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 151, Train Loss: 0.0010238, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 152, Train Loss: 0.0010295, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 153, Train Loss: 0.0010356, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 154, Train Loss: 0.0010284, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 155, Train Loss: 0.0010224, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 156, Train Loss: 0.0010594, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 157, Train Loss: 0.0010520, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 158, Train Loss: 0.0010135, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 159, Train Loss: 0.0011012, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 160, Train Loss: 0.0010318, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 161, Train Loss: 0.0010654, Train Acc: 1.0000000, Test Acc: 0.7500000\n",
            "Epoch: 162, Train Loss: 0.0010189, Train Acc: 1.0000000, Test Acc: 0.7500000\n",
            "Epoch: 163, Train Loss: 0.0011196, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 164, Train Loss: 0.0010009, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 165, Train Loss: 0.0009762, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 166, Train Loss: 0.0010028, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 167, Train Loss: 0.0009327, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 168, Train Loss: 0.0009038, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 169, Train Loss: 0.0009341, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 170, Train Loss: 0.0009948, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 171, Train Loss: 0.0009729, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 172, Train Loss: 0.0009451, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 173, Train Loss: 0.0009398, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 174, Train Loss: 0.0008899, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 175, Train Loss: 0.0008803, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 176, Train Loss: 0.0008999, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 177, Train Loss: 0.0008928, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 178, Train Loss: 0.0008518, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 179, Train Loss: 0.0009597, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 180, Train Loss: 0.0008564, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 181, Train Loss: 0.0008186, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 182, Train Loss: 0.0008004, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 183, Train Loss: 0.0008121, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 184, Train Loss: 0.0007874, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 185, Train Loss: 0.0007774, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 186, Train Loss: 0.0007846, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 187, Train Loss: 0.0008328, Train Acc: 1.0000000, Test Acc: 0.7500000\n",
            "Epoch: 188, Train Loss: 0.0008279, Train Acc: 1.0000000, Test Acc: 0.7500000\n",
            "Epoch: 189, Train Loss: 0.0007568, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 190, Train Loss: 0.0007862, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 191, Train Loss: 0.0007280, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 192, Train Loss: 0.0007187, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 193, Train Loss: 0.0007363, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 194, Train Loss: 0.0007614, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 195, Train Loss: 0.0007307, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 196, Train Loss: 0.0006953, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 197, Train Loss: 0.0008014, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 198, Train Loss: 0.0007421, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 199, Train Loss: 0.0006993, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 200, Train Loss: 0.0007195, Train Acc: 1.0000000, Test Acc: 0.7500000\n",
            "Epoch: 201, Train Loss: 0.0007039, Train Acc: 1.0000000, Test Acc: 0.7500000\n",
            "Epoch: 202, Train Loss: 0.0006916, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 203, Train Loss: 0.0006962, Train Acc: 1.0000000, Test Acc: 0.7500000\n",
            "Epoch: 204, Train Loss: 0.0006914, Train Acc: 1.0000000, Test Acc: 0.7500000\n",
            "Epoch: 205, Train Loss: 0.0006960, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 206, Train Loss: 0.0006567, Train Acc: 1.0000000, Test Acc: 0.7500000\n",
            "Epoch: 207, Train Loss: 0.0010016, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 208, Train Loss: 0.0008264, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 209, Train Loss: 0.0006994, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 210, Train Loss: 0.0006797, Train Acc: 1.0000000, Test Acc: 0.7500000\n",
            "Epoch: 211, Train Loss: 0.0007267, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 212, Train Loss: 0.0006950, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 213, Train Loss: 0.0006699, Train Acc: 1.0000000, Test Acc: 0.7500000\n",
            "Epoch: 214, Train Loss: 0.0006612, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 215, Train Loss: 0.0006507, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 216, Train Loss: 0.0007494, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 217, Train Loss: 0.0006748, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 218, Train Loss: 0.0007191, Train Acc: 1.0000000, Test Acc: 0.7500000\n",
            "Epoch: 219, Train Loss: 0.0007007, Train Acc: 1.0000000, Test Acc: 0.7500000\n",
            "Epoch: 220, Train Loss: 0.0006638, Train Acc: 1.0000000, Test Acc: 0.7500000\n",
            "Epoch: 221, Train Loss: 0.0006613, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 222, Train Loss: 0.0006615, Train Acc: 1.0000000, Test Acc: 0.7500000\n",
            "Epoch: 223, Train Loss: 0.0006175, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 224, Train Loss: 0.0006124, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 225, Train Loss: 0.0007178, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 226, Train Loss: 0.0006524, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 227, Train Loss: 0.0006356, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 228, Train Loss: 0.0006134, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 229, Train Loss: 0.0006106, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 230, Train Loss: 0.0006285, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 231, Train Loss: 0.0005971, Train Acc: 1.0000000, Test Acc: 0.7500000\n",
            "Epoch: 232, Train Loss: 0.0006163, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 233, Train Loss: 0.0005838, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 234, Train Loss: 0.0005957, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 235, Train Loss: 0.0005836, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 236, Train Loss: 0.0006016, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 237, Train Loss: 0.0006682, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 238, Train Loss: 0.0006307, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 239, Train Loss: 0.0006057, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 240, Train Loss: 0.0005814, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 241, Train Loss: 0.0005988, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 242, Train Loss: 0.0006232, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 243, Train Loss: 0.0005819, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 244, Train Loss: 0.0006832, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 245, Train Loss: 0.0005890, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 246, Train Loss: 0.0005897, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 247, Train Loss: 0.0005544, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 248, Train Loss: 0.0005724, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 249, Train Loss: 0.0005594, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 250, Train Loss: 0.0005541, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 251, Train Loss: 0.0005408, Train Acc: 1.0000000, Test Acc: 0.7500000\n",
            "Epoch: 252, Train Loss: 0.0005166, Train Acc: 1.0000000, Test Acc: 0.7500000\n",
            "Epoch: 253, Train Loss: 0.0005120, Train Acc: 1.0000000, Test Acc: 0.7500000\n",
            "Epoch: 254, Train Loss: 0.0005205, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 255, Train Loss: 0.0005471, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 256, Train Loss: 0.0005435, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 257, Train Loss: 0.0005455, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 258, Train Loss: 0.0005299, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 259, Train Loss: 0.0005155, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 260, Train Loss: 0.0005153, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 261, Train Loss: 0.0005401, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 262, Train Loss: 0.0005074, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 263, Train Loss: 0.0005113, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 264, Train Loss: 0.0005201, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 265, Train Loss: 0.0005049, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 266, Train Loss: 0.0005147, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 267, Train Loss: 0.0005167, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 268, Train Loss: 0.0005025, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 269, Train Loss: 0.0006501, Train Acc: 1.0000000, Test Acc: 0.7500000\n",
            "Epoch: 270, Train Loss: 0.0006356, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 271, Train Loss: 0.0005696, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 272, Train Loss: 0.0005077, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 273, Train Loss: 0.0005232, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 274, Train Loss: 0.0005018, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 275, Train Loss: 0.0004829, Train Acc: 1.0000000, Test Acc: 0.7500000\n",
            "Epoch: 276, Train Loss: 0.0004842, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 277, Train Loss: 0.0005101, Train Acc: 1.0000000, Test Acc: 0.7500000\n",
            "Epoch: 278, Train Loss: 0.0004967, Train Acc: 1.0000000, Test Acc: 0.7500000\n",
            "Epoch: 279, Train Loss: 0.0005050, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 280, Train Loss: 0.0004948, Train Acc: 1.0000000, Test Acc: 0.7500000\n",
            "Epoch: 281, Train Loss: 0.0005742, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 282, Train Loss: 0.0005159, Train Acc: 1.0000000, Test Acc: 0.7500000\n",
            "Epoch: 283, Train Loss: 0.0005048, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 284, Train Loss: 0.0004996, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 285, Train Loss: 0.0004814, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 286, Train Loss: 0.0004945, Train Acc: 1.0000000, Test Acc: 0.7500000\n",
            "Epoch: 287, Train Loss: 0.0005419, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 288, Train Loss: 0.0005364, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 289, Train Loss: 0.0005233, Train Acc: 1.0000000, Test Acc: 0.7500000\n",
            "Epoch: 290, Train Loss: 0.0004985, Train Acc: 1.0000000, Test Acc: 0.7500000\n",
            "Epoch: 291, Train Loss: 0.0005150, Train Acc: 1.0000000, Test Acc: 0.7500000\n",
            "Epoch: 292, Train Loss: 0.0005168, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 293, Train Loss: 0.0004832, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 294, Train Loss: 0.0004863, Train Acc: 1.0000000, Test Acc: 0.7500000\n",
            "Epoch: 295, Train Loss: 0.0005360, Train Acc: 1.0000000, Test Acc: 0.7500000\n",
            "Epoch: 296, Train Loss: 0.0005288, Train Acc: 1.0000000, Test Acc: 0.7500000\n",
            "Epoch: 297, Train Loss: 0.0004693, Train Acc: 1.0000000, Test Acc: 0.7500000\n",
            "Epoch: 298, Train Loss: 0.0004672, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 299, Train Loss: 0.0004634, Train Acc: 1.0000000, Test Acc: 0.7500000\n",
            "Epoch: 300, Train Loss: 0.0004641, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 301, Train Loss: 0.0004564, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 302, Train Loss: 0.0004851, Train Acc: 1.0000000, Test Acc: 0.7500000\n",
            "Epoch: 303, Train Loss: 0.0004586, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 304, Train Loss: 0.0004412, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 305, Train Loss: 0.0004494, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 306, Train Loss: 0.0004529, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 307, Train Loss: 0.0004466, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 308, Train Loss: 0.0005055, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 309, Train Loss: 0.0004570, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 310, Train Loss: 0.0004328, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 311, Train Loss: 0.0004858, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 312, Train Loss: 0.0004884, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 313, Train Loss: 0.0004590, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 314, Train Loss: 0.0004571, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 315, Train Loss: 0.0004654, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 316, Train Loss: 0.0004325, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 317, Train Loss: 0.0004441, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 318, Train Loss: 0.0004496, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 319, Train Loss: 0.0004283, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 320, Train Loss: 0.0004301, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 321, Train Loss: 0.0004313, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 322, Train Loss: 0.0004207, Train Acc: 1.0000000, Test Acc: 0.7500000\n",
            "Epoch: 323, Train Loss: 0.0004191, Train Acc: 1.0000000, Test Acc: 0.7500000\n",
            "Epoch: 324, Train Loss: 0.0004211, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 325, Train Loss: 0.0004074, Train Acc: 1.0000000, Test Acc: 0.7500000\n",
            "Epoch: 326, Train Loss: 0.0004817, Train Acc: 1.0000000, Test Acc: 0.7500000\n",
            "Epoch: 327, Train Loss: 0.0004729, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 328, Train Loss: 0.0004660, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 329, Train Loss: 0.0004304, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 330, Train Loss: 0.0004282, Train Acc: 1.0000000, Test Acc: 0.7500000\n",
            "Epoch: 331, Train Loss: 0.0005078, Train Acc: 1.0000000, Test Acc: 0.7500000\n",
            "Epoch: 332, Train Loss: 0.0004415, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 333, Train Loss: 0.0005011, Train Acc: 1.0000000, Test Acc: 0.7500000\n",
            "Epoch: 334, Train Loss: 0.0004675, Train Acc: 1.0000000, Test Acc: 0.7500000\n",
            "Epoch: 335, Train Loss: 0.0004211, Train Acc: 1.0000000, Test Acc: 0.7500000\n",
            "Epoch: 336, Train Loss: 0.0004352, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 337, Train Loss: 0.0004344, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 338, Train Loss: 0.0004223, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 339, Train Loss: 0.0004077, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 340, Train Loss: 0.0004167, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 341, Train Loss: 0.0004029, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 342, Train Loss: 0.0004190, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 343, Train Loss: 0.0004128, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 344, Train Loss: 0.0004171, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 345, Train Loss: 0.0004254, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 346, Train Loss: 0.0004177, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 347, Train Loss: 0.0004464, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 348, Train Loss: 0.0004386, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 349, Train Loss: 0.0004288, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 350, Train Loss: 0.0004046, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 351, Train Loss: 0.0004041, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 352, Train Loss: 0.0004017, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 353, Train Loss: 0.0004110, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 354, Train Loss: 0.0003924, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 355, Train Loss: 0.0004803, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 356, Train Loss: 0.0004272, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 357, Train Loss: 0.0003971, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 358, Train Loss: 0.0003901, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 359, Train Loss: 0.0003827, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 360, Train Loss: 0.0003960, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 361, Train Loss: 0.0003924, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 362, Train Loss: 0.0003915, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 363, Train Loss: 0.0003939, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 364, Train Loss: 0.0003717, Train Acc: 1.0000000, Test Acc: 0.7500000\n",
            "Epoch: 365, Train Loss: 0.0004033, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 366, Train Loss: 0.0003711, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 367, Train Loss: 0.0003901, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 368, Train Loss: 0.0003782, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 369, Train Loss: 0.0003811, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 370, Train Loss: 0.0003731, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 371, Train Loss: 0.0003855, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 372, Train Loss: 0.0003859, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 373, Train Loss: 0.0003737, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 374, Train Loss: 0.0003719, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 375, Train Loss: 0.0003787, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 376, Train Loss: 0.0003959, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 377, Train Loss: 0.0003726, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 378, Train Loss: 0.0003627, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 379, Train Loss: 0.0003728, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 380, Train Loss: 0.0003845, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 381, Train Loss: 0.0003883, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 382, Train Loss: 0.0003709, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 383, Train Loss: 0.0003722, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 384, Train Loss: 0.0003908, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 385, Train Loss: 0.0003701, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 386, Train Loss: 0.0004164, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 387, Train Loss: 0.0003925, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 388, Train Loss: 0.0003756, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 389, Train Loss: 0.0003981, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 390, Train Loss: 0.0003887, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 391, Train Loss: 0.0003880, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 392, Train Loss: 0.0003833, Train Acc: 1.0000000, Test Acc: 0.7500000\n",
            "Epoch: 393, Train Loss: 0.0003677, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 394, Train Loss: 0.0003680, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 395, Train Loss: 0.0004104, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 396, Train Loss: 0.0004139, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 397, Train Loss: 0.0004549, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 398, Train Loss: 0.0004210, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 399, Train Loss: 0.0004157, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 400, Train Loss: 0.0003756, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 401, Train Loss: 0.0003669, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 402, Train Loss: 0.0003496, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 403, Train Loss: 0.0003707, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 404, Train Loss: 0.0003669, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 405, Train Loss: 0.0003622, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 406, Train Loss: 0.0004343, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 407, Train Loss: 0.0004224, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 408, Train Loss: 0.0003986, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 409, Train Loss: 0.0003743, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 410, Train Loss: 0.0003635, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 411, Train Loss: 0.0003557, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 412, Train Loss: 0.0004175, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 413, Train Loss: 0.0003855, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 414, Train Loss: 0.0003661, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 415, Train Loss: 0.0003518, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 416, Train Loss: 0.0003361, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 417, Train Loss: 0.0003626, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 418, Train Loss: 0.0003504, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 419, Train Loss: 0.0003697, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 420, Train Loss: 0.0003496, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 421, Train Loss: 0.0003423, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 422, Train Loss: 0.0003635, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 423, Train Loss: 0.0003555, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 424, Train Loss: 0.0003470, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 425, Train Loss: 0.0003460, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 426, Train Loss: 0.0004290, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 427, Train Loss: 0.0003768, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 428, Train Loss: 0.0003562, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 429, Train Loss: 0.0003640, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 430, Train Loss: 0.0003685, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 431, Train Loss: 0.0004408, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 432, Train Loss: 0.0003787, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 433, Train Loss: 0.0003663, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 434, Train Loss: 0.0003880, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 435, Train Loss: 0.0003494, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 436, Train Loss: 0.0003345, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 437, Train Loss: 0.0003287, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 438, Train Loss: 0.0003310, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 439, Train Loss: 0.0003472, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 440, Train Loss: 0.0003484, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 441, Train Loss: 0.0003370, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 442, Train Loss: 0.0003513, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 443, Train Loss: 0.0003642, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 444, Train Loss: 0.0003488, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 445, Train Loss: 0.0003504, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 446, Train Loss: 0.0003497, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 447, Train Loss: 0.0003546, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 448, Train Loss: 0.0003647, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 449, Train Loss: 0.0003411, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 450, Train Loss: 0.0003465, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 451, Train Loss: 0.0003480, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 452, Train Loss: 0.0003875, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 453, Train Loss: 0.0003618, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 454, Train Loss: 0.0003508, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 455, Train Loss: 0.0003685, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 456, Train Loss: 0.0004219, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 457, Train Loss: 0.0003612, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 458, Train Loss: 0.0003502, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 459, Train Loss: 0.0003292, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 460, Train Loss: 0.0003438, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 461, Train Loss: 0.0003459, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 462, Train Loss: 0.0003509, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 463, Train Loss: 0.0003630, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 464, Train Loss: 0.0004128, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 465, Train Loss: 0.0003845, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 466, Train Loss: 0.0003715, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 467, Train Loss: 0.0003745, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 468, Train Loss: 0.0003392, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 469, Train Loss: 0.0003348, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 470, Train Loss: 0.0003237, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 471, Train Loss: 0.0003413, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 472, Train Loss: 0.0003550, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 473, Train Loss: 0.0003434, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 474, Train Loss: 0.0003861, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 475, Train Loss: 0.0003473, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 476, Train Loss: 0.0003758, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 477, Train Loss: 0.0003506, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 478, Train Loss: 0.0003725, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 479, Train Loss: 0.0003626, Train Acc: 1.0000000, Test Acc: 0.7500000\n",
            "Epoch: 480, Train Loss: 0.0003569, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 481, Train Loss: 0.0003848, Train Acc: 1.0000000, Test Acc: 0.7500000\n",
            "Epoch: 482, Train Loss: 0.0003658, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 483, Train Loss: 0.0003332, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 484, Train Loss: 0.0003481, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 485, Train Loss: 0.0003351, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 486, Train Loss: 0.0003466, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 487, Train Loss: 0.0003584, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 488, Train Loss: 0.0003497, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 489, Train Loss: 0.0003432, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 490, Train Loss: 0.0003354, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 491, Train Loss: 0.0003257, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 492, Train Loss: 0.0003350, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 493, Train Loss: 0.0003310, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 494, Train Loss: 0.0003421, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 495, Train Loss: 0.0003689, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 496, Train Loss: 0.0003371, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 497, Train Loss: 0.0003371, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 498, Train Loss: 0.0003262, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 499, Train Loss: 0.0003286, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "Epoch: 500, Train Loss: 0.0003275, Train Acc: 1.0000000, Test Acc: 0.7333333\n",
            "--------\n",
            "Best Test Acc:   0.7222222222222222_0.08050764858994133, Epoch: 293\n",
            "Best Train Loss: 0.0003723303246174828_6.0735442054715844e-05\n",
            "Best Train Acc:  1.0_0.0\n",
            "FOLD9, ENZYMES, 1668698769/_rhoE0.5P18I1_GELU_X_6_256_0_0.001_50_0.6_B64S0W8, BT=0.7222222222222222_0.08050764858994133, BE=293, ID=\n",
            "train_set : test_set = %d : %d 540 60\n",
            "-------- FOLD9 DATASET=ENZYMES, COMMIT_ID=\n",
            "#Params: 1334534\n",
            "Epoch: 001, Train Loss: 9.6485396, Train Acc: 0.2185185, Test Acc: 0.2000000\n",
            "Epoch: 002, Train Loss: 3.3839913, Train Acc: 0.2185185, Test Acc: 0.1666667\n",
            "Epoch: 003, Train Loss: 1.9425715, Train Acc: 0.2537037, Test Acc: 0.1666667\n",
            "Epoch: 004, Train Loss: 1.7298950, Train Acc: 0.3407407, Test Acc: 0.2166667\n",
            "Epoch: 005, Train Loss: 1.5516145, Train Acc: 0.3685185, Test Acc: 0.3000000\n",
            "Epoch: 006, Train Loss: 1.4737944, Train Acc: 0.4074074, Test Acc: 0.3166667\n",
            "Epoch: 007, Train Loss: 1.4487199, Train Acc: 0.4203704, Test Acc: 0.3333333\n",
            "Epoch: 008, Train Loss: 1.4548557, Train Acc: 0.4277778, Test Acc: 0.3000000\n",
            "Epoch: 009, Train Loss: 1.3961900, Train Acc: 0.4296296, Test Acc: 0.3500000\n",
            "Epoch: 010, Train Loss: 1.4723450, Train Acc: 0.4648148, Test Acc: 0.3833333\n",
            "Epoch: 011, Train Loss: 1.3203337, Train Acc: 0.5000000, Test Acc: 0.4333333\n",
            "Epoch: 012, Train Loss: 1.2772219, Train Acc: 0.5481481, Test Acc: 0.4333333\n",
            "Epoch: 013, Train Loss: 1.2974588, Train Acc: 0.4777778, Test Acc: 0.3166667\n",
            "Epoch: 014, Train Loss: 1.4123102, Train Acc: 0.4685185, Test Acc: 0.3333333\n",
            "Epoch: 015, Train Loss: 1.4152468, Train Acc: 0.4444444, Test Acc: 0.3833333\n",
            "Epoch: 016, Train Loss: 1.1839472, Train Acc: 0.5685185, Test Acc: 0.4500000\n",
            "Epoch: 017, Train Loss: 1.2731919, Train Acc: 0.5425926, Test Acc: 0.3666667\n",
            "Epoch: 018, Train Loss: 1.2050023, Train Acc: 0.5148148, Test Acc: 0.3500000\n",
            "Epoch: 019, Train Loss: 1.1936029, Train Acc: 0.5629630, Test Acc: 0.3666667\n",
            "Epoch: 020, Train Loss: 1.0021739, Train Acc: 0.6555556, Test Acc: 0.4166667\n",
            "Epoch: 021, Train Loss: 0.9793687, Train Acc: 0.6370370, Test Acc: 0.4333333\n",
            "Epoch: 022, Train Loss: 1.1775070, Train Acc: 0.5611111, Test Acc: 0.3333333\n",
            "Epoch: 023, Train Loss: 1.3826620, Train Acc: 0.5814815, Test Acc: 0.3500000\n",
            "Epoch: 024, Train Loss: 1.1016986, Train Acc: 0.6129630, Test Acc: 0.3833333\n",
            "Epoch: 025, Train Loss: 0.7989702, Train Acc: 0.7796296, Test Acc: 0.4666667\n",
            "Epoch: 026, Train Loss: 0.9485994, Train Acc: 0.6611111, Test Acc: 0.4666667\n",
            "Epoch: 027, Train Loss: 0.7835319, Train Acc: 0.7277778, Test Acc: 0.4666667\n",
            "Epoch: 028, Train Loss: 0.7686619, Train Acc: 0.7444444, Test Acc: 0.4166667\n",
            "Epoch: 029, Train Loss: 0.7149907, Train Acc: 0.7574074, Test Acc: 0.5500000\n",
            "Epoch: 030, Train Loss: 0.5615550, Train Acc: 0.8555556, Test Acc: 0.5333333\n",
            "Epoch: 031, Train Loss: 0.6947660, Train Acc: 0.7277778, Test Acc: 0.4500000\n",
            "Epoch: 032, Train Loss: 0.6258380, Train Acc: 0.7833333, Test Acc: 0.5833333\n",
            "Epoch: 033, Train Loss: 0.6483384, Train Acc: 0.7703704, Test Acc: 0.5166667\n",
            "Epoch: 034, Train Loss: 0.4367917, Train Acc: 0.8981481, Test Acc: 0.5833333\n",
            "Epoch: 035, Train Loss: 0.4495663, Train Acc: 0.8722222, Test Acc: 0.5500000\n",
            "Epoch: 036, Train Loss: 0.4342353, Train Acc: 0.8759259, Test Acc: 0.5666667\n",
            "Epoch: 037, Train Loss: 0.5514752, Train Acc: 0.7925926, Test Acc: 0.4833333\n",
            "Epoch: 038, Train Loss: 0.4922806, Train Acc: 0.8166667, Test Acc: 0.5000000\n",
            "Epoch: 039, Train Loss: 0.4390905, Train Acc: 0.8518519, Test Acc: 0.5333333\n",
            "Epoch: 040, Train Loss: 0.5162522, Train Acc: 0.7981481, Test Acc: 0.5500000\n",
            "Epoch: 041, Train Loss: 0.2866013, Train Acc: 0.9481481, Test Acc: 0.6333333\n",
            "Epoch: 042, Train Loss: 0.3076066, Train Acc: 0.9222222, Test Acc: 0.6166667\n",
            "Epoch: 043, Train Loss: 0.1947607, Train Acc: 0.9703704, Test Acc: 0.6500000\n",
            "Epoch: 044, Train Loss: 0.2111355, Train Acc: 0.9500000, Test Acc: 0.5833333\n",
            "Epoch: 045, Train Loss: 0.2295404, Train Acc: 0.9500000, Test Acc: 0.5666667\n",
            "Epoch: 046, Train Loss: 0.2405382, Train Acc: 0.9444444, Test Acc: 0.6166667\n",
            "Epoch: 047, Train Loss: 0.1510449, Train Acc: 0.9796296, Test Acc: 0.6000000\n",
            "Epoch: 048, Train Loss: 0.1186946, Train Acc: 0.9888889, Test Acc: 0.6666667\n",
            "Epoch: 049, Train Loss: 0.1841891, Train Acc: 0.9629630, Test Acc: 0.5666667\n",
            "Epoch: 050, Train Loss: 0.1189369, Train Acc: 0.9796296, Test Acc: 0.6166667\n",
            "Epoch: 051, Train Loss: 0.1037198, Train Acc: 0.9888889, Test Acc: 0.6500000\n",
            "Epoch: 052, Train Loss: 0.0803287, Train Acc: 0.9944444, Test Acc: 0.6000000\n",
            "Epoch: 053, Train Loss: 0.1130694, Train Acc: 0.9759259, Test Acc: 0.6666667\n",
            "Epoch: 054, Train Loss: 0.0673918, Train Acc: 0.9981481, Test Acc: 0.6333333\n",
            "Epoch: 055, Train Loss: 0.1379401, Train Acc: 0.9648148, Test Acc: 0.5333333\n",
            "Epoch: 056, Train Loss: 0.0847129, Train Acc: 0.9870370, Test Acc: 0.6333333\n",
            "Epoch: 057, Train Loss: 0.0337093, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 058, Train Loss: 0.0480917, Train Acc: 1.0000000, Test Acc: 0.6000000\n",
            "Epoch: 059, Train Loss: 0.0614769, Train Acc: 0.9944444, Test Acc: 0.6166667\n",
            "Epoch: 060, Train Loss: 0.1422325, Train Acc: 0.9537037, Test Acc: 0.5666667\n",
            "Epoch: 061, Train Loss: 0.0483320, Train Acc: 0.9962963, Test Acc: 0.6500000\n",
            "Epoch: 062, Train Loss: 0.0317433, Train Acc: 1.0000000, Test Acc: 0.6000000\n",
            "Epoch: 063, Train Loss: 0.0186077, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 064, Train Loss: 0.0150693, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 065, Train Loss: 0.0152731, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 066, Train Loss: 0.0129876, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 067, Train Loss: 0.0135761, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 068, Train Loss: 0.0113804, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 069, Train Loss: 0.0101125, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 070, Train Loss: 0.0097397, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 071, Train Loss: 0.0088934, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 072, Train Loss: 0.0086859, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 073, Train Loss: 0.0081931, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 074, Train Loss: 0.0074943, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 075, Train Loss: 0.0071360, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 076, Train Loss: 0.0068023, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 077, Train Loss: 0.0067756, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 078, Train Loss: 0.0069923, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 079, Train Loss: 0.0070449, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 080, Train Loss: 0.0091276, Train Acc: 1.0000000, Test Acc: 0.6333333\n",
            "Epoch: 081, Train Loss: 0.0066061, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 082, Train Loss: 0.0061537, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 083, Train Loss: 0.0066516, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 084, Train Loss: 0.0078098, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 085, Train Loss: 0.0084770, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 086, Train Loss: 0.0057101, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 087, Train Loss: 0.0048196, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 088, Train Loss: 0.0045036, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 089, Train Loss: 0.0043583, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 090, Train Loss: 0.0042030, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 091, Train Loss: 0.0039298, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 092, Train Loss: 0.0041127, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 093, Train Loss: 0.0045248, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 094, Train Loss: 0.0050583, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 095, Train Loss: 0.0052510, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 096, Train Loss: 0.0060259, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 097, Train Loss: 0.0053876, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 098, Train Loss: 0.0091046, Train Acc: 1.0000000, Test Acc: 0.6500000\n",
            "Epoch: 099, Train Loss: 0.0067332, Train Acc: 1.0000000, Test Acc: 0.6333333\n",
            "Epoch: 100, Train Loss: 0.0041946, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 101, Train Loss: 0.0041228, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 102, Train Loss: 0.0034766, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 103, Train Loss: 0.0033093, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 104, Train Loss: 0.0030984, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 105, Train Loss: 0.0029066, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 106, Train Loss: 0.0028525, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 107, Train Loss: 0.0028326, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 108, Train Loss: 0.0026074, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 109, Train Loss: 0.0026335, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 110, Train Loss: 0.0024728, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 111, Train Loss: 0.0026789, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 112, Train Loss: 0.0025167, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 113, Train Loss: 0.0024387, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 114, Train Loss: 0.0027761, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 115, Train Loss: 0.0026598, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 116, Train Loss: 0.0024346, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 117, Train Loss: 0.0022756, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 118, Train Loss: 0.0021553, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 119, Train Loss: 0.0021428, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 120, Train Loss: 0.0023366, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 121, Train Loss: 0.0022069, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 122, Train Loss: 0.0020822, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 123, Train Loss: 0.0020521, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 124, Train Loss: 0.0019656, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 125, Train Loss: 0.0019628, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 126, Train Loss: 0.0019917, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 127, Train Loss: 0.0019511, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 128, Train Loss: 0.0019346, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 129, Train Loss: 0.0024725, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 130, Train Loss: 0.0018445, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 131, Train Loss: 0.0019419, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 132, Train Loss: 0.0018363, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 133, Train Loss: 0.0048852, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 134, Train Loss: 0.0022934, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 135, Train Loss: 0.0021381, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 136, Train Loss: 0.0017979, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 137, Train Loss: 0.0017504, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 138, Train Loss: 0.0017734, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 139, Train Loss: 0.0015863, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 140, Train Loss: 0.0015243, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 141, Train Loss: 0.0017254, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 142, Train Loss: 0.0016341, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 143, Train Loss: 0.0019729, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 144, Train Loss: 0.0016737, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 145, Train Loss: 0.0015700, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 146, Train Loss: 0.0015235, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 147, Train Loss: 0.0016295, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 148, Train Loss: 0.0014987, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 149, Train Loss: 0.0014198, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 150, Train Loss: 0.0014564, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 151, Train Loss: 0.0013603, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 152, Train Loss: 0.0013356, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 153, Train Loss: 0.0012766, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 154, Train Loss: 0.0013619, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 155, Train Loss: 0.0012131, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 156, Train Loss: 0.0011934, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 157, Train Loss: 0.0011976, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 158, Train Loss: 0.0012498, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 159, Train Loss: 0.0012008, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 160, Train Loss: 0.0011463, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 161, Train Loss: 0.0012408, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 162, Train Loss: 0.0014960, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 163, Train Loss: 0.0012649, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 164, Train Loss: 0.0012717, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 165, Train Loss: 0.0011973, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 166, Train Loss: 0.0010924, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 167, Train Loss: 0.0010854, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 168, Train Loss: 0.0010744, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 169, Train Loss: 0.0011225, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 170, Train Loss: 0.0010773, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 171, Train Loss: 0.0011179, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 172, Train Loss: 0.0010756, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 173, Train Loss: 0.0011342, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 174, Train Loss: 0.0011200, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 175, Train Loss: 0.0010598, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 176, Train Loss: 0.0010600, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 177, Train Loss: 0.0010569, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 178, Train Loss: 0.0010629, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 179, Train Loss: 0.0010179, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 180, Train Loss: 0.0009438, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 181, Train Loss: 0.0009431, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 182, Train Loss: 0.0010092, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 183, Train Loss: 0.0010781, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 184, Train Loss: 0.0010053, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 185, Train Loss: 0.0009221, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 186, Train Loss: 0.0009203, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 187, Train Loss: 0.0009405, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 188, Train Loss: 0.0009140, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 189, Train Loss: 0.0009159, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 190, Train Loss: 0.0009455, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 191, Train Loss: 0.0009018, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 192, Train Loss: 0.0008992, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 193, Train Loss: 0.0009228, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 194, Train Loss: 0.0009332, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 195, Train Loss: 0.0008568, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 196, Train Loss: 0.0008763, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 197, Train Loss: 0.0008651, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 198, Train Loss: 0.0008576, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 199, Train Loss: 0.0008605, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 200, Train Loss: 0.0009010, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 201, Train Loss: 0.0008569, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 202, Train Loss: 0.0008380, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 203, Train Loss: 0.0008403, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 204, Train Loss: 0.0008605, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 205, Train Loss: 0.0008256, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 206, Train Loss: 0.0008391, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 207, Train Loss: 0.0008671, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 208, Train Loss: 0.0008807, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 209, Train Loss: 0.0008585, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 210, Train Loss: 0.0007828, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 211, Train Loss: 0.0007931, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 212, Train Loss: 0.0007797, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 213, Train Loss: 0.0008080, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 214, Train Loss: 0.0009022, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 215, Train Loss: 0.0008931, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 216, Train Loss: 0.0008378, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 217, Train Loss: 0.0008021, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 218, Train Loss: 0.0008375, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 219, Train Loss: 0.0007604, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 220, Train Loss: 0.0007663, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 221, Train Loss: 0.0007667, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 222, Train Loss: 0.0007659, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 223, Train Loss: 0.0007299, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 224, Train Loss: 0.0007445, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 225, Train Loss: 0.0008623, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 226, Train Loss: 0.0022627, Train Acc: 1.0000000, Test Acc: 0.6500000\n",
            "Epoch: 227, Train Loss: 0.0011074, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 228, Train Loss: 0.0010510, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 229, Train Loss: 0.0008667, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 230, Train Loss: 0.0008320, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 231, Train Loss: 0.0008287, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 232, Train Loss: 0.0007947, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 233, Train Loss: 0.0007651, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 234, Train Loss: 0.0007360, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 235, Train Loss: 0.0008154, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 236, Train Loss: 0.0007541, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 237, Train Loss: 0.0007160, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 238, Train Loss: 0.0007486, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 239, Train Loss: 0.0007331, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 240, Train Loss: 0.0007416, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 241, Train Loss: 0.0007844, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 242, Train Loss: 0.0007541, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 243, Train Loss: 0.0007339, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 244, Train Loss: 0.0007336, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 245, Train Loss: 0.0007971, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 246, Train Loss: 0.0007250, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 247, Train Loss: 0.0006635, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 248, Train Loss: 0.0006955, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 249, Train Loss: 0.0006757, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 250, Train Loss: 0.0006930, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 251, Train Loss: 0.0006696, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 252, Train Loss: 0.0007896, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 253, Train Loss: 0.0006716, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 254, Train Loss: 0.0006612, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 255, Train Loss: 0.0006980, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 256, Train Loss: 0.0006559, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 257, Train Loss: 0.0006429, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 258, Train Loss: 0.0006373, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 259, Train Loss: 0.0006157, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 260, Train Loss: 0.0006090, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 261, Train Loss: 0.0006139, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 262, Train Loss: 0.0006160, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 263, Train Loss: 0.0006072, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 264, Train Loss: 0.0006006, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 265, Train Loss: 0.0006084, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 266, Train Loss: 0.0005998, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 267, Train Loss: 0.0006146, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 268, Train Loss: 0.0005969, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 269, Train Loss: 0.0005748, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 270, Train Loss: 0.0005798, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 271, Train Loss: 0.0006070, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 272, Train Loss: 0.0005783, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 273, Train Loss: 0.0005840, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 274, Train Loss: 0.0005844, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 275, Train Loss: 0.0006399, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 276, Train Loss: 0.0006106, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 277, Train Loss: 0.0005897, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 278, Train Loss: 0.0005897, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 279, Train Loss: 0.0005936, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 280, Train Loss: 0.0005746, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 281, Train Loss: 0.0006228, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 282, Train Loss: 0.0005978, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 283, Train Loss: 0.0005792, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 284, Train Loss: 0.0005886, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 285, Train Loss: 0.0005620, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 286, Train Loss: 0.0005600, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 287, Train Loss: 0.0005676, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 288, Train Loss: 0.0005824, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 289, Train Loss: 0.0005774, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 290, Train Loss: 0.0005850, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 291, Train Loss: 0.0005650, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 292, Train Loss: 0.0005536, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 293, Train Loss: 0.0005705, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 294, Train Loss: 0.0005414, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 295, Train Loss: 0.0005529, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 296, Train Loss: 0.0005400, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 297, Train Loss: 0.0005531, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 298, Train Loss: 0.0005341, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 299, Train Loss: 0.0005567, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 300, Train Loss: 0.0005678, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 301, Train Loss: 0.0005392, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 302, Train Loss: 0.0005414, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 303, Train Loss: 0.0005379, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 304, Train Loss: 0.0005287, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 305, Train Loss: 0.0005319, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 306, Train Loss: 0.0005321, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 307, Train Loss: 0.0005316, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 308, Train Loss: 0.0005234, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 309, Train Loss: 0.0005344, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 310, Train Loss: 0.0005549, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 311, Train Loss: 0.0005808, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 312, Train Loss: 0.0005536, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 313, Train Loss: 0.0005579, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 314, Train Loss: 0.0005439, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 315, Train Loss: 0.0005087, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 316, Train Loss: 0.0005235, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 317, Train Loss: 0.0005250, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 318, Train Loss: 0.0005385, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 319, Train Loss: 0.0005232, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 320, Train Loss: 0.0005144, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 321, Train Loss: 0.0004898, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 322, Train Loss: 0.0005259, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 323, Train Loss: 0.0005076, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 324, Train Loss: 0.0004895, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 325, Train Loss: 0.0005031, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 326, Train Loss: 0.0005406, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 327, Train Loss: 0.0005180, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 328, Train Loss: 0.0005140, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 329, Train Loss: 0.0004821, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 330, Train Loss: 0.0004880, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 331, Train Loss: 0.0004976, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 332, Train Loss: 0.0005238, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 333, Train Loss: 0.0005190, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 334, Train Loss: 0.0004941, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 335, Train Loss: 0.0005069, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 336, Train Loss: 0.0004792, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 337, Train Loss: 0.0004818, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 338, Train Loss: 0.0004912, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 339, Train Loss: 0.0004940, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 340, Train Loss: 0.0004967, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 341, Train Loss: 0.0004834, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 342, Train Loss: 0.0004986, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 343, Train Loss: 0.0004927, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 344, Train Loss: 0.0004971, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 345, Train Loss: 0.0004930, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 346, Train Loss: 0.0004884, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 347, Train Loss: 0.0004970, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 348, Train Loss: 0.0005195, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 349, Train Loss: 0.0005139, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 350, Train Loss: 0.0004794, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 351, Train Loss: 0.0004841, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 352, Train Loss: 0.0005716, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 353, Train Loss: 0.0005306, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 354, Train Loss: 0.0004981, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 355, Train Loss: 0.0005239, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 356, Train Loss: 0.0005156, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 357, Train Loss: 0.0005139, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 358, Train Loss: 0.0004791, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 359, Train Loss: 0.0004902, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 360, Train Loss: 0.0004680, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 361, Train Loss: 0.0004574, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 362, Train Loss: 0.0004736, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 363, Train Loss: 0.0004523, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 364, Train Loss: 0.0004658, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 365, Train Loss: 0.0004845, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 366, Train Loss: 0.0004951, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 367, Train Loss: 0.0004856, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 368, Train Loss: 0.0004924, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 369, Train Loss: 0.0004742, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 370, Train Loss: 0.0004836, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 371, Train Loss: 0.0004868, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 372, Train Loss: 0.0005052, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 373, Train Loss: 0.0004715, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 374, Train Loss: 0.0004627, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 375, Train Loss: 0.0004508, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 376, Train Loss: 0.0004585, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 377, Train Loss: 0.0004566, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 378, Train Loss: 0.0004572, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 379, Train Loss: 0.0004543, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 380, Train Loss: 0.0004669, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 381, Train Loss: 0.0004542, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 382, Train Loss: 0.0004953, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 383, Train Loss: 0.0005049, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 384, Train Loss: 0.0004872, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 385, Train Loss: 0.0004623, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 386, Train Loss: 0.0004583, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 387, Train Loss: 0.0004453, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 388, Train Loss: 0.0004458, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 389, Train Loss: 0.0004726, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 390, Train Loss: 0.0004710, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 391, Train Loss: 0.0004671, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 392, Train Loss: 0.0004539, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 393, Train Loss: 0.0004438, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 394, Train Loss: 0.0004481, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 395, Train Loss: 0.0004654, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 396, Train Loss: 0.0004334, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 397, Train Loss: 0.0004589, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 398, Train Loss: 0.0004604, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 399, Train Loss: 0.0004454, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 400, Train Loss: 0.0004471, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 401, Train Loss: 0.0004475, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 402, Train Loss: 0.0004431, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 403, Train Loss: 0.0004407, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 404, Train Loss: 0.0004521, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 405, Train Loss: 0.0004469, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 406, Train Loss: 0.0004374, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 407, Train Loss: 0.0005051, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 408, Train Loss: 0.0004609, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 409, Train Loss: 0.0004707, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 410, Train Loss: 0.0004527, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 411, Train Loss: 0.0004697, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 412, Train Loss: 0.0004549, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 413, Train Loss: 0.0004484, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 414, Train Loss: 0.0004318, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 415, Train Loss: 0.0004230, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 416, Train Loss: 0.0004447, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 417, Train Loss: 0.0004488, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 418, Train Loss: 0.0004618, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 419, Train Loss: 0.0004473, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 420, Train Loss: 0.0004363, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 421, Train Loss: 0.0004341, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 422, Train Loss: 0.0004883, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 423, Train Loss: 0.0004644, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 424, Train Loss: 0.0004428, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 425, Train Loss: 0.0004336, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 426, Train Loss: 0.0004383, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 427, Train Loss: 0.0004350, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 428, Train Loss: 0.0004237, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 429, Train Loss: 0.0004423, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 430, Train Loss: 0.0004468, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 431, Train Loss: 0.0004268, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 432, Train Loss: 0.0004218, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 433, Train Loss: 0.0004207, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 434, Train Loss: 0.0004789, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 435, Train Loss: 0.0004458, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 436, Train Loss: 0.0004202, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 437, Train Loss: 0.0004268, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 438, Train Loss: 0.0004390, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 439, Train Loss: 0.0004202, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 440, Train Loss: 0.0004235, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 441, Train Loss: 0.0004273, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 442, Train Loss: 0.0004541, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 443, Train Loss: 0.0004574, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 444, Train Loss: 0.0004504, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 445, Train Loss: 0.0004621, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 446, Train Loss: 0.0004358, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 447, Train Loss: 0.0004364, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 448, Train Loss: 0.0004590, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 449, Train Loss: 0.0004169, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 450, Train Loss: 0.0004143, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 451, Train Loss: 0.0004180, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 452, Train Loss: 0.0004333, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 453, Train Loss: 0.0004466, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 454, Train Loss: 0.0004242, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 455, Train Loss: 0.0004149, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 456, Train Loss: 0.0004203, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 457, Train Loss: 0.0004177, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 458, Train Loss: 0.0004270, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 459, Train Loss: 0.0004286, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 460, Train Loss: 0.0004126, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 461, Train Loss: 0.0004185, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 462, Train Loss: 0.0004157, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 463, Train Loss: 0.0004043, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 464, Train Loss: 0.0004137, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 465, Train Loss: 0.0004167, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 466, Train Loss: 0.0004205, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 467, Train Loss: 0.0004340, Train Acc: 1.0000000, Test Acc: 0.6666667\n",
            "Epoch: 468, Train Loss: 0.0004581, Train Acc: 1.0000000, Test Acc: 0.6833333\n",
            "Epoch: 469, Train Loss: 0.0004258, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 470, Train Loss: 0.0004274, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 471, Train Loss: 0.0004149, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 472, Train Loss: 0.0004907, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 473, Train Loss: 0.0004301, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 474, Train Loss: 0.0004360, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 475, Train Loss: 0.0004230, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 476, Train Loss: 0.0004258, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 477, Train Loss: 0.0004158, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 478, Train Loss: 0.0004035, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 479, Train Loss: 0.0003952, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 480, Train Loss: 0.0003960, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 481, Train Loss: 0.0003902, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 482, Train Loss: 0.0004156, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 483, Train Loss: 0.0003977, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 484, Train Loss: 0.0004089, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 485, Train Loss: 0.0003863, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 486, Train Loss: 0.0003980, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 487, Train Loss: 0.0003903, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 488, Train Loss: 0.0003854, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 489, Train Loss: 0.0003854, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 490, Train Loss: 0.0003886, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 491, Train Loss: 0.0004023, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "Epoch: 492, Train Loss: 0.0003740, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 493, Train Loss: 0.0003873, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 494, Train Loss: 0.0004030, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 495, Train Loss: 0.0003969, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 496, Train Loss: 0.0004292, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 497, Train Loss: 0.0004201, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 498, Train Loss: 0.0003933, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 499, Train Loss: 0.0003985, Train Acc: 1.0000000, Test Acc: 0.7166667\n",
            "Epoch: 500, Train Loss: 0.0004079, Train Acc: 1.0000000, Test Acc: 0.7000000\n",
            "--------\n",
            "Best Test Acc:   0.72_0.07666666666666667, Epoch: 293\n",
            "Best Train Loss: 0.0003747012845727753_5.805607280388484e-05\n",
            "Best Train Acc:  1.0_0.0\n",
            "FOLD10, ENZYMES, 1668698769/_rhoE0.5P18I1_GELU_X_6_256_0_0.001_50_0.6_B64S0W8, BT=0.72_0.07666666666666667, BE=293, ID=\n"
          ]
        }
      ],
      "source": [
        "!python -u ./main.py --config=\"./configs/ENZYMES.json\""
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "k6oQHt_s1kde"
      },
      "source": [
        "### Mutagenicity (Norm-GN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "RPa4h8zUkmE8",
        "outputId": "ef806c84-1a26-40f3-800e-098f355541f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DGL backend not selected or invalid.  Assuming PyTorch for now.\n",
            "Setting the default backend to \"pytorch\". You can change it in the ~/.dgl/config.json file or export the DGLBACKEND environment variable.  Valid options are: pytorch, mxnet, tensorflow (all lowercase)\n",
            "{'dataset_name': 'Mutagenicity', 'basis': 'rho', 'epsilon': 0.5, 'power': 18, 'seeds': [0], 'num_folds': 10, 'num_workers': 8, 'hyperparams': {'batch_size': 64, 'epochs': 121, 'learning_rate': 0.001, 'step_size': 50, 'decay_rate': 0.6}, 'architecture': {'nonlinear': 'GELU', 'layers': 6, 'hidden': 256, 'pooling': 'X', 'dropout': 0}, 'commit_id': '', 'time_stamp': '', 'directory': ''}\n",
            "{'dataset_name': 'Mutagenicity', 'basis': 'rho', 'epsilon': 0.5, 'power': 18, 'seeds': [0], 'num_folds': 10, 'num_workers': 8, 'hyperparams': {'batch_size': 64, 'epochs': 121, 'learning_rate': 0.001, 'step_size': 50, 'decay_rate': 0.6}, 'architecture': {'nonlinear': 'GELU', 'layers': 6, 'hidden': 256, 'pooling': 'X', 'dropout': 0}, 'commit_id': '', 'time_stamp': 1668873489, 'directory': '', 'seed': 0}\n",
            "2022-11-19 15:58:10.098981: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "Downloading ./dataset/Mutagenicity.zip from https://www.chrsmrrs.com/graphkerneldatasets/Mutagenicity.zip...\n",
            "Extracting file to ./dataset/Mutagenicity\n",
            "No Node Attribute Data\n",
            "Pre-process: 100% 4337/4337 [00:21<00:00, 206.47it/s]\n",
            "Basis total: 19\n",
            "train_set : test_set = %d : %d 3903 434\n",
            "-------- FOLD0 DATASET=Mutagenicity, COMMIT_ID=\n",
            "#Params: 1332482\n",
            "Epoch: 001, Train Loss: 0.5685781, Train Acc: 0.6966436, Test Acc: 0.6843318\n",
            "Epoch: 002, Train Loss: 0.5579856, Train Acc: 0.7109915, Test Acc: 0.7119816\n",
            "Epoch: 003, Train Loss: 0.5264892, Train Acc: 0.7327697, Test Acc: 0.7603687\n",
            "Epoch: 004, Train Loss: 0.5866803, Train Acc: 0.7294389, Test Acc: 0.7050691\n",
            "Epoch: 005, Train Loss: 0.6316678, Train Acc: 0.6753779, Test Acc: 0.7004608\n",
            "Epoch: 006, Train Loss: 0.4628898, Train Acc: 0.7788880, Test Acc: 0.7580645\n",
            "Epoch: 007, Train Loss: 0.4544878, Train Acc: 0.7998975, Test Acc: 0.7903226\n",
            "Epoch: 008, Train Loss: 0.4369245, Train Acc: 0.8116833, Test Acc: 0.7972350\n",
            "Epoch: 009, Train Loss: 0.4385264, Train Acc: 0.8075839, Test Acc: 0.7834101\n",
            "Epoch: 010, Train Loss: 0.4194883, Train Acc: 0.8203946, Test Acc: 0.7995392\n",
            "Epoch: 011, Train Loss: 0.4038826, Train Acc: 0.8250064, Test Acc: 0.8087558\n",
            "Epoch: 012, Train Loss: 0.3902997, Train Acc: 0.8303869, Test Acc: 0.8248848\n",
            "Epoch: 013, Train Loss: 0.4081704, Train Acc: 0.8250064, Test Acc: 0.8064516\n",
            "Epoch: 014, Train Loss: 0.3730903, Train Acc: 0.8434538, Test Acc: 0.8271889\n",
            "Epoch: 015, Train Loss: 0.4691379, Train Acc: 0.7801691, Test Acc: 0.7695853\n",
            "Epoch: 016, Train Loss: 0.3538944, Train Acc: 0.8506277, Test Acc: 0.8433180\n",
            "Epoch: 017, Train Loss: 0.4086070, Train Acc: 0.8216756, Test Acc: 0.7972350\n",
            "Epoch: 018, Train Loss: 0.3990040, Train Acc: 0.8288496, Test Acc: 0.7903226\n",
            "Epoch: 019, Train Loss: 0.3303553, Train Acc: 0.8657443, Test Acc: 0.8248848\n",
            "Epoch: 020, Train Loss: 0.3502670, Train Acc: 0.8462721, Test Acc: 0.8248848\n",
            "Epoch: 021, Train Loss: 0.3642420, Train Acc: 0.8467845, Test Acc: 0.8248848\n",
            "Epoch: 022, Train Loss: 0.3092287, Train Acc: 0.8718934, Test Acc: 0.8317972\n",
            "Epoch: 023, Train Loss: 0.2966816, Train Acc: 0.8780425, Test Acc: 0.8364055\n",
            "Epoch: 024, Train Loss: 0.3103909, Train Acc: 0.8726621, Test Acc: 0.8179724\n",
            "Epoch: 025, Train Loss: 0.2800929, Train Acc: 0.8790674, Test Acc: 0.8202765\n",
            "Epoch: 026, Train Loss: 0.3231885, Train Acc: 0.8539585, Test Acc: 0.8041475\n",
            "Epoch: 027, Train Loss: 0.2752762, Train Acc: 0.8839354, Test Acc: 0.8410138\n",
            "Epoch: 028, Train Loss: 0.3107935, Train Acc: 0.8729183, Test Acc: 0.8041475\n",
            "Epoch: 029, Train Loss: 0.3412341, Train Acc: 0.8531899, Test Acc: 0.7811060\n",
            "Epoch: 030, Train Loss: 0.2949613, Train Acc: 0.8713810, Test Acc: 0.8225806\n",
            "Epoch: 031, Train Loss: 0.2403266, Train Acc: 0.9023828, Test Acc: 0.8294931\n",
            "Epoch: 032, Train Loss: 0.2580637, Train Acc: 0.8913656, Test Acc: 0.8364055\n",
            "Epoch: 033, Train Loss: 0.2500041, Train Acc: 0.8936715, Test Acc: 0.8387097\n",
            "Epoch: 034, Train Loss: 0.2172826, Train Acc: 0.9121189, Test Acc: 0.8341014\n",
            "Epoch: 035, Train Loss: 0.2401340, Train Acc: 0.9052011, Test Acc: 0.8317972\n",
            "Epoch: 036, Train Loss: 0.2191268, Train Acc: 0.9085319, Test Acc: 0.8041475\n",
            "Epoch: 037, Train Loss: 0.2031030, Train Acc: 0.9162183, Test Acc: 0.8387097\n",
            "Epoch: 038, Train Loss: 0.3346221, Train Acc: 0.8506277, Test Acc: 0.8133641\n",
            "Epoch: 039, Train Loss: 0.2717371, Train Acc: 0.8785550, Test Acc: 0.8041475\n",
            "Epoch: 040, Train Loss: 0.1833632, Train Acc: 0.9226236, Test Acc: 0.8341014\n",
            "Epoch: 041, Train Loss: 0.1666700, Train Acc: 0.9346656, Test Acc: 0.8456221\n",
            "Epoch: 042, Train Loss: 0.2134442, Train Acc: 0.9087881, Test Acc: 0.8110599\n",
            "Epoch: 043, Train Loss: 0.1795148, Train Acc: 0.9303100, Test Acc: 0.8433180\n",
            "Epoch: 044, Train Loss: 0.1749107, Train Acc: 0.9285165, Test Acc: 0.8179724\n",
            "Epoch: 045, Train Loss: 0.1815268, Train Acc: 0.9246733, Test Acc: 0.8271889\n",
            "Epoch: 046, Train Loss: 0.1841770, Train Acc: 0.9203177, Test Acc: 0.8271889\n",
            "Epoch: 047, Train Loss: 0.1544020, Train Acc: 0.9354343, Test Acc: 0.8202765\n",
            "Epoch: 048, Train Loss: 0.1779769, Train Acc: 0.9318473, Test Acc: 0.8294931\n",
            "Epoch: 049, Train Loss: 0.1121417, Train Acc: 0.9584935, Test Acc: 0.8456221\n",
            "Epoch: 050, Train Loss: 0.1536682, Train Acc: 0.9349219, Test Acc: 0.8317972\n",
            "Epoch: 051, Train Loss: 0.0775904, Train Acc: 0.9736100, Test Acc: 0.8433180\n",
            "Epoch: 052, Train Loss: 0.0539123, Train Acc: 0.9833461, Test Acc: 0.8433180\n",
            "Epoch: 053, Train Loss: 0.0828837, Train Acc: 0.9720728, Test Acc: 0.8433180\n",
            "Epoch: 054, Train Loss: 0.0462631, Train Acc: 0.9861645, Test Acc: 0.8225806\n",
            "Epoch: 055, Train Loss: 0.0652732, Train Acc: 0.9743787, Test Acc: 0.8525346\n",
            "Epoch: 056, Train Loss: 0.0526317, Train Acc: 0.9823213, Test Acc: 0.8341014\n",
            "Epoch: 057, Train Loss: 0.0664017, Train Acc: 0.9756597, Test Acc: 0.8317972\n",
            "Epoch: 058, Train Loss: 0.0623533, Train Acc: 0.9823213, Test Acc: 0.8387097\n",
            "Epoch: 059, Train Loss: 0.0777786, Train Acc: 0.9718166, Test Acc: 0.8387097\n",
            "Epoch: 060, Train Loss: 0.0466573, Train Acc: 0.9823213, Test Acc: 0.8456221\n",
            "Epoch: 061, Train Loss: 0.0731845, Train Acc: 0.9777095, Test Acc: 0.8294931\n",
            "Epoch: 062, Train Loss: 0.0471052, Train Acc: 0.9838586, Test Acc: 0.8502304\n",
            "Epoch: 063, Train Loss: 0.0453811, Train Acc: 0.9843710, Test Acc: 0.8317972\n",
            "Epoch: 064, Train Loss: 0.0716066, Train Acc: 0.9738663, Test Acc: 0.8525346\n",
            "Epoch: 065, Train Loss: 0.0473807, Train Acc: 0.9843710, Test Acc: 0.8387097\n",
            "Epoch: 066, Train Loss: 0.0278583, Train Acc: 0.9905201, Test Acc: 0.8456221\n",
            "Epoch: 067, Train Loss: 0.0411435, Train Acc: 0.9851396, Test Acc: 0.8364055\n",
            "Epoch: 068, Train Loss: 0.0596376, Train Acc: 0.9787343, Test Acc: 0.8341014\n",
            "Epoch: 069, Train Loss: 0.0406256, Train Acc: 0.9841148, Test Acc: 0.8456221\n",
            "Epoch: 070, Train Loss: 0.0330431, Train Acc: 0.9874456, Test Acc: 0.8410138\n",
            "Epoch: 071, Train Loss: 0.0413604, Train Acc: 0.9846272, Test Acc: 0.8456221\n",
            "Epoch: 072, Train Loss: 0.1428868, Train Acc: 0.9461952, Test Acc: 0.8087558\n",
            "Epoch: 073, Train Loss: 0.0432452, Train Acc: 0.9848834, Test Acc: 0.8341014\n",
            "Epoch: 074, Train Loss: 0.0259200, Train Acc: 0.9920574, Test Acc: 0.8248848\n",
            "Epoch: 075, Train Loss: 0.0440339, Train Acc: 0.9818089, Test Acc: 0.8317972\n",
            "Epoch: 076, Train Loss: 0.1867329, Train Acc: 0.9474763, Test Acc: 0.8248848\n",
            "Epoch: 077, Train Loss: 0.0383392, Train Acc: 0.9882142, Test Acc: 0.8202765\n",
            "Epoch: 078, Train Loss: 0.0341767, Train Acc: 0.9856521, Test Acc: 0.8387097\n",
            "Epoch: 079, Train Loss: 0.0260312, Train Acc: 0.9920574, Test Acc: 0.8156682\n",
            "Epoch: 080, Train Loss: 0.0345101, Train Acc: 0.9889828, Test Acc: 0.8064516\n",
            "Epoch: 081, Train Loss: 0.0309532, Train Acc: 0.9897515, Test Acc: 0.8179724\n",
            "Epoch: 082, Train Loss: 0.0674406, Train Acc: 0.9738663, Test Acc: 0.8202765\n",
            "Epoch: 083, Train Loss: 0.0354725, Train Acc: 0.9877018, Test Acc: 0.8248848\n",
            "Epoch: 084, Train Loss: 0.0382846, Train Acc: 0.9843710, Test Acc: 0.8156682\n",
            "Epoch: 085, Train Loss: 0.0289836, Train Acc: 0.9877018, Test Acc: 0.8225806\n",
            "Epoch: 086, Train Loss: 0.0503359, Train Acc: 0.9818089, Test Acc: 0.8202765\n",
            "Epoch: 087, Train Loss: 0.0338071, Train Acc: 0.9871893, Test Acc: 0.8179724\n",
            "Epoch: 088, Train Loss: 0.0295320, Train Acc: 0.9902639, Test Acc: 0.8225806\n",
            "Epoch: 089, Train Loss: 0.0221552, Train Acc: 0.9935947, Test Acc: 0.8133641\n",
            "Epoch: 090, Train Loss: 0.0750107, Train Acc: 0.9689982, Test Acc: 0.8110599\n",
            "Epoch: 091, Train Loss: 0.0203664, Train Acc: 0.9930822, Test Acc: 0.8271889\n",
            "Epoch: 092, Train Loss: 0.0218536, Train Acc: 0.9925698, Test Acc: 0.8133641\n",
            "Epoch: 093, Train Loss: 0.0149887, Train Acc: 0.9964130, Test Acc: 0.8387097\n",
            "Epoch: 094, Train Loss: 0.0403910, Train Acc: 0.9877018, Test Acc: 0.8133641\n",
            "Epoch: 095, Train Loss: 0.0127405, Train Acc: 0.9961568, Test Acc: 0.8271889\n",
            "Epoch: 096, Train Loss: 0.1238042, Train Acc: 0.9559313, Test Acc: 0.8110599\n",
            "Epoch: 097, Train Loss: 0.0341929, Train Acc: 0.9902639, Test Acc: 0.8364055\n",
            "Epoch: 098, Train Loss: 0.0205905, Train Acc: 0.9935947, Test Acc: 0.8387097\n",
            "Epoch: 099, Train Loss: 0.0164738, Train Acc: 0.9951319, Test Acc: 0.8433180\n",
            "Epoch: 100, Train Loss: 0.0256695, Train Acc: 0.9897515, Test Acc: 0.8225806\n",
            "Epoch: 101, Train Loss: 0.0064944, Train Acc: 0.9984627, Test Acc: 0.8271889\n",
            "Epoch: 102, Train Loss: 0.0035372, Train Acc: 0.9992314, Test Acc: 0.8294931\n",
            "Epoch: 103, Train Loss: 0.0043250, Train Acc: 0.9982065, Test Acc: 0.8341014\n",
            "Epoch: 104, Train Loss: 0.0045378, Train Acc: 0.9989751, Test Acc: 0.8294931\n",
            "Epoch: 105, Train Loss: 0.0046837, Train Acc: 0.9987189, Test Acc: 0.8502304\n",
            "Epoch: 106, Train Loss: 0.0028705, Train Acc: 0.9997438, Test Acc: 0.8387097\n",
            "Epoch: 107, Train Loss: 0.0088537, Train Acc: 0.9976941, Test Acc: 0.8410138\n",
            "Epoch: 108, Train Loss: 0.0048944, Train Acc: 0.9982065, Test Acc: 0.8225806\n",
            "Epoch: 109, Train Loss: 0.0030589, Train Acc: 0.9992314, Test Acc: 0.8410138\n",
            "Epoch: 110, Train Loss: 0.0031784, Train Acc: 0.9989751, Test Acc: 0.8387097\n",
            "Epoch: 111, Train Loss: 0.0044606, Train Acc: 0.9992314, Test Acc: 0.8433180\n",
            "Epoch: 112, Train Loss: 0.0165871, Train Acc: 0.9964130, Test Acc: 0.8294931\n",
            "Epoch: 113, Train Loss: 0.0043771, Train Acc: 0.9989751, Test Acc: 0.8364055\n",
            "Epoch: 114, Train Loss: 0.0024402, Train Acc: 0.9994876, Test Acc: 0.8179724\n",
            "Epoch: 115, Train Loss: 0.0057376, Train Acc: 0.9979503, Test Acc: 0.8317972\n",
            "Epoch: 116, Train Loss: 0.0079626, Train Acc: 0.9974379, Test Acc: 0.8410138\n",
            "Epoch: 117, Train Loss: 0.0045632, Train Acc: 0.9992314, Test Acc: 0.8387097\n",
            "Epoch: 118, Train Loss: 0.0028687, Train Acc: 0.9992314, Test Acc: 0.8502304\n",
            "Epoch: 119, Train Loss: 0.0019954, Train Acc: 0.9992314, Test Acc: 0.8387097\n",
            "Epoch: 120, Train Loss: 0.0022952, Train Acc: 0.9992314, Test Acc: 0.8271889\n",
            "--------\n",
            "Best Test Acc:   0.8525345622119815_0.0, Epoch: 54\n",
            "Best Train Loss: 0.001995403542905574_0.0\n",
            "Best Train Acc:  0.9997437868306431_0.0\n",
            "FOLD1, Mutagenicity, 1668873489/_rhoE0.5P18I1_GELU_X_6_256_0_0.001_50_0.6_B64S0W8, BT=0.8525345622119815_0.0, BE=54, ID=\n",
            "train_set : test_set = %d : %d 3903 434\n",
            "-------- FOLD1 DATASET=Mutagenicity, COMMIT_ID=\n",
            "#Params: 1332482\n",
            "Epoch: 001, Train Loss: 0.5906870, Train Acc: 0.6764028, Test Acc: 0.6451613\n",
            "Epoch: 002, Train Loss: 0.5720231, Train Acc: 0.7084294, Test Acc: 0.6935484\n",
            "Epoch: 003, Train Loss: 0.4976542, Train Acc: 0.7647963, Test Acc: 0.7488479\n",
            "Epoch: 004, Train Loss: 0.4883658, Train Acc: 0.7735076, Test Acc: 0.7165899\n",
            "Epoch: 005, Train Loss: 0.4780750, Train Acc: 0.7804253, Test Acc: 0.7488479\n",
            "Epoch: 006, Train Loss: 0.4709285, Train Acc: 0.7806815, Test Acc: 0.7557604\n",
            "Epoch: 007, Train Loss: 0.4411030, Train Acc: 0.8101460, Test Acc: 0.7764977\n",
            "Epoch: 008, Train Loss: 0.5165824, Train Acc: 0.7522419, Test Acc: 0.6912442\n",
            "Epoch: 009, Train Loss: 0.4318520, Train Acc: 0.8142455, Test Acc: 0.7857143\n",
            "Epoch: 010, Train Loss: 0.4213108, Train Acc: 0.8096336, Test Acc: 0.7880184\n",
            "Epoch: 011, Train Loss: 0.3943491, Train Acc: 0.8296182, Test Acc: 0.7880184\n",
            "Epoch: 012, Train Loss: 0.3967891, Train Acc: 0.8291058, Test Acc: 0.8041475\n",
            "Epoch: 013, Train Loss: 0.4140900, Train Acc: 0.8139892, Test Acc: 0.7718894\n",
            "Epoch: 014, Train Loss: 0.3818809, Train Acc: 0.8365360, Test Acc: 0.7972350\n",
            "Epoch: 015, Train Loss: 0.4062514, Train Acc: 0.8242378, Test Acc: 0.7534562\n",
            "Epoch: 016, Train Loss: 0.4444779, Train Acc: 0.7901614, Test Acc: 0.7626728\n",
            "Epoch: 017, Train Loss: 0.4014605, Train Acc: 0.8252626, Test Acc: 0.7672811\n",
            "Epoch: 018, Train Loss: 0.3387374, Train Acc: 0.8621573, Test Acc: 0.8087558\n",
            "Epoch: 019, Train Loss: 0.3429034, Train Acc: 0.8480656, Test Acc: 0.8041475\n",
            "Epoch: 020, Train Loss: 0.3199480, Train Acc: 0.8626697, Test Acc: 0.8364055\n",
            "Epoch: 021, Train Loss: 0.3335271, Train Acc: 0.8580579, Test Acc: 0.8156682\n",
            "Epoch: 022, Train Loss: 0.3052721, Train Acc: 0.8775301, Test Acc: 0.8225806\n",
            "Epoch: 023, Train Loss: 0.3276888, Train Acc: 0.8749680, Test Acc: 0.7972350\n",
            "Epoch: 024, Train Loss: 0.3315600, Train Acc: 0.8590828, Test Acc: 0.8018433\n",
            "Epoch: 025, Train Loss: 0.2933286, Train Acc: 0.8785550, Test Acc: 0.8248848\n",
            "Epoch: 026, Train Loss: 0.2800172, Train Acc: 0.8790674, Test Acc: 0.8202765\n",
            "Epoch: 027, Train Loss: 0.2707339, Train Acc: 0.8893159, Test Acc: 0.8156682\n",
            "Epoch: 028, Train Loss: 0.2710566, Train Acc: 0.8854727, Test Acc: 0.8364055\n",
            "Epoch: 029, Train Loss: 0.2912934, Train Acc: 0.8793236, Test Acc: 0.8156682\n",
            "Epoch: 030, Train Loss: 0.2541518, Train Acc: 0.8880348, Test Acc: 0.8248848\n",
            "Epoch: 031, Train Loss: 0.2600858, Train Acc: 0.8954650, Test Acc: 0.8179724\n",
            "Epoch: 032, Train Loss: 0.2426605, Train Acc: 0.8949526, Test Acc: 0.8179724\n",
            "Epoch: 033, Train Loss: 0.2243565, Train Acc: 0.9008455, Test Acc: 0.8317972\n",
            "Epoch: 034, Train Loss: 0.2093483, Train Acc: 0.9105816, Test Acc: 0.8156682\n",
            "Epoch: 035, Train Loss: 0.2117279, Train Acc: 0.9139124, Test Acc: 0.8179724\n",
            "Epoch: 036, Train Loss: 0.2716558, Train Acc: 0.8849603, Test Acc: 0.8202765\n",
            "Epoch: 037, Train Loss: 0.1722029, Train Acc: 0.9310787, Test Acc: 0.8179724\n",
            "Epoch: 038, Train Loss: 0.2718448, Train Acc: 0.8893159, Test Acc: 0.7880184\n",
            "Epoch: 039, Train Loss: 0.2078960, Train Acc: 0.9139124, Test Acc: 0.8271889\n",
            "Epoch: 040, Train Loss: 0.2120488, Train Acc: 0.9077633, Test Acc: 0.8271889\n",
            "Epoch: 041, Train Loss: 0.1619223, Train Acc: 0.9333846, Test Acc: 0.8202765\n",
            "Epoch: 042, Train Loss: 0.1740926, Train Acc: 0.9274917, Test Acc: 0.8456221\n",
            "Epoch: 043, Train Loss: 0.1474895, Train Acc: 0.9446580, Test Acc: 0.8179724\n",
            "Epoch: 044, Train Loss: 0.1949583, Train Acc: 0.9185242, Test Acc: 0.8225806\n",
            "Epoch: 045, Train Loss: 0.1141675, Train Acc: 0.9592621, Test Acc: 0.8364055\n",
            "Epoch: 046, Train Loss: 0.1308044, Train Acc: 0.9490136, Test Acc: 0.8225806\n",
            "Epoch: 047, Train Loss: 0.1608484, Train Acc: 0.9328721, Test Acc: 0.8133641\n",
            "Epoch: 048, Train Loss: 0.1418673, Train Acc: 0.9454266, Test Acc: 0.8133641\n",
            "Epoch: 049, Train Loss: 0.1699310, Train Acc: 0.9315911, Test Acc: 0.8041475\n",
            "Epoch: 050, Train Loss: 0.1327826, Train Acc: 0.9482449, Test Acc: 0.8341014\n",
            "Epoch: 051, Train Loss: 0.0730085, Train Acc: 0.9741225, Test Acc: 0.8248848\n",
            "Epoch: 052, Train Loss: 0.0677939, Train Acc: 0.9777095, Test Acc: 0.8133641\n",
            "Epoch: 053, Train Loss: 0.0605133, Train Acc: 0.9733538, Test Acc: 0.8248848\n",
            "Epoch: 054, Train Loss: 0.0630171, Train Acc: 0.9756597, Test Acc: 0.8248848\n",
            "Epoch: 055, Train Loss: 0.0527901, Train Acc: 0.9802716, Test Acc: 0.8202765\n",
            "Epoch: 056, Train Loss: 0.0589745, Train Acc: 0.9805278, Test Acc: 0.8225806\n",
            "Epoch: 057, Train Loss: 0.0396684, Train Acc: 0.9877018, Test Acc: 0.8317972\n",
            "Epoch: 058, Train Loss: 0.0655174, Train Acc: 0.9748911, Test Acc: 0.8018433\n",
            "Epoch: 059, Train Loss: 0.0555339, Train Acc: 0.9805278, Test Acc: 0.8294931\n",
            "Epoch: 060, Train Loss: 0.0409038, Train Acc: 0.9861645, Test Acc: 0.8156682\n",
            "Epoch: 061, Train Loss: 0.0475557, Train Acc: 0.9828337, Test Acc: 0.8433180\n",
            "Epoch: 062, Train Loss: 0.0454187, Train Acc: 0.9830899, Test Acc: 0.8133641\n",
            "Epoch: 063, Train Loss: 0.0360880, Train Acc: 0.9889828, Test Acc: 0.8248848\n",
            "Epoch: 064, Train Loss: 0.0356071, Train Acc: 0.9907763, Test Acc: 0.8341014\n",
            "Epoch: 065, Train Loss: 0.0345339, Train Acc: 0.9879580, Test Acc: 0.8225806\n",
            "Epoch: 066, Train Loss: 0.0469786, Train Acc: 0.9836024, Test Acc: 0.8410138\n",
            "Epoch: 067, Train Loss: 0.0313903, Train Acc: 0.9892390, Test Acc: 0.8225806\n",
            "Epoch: 068, Train Loss: 0.0308233, Train Acc: 0.9889828, Test Acc: 0.8110599\n",
            "Epoch: 069, Train Loss: 0.0473681, Train Acc: 0.9843710, Test Acc: 0.8294931\n",
            "Epoch: 070, Train Loss: 0.0412337, Train Acc: 0.9841148, Test Acc: 0.8317972\n",
            "Epoch: 071, Train Loss: 0.0317194, Train Acc: 0.9882142, Test Acc: 0.8179724\n",
            "Epoch: 072, Train Loss: 0.0352696, Train Acc: 0.9882142, Test Acc: 0.8156682\n",
            "Epoch: 073, Train Loss: 0.0333166, Train Acc: 0.9907763, Test Acc: 0.8156682\n",
            "Epoch: 074, Train Loss: 0.0643570, Train Acc: 0.9754035, Test Acc: 0.7949309\n",
            "Epoch: 075, Train Loss: 0.0268110, Train Acc: 0.9912888, Test Acc: 0.8202765\n",
            "Epoch: 076, Train Loss: 0.0208090, Train Acc: 0.9943633, Test Acc: 0.8248848\n",
            "Epoch: 077, Train Loss: 0.0164949, Train Acc: 0.9948757, Test Acc: 0.8248848\n",
            "Epoch: 078, Train Loss: 0.0206870, Train Acc: 0.9928260, Test Acc: 0.8248848\n",
            "Epoch: 079, Train Loss: 0.0289660, Train Acc: 0.9900077, Test Acc: 0.8156682\n",
            "Epoch: 080, Train Loss: 0.1046806, Train Acc: 0.9638739, Test Acc: 0.7926267\n",
            "Epoch: 081, Train Loss: 0.0343164, Train Acc: 0.9879580, Test Acc: 0.8225806\n",
            "Epoch: 082, Train Loss: 0.0146637, Train Acc: 0.9961568, Test Acc: 0.8387097\n",
            "Epoch: 083, Train Loss: 0.0200560, Train Acc: 0.9938509, Test Acc: 0.8271889\n",
            "Epoch: 084, Train Loss: 0.0412485, Train Acc: 0.9833461, Test Acc: 0.8110599\n",
            "Epoch: 085, Train Loss: 0.0255128, Train Acc: 0.9902639, Test Acc: 0.8248848\n",
            "Epoch: 086, Train Loss: 0.0507750, Train Acc: 0.9825775, Test Acc: 0.7995392\n",
            "Epoch: 087, Train Loss: 0.0795842, Train Acc: 0.9692544, Test Acc: 0.7880184\n",
            "Epoch: 088, Train Loss: 0.0190100, Train Acc: 0.9943633, Test Acc: 0.8179724\n",
            "Epoch: 089, Train Loss: 0.0351368, Train Acc: 0.9882142, Test Acc: 0.8064516\n",
            "Epoch: 090, Train Loss: 0.0339353, Train Acc: 0.9866769, Test Acc: 0.8133641\n",
            "Epoch: 091, Train Loss: 0.0105617, Train Acc: 0.9971817, Test Acc: 0.8433180\n",
            "Epoch: 092, Train Loss: 0.0113915, Train Acc: 0.9961568, Test Acc: 0.8179724\n",
            "Epoch: 093, Train Loss: 0.0141154, Train Acc: 0.9959006, Test Acc: 0.8133641\n",
            "Epoch: 094, Train Loss: 0.0102493, Train Acc: 0.9969254, Test Acc: 0.8202765\n",
            "Epoch: 095, Train Loss: 0.0170830, Train Acc: 0.9943633, Test Acc: 0.8156682\n",
            "Epoch: 096, Train Loss: 0.0125064, Train Acc: 0.9953882, Test Acc: 0.8317972\n",
            "Epoch: 097, Train Loss: 0.0212587, Train Acc: 0.9920574, Test Acc: 0.8087558\n",
            "Epoch: 098, Train Loss: 0.0214530, Train Acc: 0.9925698, Test Acc: 0.8202765\n",
            "Epoch: 099, Train Loss: 0.0133713, Train Acc: 0.9959006, Test Acc: 0.8271889\n",
            "Epoch: 100, Train Loss: 0.0261625, Train Acc: 0.9905201, Test Acc: 0.8271889\n",
            "Epoch: 101, Train Loss: 0.0076958, Train Acc: 0.9984627, Test Acc: 0.8133641\n",
            "Epoch: 102, Train Loss: 0.0043699, Train Acc: 0.9994876, Test Acc: 0.8294931\n",
            "Epoch: 103, Train Loss: 0.0103180, Train Acc: 0.9969254, Test Acc: 0.8110599\n",
            "Epoch: 104, Train Loss: 0.0030534, Train Acc: 0.9987189, Test Acc: 0.8317972\n",
            "Epoch: 105, Train Loss: 0.0033378, Train Acc: 0.9992314, Test Acc: 0.8387097\n",
            "Epoch: 106, Train Loss: 0.0044941, Train Acc: 0.9982065, Test Acc: 0.8294931\n",
            "Epoch: 107, Train Loss: 0.0031241, Train Acc: 0.9989751, Test Acc: 0.8364055\n",
            "Epoch: 108, Train Loss: 0.0064099, Train Acc: 0.9976941, Test Acc: 0.8248848\n",
            "Epoch: 109, Train Loss: 0.0038625, Train Acc: 0.9989751, Test Acc: 0.8133641\n",
            "Epoch: 110, Train Loss: 0.0050384, Train Acc: 0.9992314, Test Acc: 0.8225806\n",
            "Epoch: 111, Train Loss: 0.0014846, Train Acc: 0.9994876, Test Acc: 0.8202765\n",
            "Epoch: 112, Train Loss: 0.0026806, Train Acc: 0.9994876, Test Acc: 0.8248848\n",
            "Epoch: 113, Train Loss: 0.0043162, Train Acc: 0.9989751, Test Acc: 0.8179724\n",
            "Epoch: 114, Train Loss: 0.0050036, Train Acc: 0.9992314, Test Acc: 0.8110599\n",
            "Epoch: 115, Train Loss: 0.0023070, Train Acc: 0.9992314, Test Acc: 0.8179724\n",
            "Epoch: 116, Train Loss: 0.0028297, Train Acc: 0.9992314, Test Acc: 0.8248848\n",
            "Epoch: 117, Train Loss: 0.0025763, Train Acc: 0.9992314, Test Acc: 0.8110599\n",
            "Epoch: 118, Train Loss: 0.0026438, Train Acc: 0.9992314, Test Acc: 0.8110599\n",
            "Epoch: 119, Train Loss: 0.0202160, Train Acc: 0.9933385, Test Acc: 0.8364055\n",
            "Epoch: 120, Train Loss: 0.0068870, Train Acc: 0.9982065, Test Acc: 0.8225806\n",
            "--------\n",
            "Best Test Acc:   0.8444700460829493_0.005760368663594473, Epoch: 104\n",
            "Best Train Loss: 0.002756246909156052_0.0001124125406263539\n",
            "Best Train Acc:  0.9993594670766077_0.0001281065846784335\n",
            "FOLD2, Mutagenicity, 1668873489/_rhoE0.5P18I1_GELU_X_6_256_0_0.001_50_0.6_B64S0W8, BT=0.8444700460829493_0.005760368663594473, BE=104, ID=\n",
            "train_set : test_set = %d : %d 3903 434\n",
            "-------- FOLD2 DATASET=Mutagenicity, COMMIT_ID=\n",
            "#Params: 1332482\n",
            "Epoch: 001, Train Loss: 0.5690257, Train Acc: 0.6968998, Test Acc: 0.7073733\n",
            "Epoch: 002, Train Loss: 0.5208440, Train Acc: 0.7337945, Test Acc: 0.7258065\n",
            "Epoch: 003, Train Loss: 0.5333820, Train Acc: 0.7486549, Test Acc: 0.7304147\n",
            "Epoch: 004, Train Loss: 0.4830594, Train Acc: 0.7796567, Test Acc: 0.7465438\n",
            "Epoch: 005, Train Loss: 0.4696435, Train Acc: 0.7916987, Test Acc: 0.7834101\n",
            "Epoch: 006, Train Loss: 0.4437531, Train Acc: 0.8019472, Test Acc: 0.7834101\n",
            "Epoch: 007, Train Loss: 0.4472359, Train Acc: 0.8024596, Test Acc: 0.7949309\n",
            "Epoch: 008, Train Loss: 0.4198588, Train Acc: 0.8160389, Test Acc: 0.7672811\n",
            "Epoch: 009, Train Loss: 0.4155410, Train Acc: 0.8142455, Test Acc: 0.7834101\n",
            "Epoch: 010, Train Loss: 0.4200742, Train Acc: 0.8060466, Test Acc: 0.7419355\n",
            "Epoch: 011, Train Loss: 0.4293461, Train Acc: 0.8178324, Test Acc: 0.7741935\n",
            "Epoch: 012, Train Loss: 0.3764519, Train Acc: 0.8283372, Test Acc: 0.8110599\n",
            "Epoch: 013, Train Loss: 0.3800113, Train Acc: 0.8306431, Test Acc: 0.7903226\n",
            "Epoch: 014, Train Loss: 0.3648511, Train Acc: 0.8472970, Test Acc: 0.8179724\n",
            "Epoch: 015, Train Loss: 0.3731380, Train Acc: 0.8385857, Test Acc: 0.7903226\n",
            "Epoch: 016, Train Loss: 0.3995741, Train Acc: 0.8257750, Test Acc: 0.7834101\n",
            "Epoch: 017, Train Loss: 0.3570424, Train Acc: 0.8426851, Test Acc: 0.8064516\n",
            "Epoch: 018, Train Loss: 0.3322743, Train Acc: 0.8593390, Test Acc: 0.8110599\n",
            "Epoch: 019, Train Loss: 0.3352399, Train Acc: 0.8611325, Test Acc: 0.8087558\n",
            "Epoch: 020, Train Loss: 0.3397655, Train Acc: 0.8490904, Test Acc: 0.7949309\n",
            "Epoch: 021, Train Loss: 0.3493321, Train Acc: 0.8526774, Test Acc: 0.7857143\n",
            "Epoch: 022, Train Loss: 0.3384215, Train Acc: 0.8549833, Test Acc: 0.8133641\n",
            "Epoch: 023, Train Loss: 0.3612436, Train Acc: 0.8406354, Test Acc: 0.7672811\n",
            "Epoch: 024, Train Loss: 0.3098586, Train Acc: 0.8693313, Test Acc: 0.7880184\n",
            "Epoch: 025, Train Loss: 0.3066495, Train Acc: 0.8695875, Test Acc: 0.8179724\n",
            "Epoch: 026, Train Loss: 0.2551809, Train Acc: 0.9016141, Test Acc: 0.8156682\n",
            "Epoch: 027, Train Loss: 0.2478809, Train Acc: 0.8967461, Test Acc: 0.8041475\n",
            "Epoch: 028, Train Loss: 0.2560361, Train Acc: 0.9067384, Test Acc: 0.8018433\n",
            "Epoch: 029, Train Loss: 0.2614161, Train Acc: 0.8918780, Test Acc: 0.8225806\n",
            "Epoch: 030, Train Loss: 0.2467319, Train Acc: 0.8957212, Test Acc: 0.7811060\n",
            "Epoch: 031, Train Loss: 0.2893653, Train Acc: 0.8741993, Test Acc: 0.7695853\n",
            "Epoch: 032, Train Loss: 0.2025592, Train Acc: 0.9182680, Test Acc: 0.8018433\n",
            "Epoch: 033, Train Loss: 0.1984036, Train Acc: 0.9210863, Test Acc: 0.8018433\n",
            "Epoch: 034, Train Loss: 0.2178231, Train Acc: 0.9095568, Test Acc: 0.8202765\n",
            "Epoch: 035, Train Loss: 0.1882272, Train Acc: 0.9200615, Test Acc: 0.7995392\n",
            "Epoch: 036, Train Loss: 0.1937301, Train Acc: 0.9205739, Test Acc: 0.7834101\n",
            "Epoch: 037, Train Loss: 0.1579821, Train Acc: 0.9379964, Test Acc: 0.7926267\n",
            "Epoch: 038, Train Loss: 0.1763295, Train Acc: 0.9315911, Test Acc: 0.7995392\n",
            "Epoch: 039, Train Loss: 0.2332260, Train Acc: 0.9054573, Test Acc: 0.7649770\n",
            "Epoch: 040, Train Loss: 0.1671866, Train Acc: 0.9354343, Test Acc: 0.8156682\n",
            "Epoch: 041, Train Loss: 0.1431637, Train Acc: 0.9500384, Test Acc: 0.8133641\n",
            "Epoch: 042, Train Loss: 0.1293766, Train Acc: 0.9469639, Test Acc: 0.8064516\n",
            "Epoch: 043, Train Loss: 0.1364645, Train Acc: 0.9518319, Test Acc: 0.7857143\n",
            "Epoch: 044, Train Loss: 0.1183939, Train Acc: 0.9556751, Test Acc: 0.8041475\n",
            "Epoch: 045, Train Loss: 0.1417108, Train Acc: 0.9413272, Test Acc: 0.8041475\n",
            "Epoch: 046, Train Loss: 0.1314208, Train Acc: 0.9500384, Test Acc: 0.7880184\n",
            "Epoch: 047, Train Loss: 0.1232148, Train Acc: 0.9518319, Test Acc: 0.8202765\n",
            "Epoch: 048, Train Loss: 0.1332851, Train Acc: 0.9485012, Test Acc: 0.8064516\n",
            "Epoch: 049, Train Loss: 0.1035610, Train Acc: 0.9615680, Test Acc: 0.7949309\n",
            "Epoch: 050, Train Loss: 0.1118991, Train Acc: 0.9567000, Test Acc: 0.7972350\n",
            "Epoch: 051, Train Loss: 0.0519129, Train Acc: 0.9846272, Test Acc: 0.7880184\n",
            "Epoch: 052, Train Loss: 0.0552230, Train Acc: 0.9815527, Test Acc: 0.7903226\n",
            "Epoch: 053, Train Loss: 0.0335482, Train Acc: 0.9889828, Test Acc: 0.7949309\n",
            "Epoch: 054, Train Loss: 0.0395479, Train Acc: 0.9861645, Test Acc: 0.7903226\n",
            "Epoch: 055, Train Loss: 0.0431624, Train Acc: 0.9841148, Test Acc: 0.7949309\n",
            "Epoch: 056, Train Loss: 0.0689585, Train Acc: 0.9695106, Test Acc: 0.7741935\n",
            "Epoch: 057, Train Loss: 0.0381475, Train Acc: 0.9892390, Test Acc: 0.7972350\n",
            "Epoch: 058, Train Loss: 0.0521705, Train Acc: 0.9820651, Test Acc: 0.7926267\n",
            "Epoch: 059, Train Loss: 0.0250185, Train Acc: 0.9918012, Test Acc: 0.7903226\n",
            "Epoch: 060, Train Loss: 0.0340462, Train Acc: 0.9892390, Test Acc: 0.8110599\n",
            "Epoch: 061, Train Loss: 0.0558517, Train Acc: 0.9784781, Test Acc: 0.7972350\n",
            "Epoch: 062, Train Loss: 0.0331064, Train Acc: 0.9900077, Test Acc: 0.8018433\n",
            "Epoch: 063, Train Loss: 0.0475101, Train Acc: 0.9815527, Test Acc: 0.7903226\n",
            "Epoch: 064, Train Loss: 0.0238202, Train Acc: 0.9920574, Test Acc: 0.7764977\n",
            "Epoch: 065, Train Loss: 0.0548509, Train Acc: 0.9810402, Test Acc: 0.7764977\n",
            "Epoch: 066, Train Loss: 0.0350572, Train Acc: 0.9884704, Test Acc: 0.7949309\n",
            "Epoch: 067, Train Loss: 0.0334072, Train Acc: 0.9915450, Test Acc: 0.7949309\n",
            "Epoch: 068, Train Loss: 0.0262546, Train Acc: 0.9915450, Test Acc: 0.7949309\n",
            "Epoch: 069, Train Loss: 0.0387423, Train Acc: 0.9866769, Test Acc: 0.7926267\n",
            "Epoch: 070, Train Loss: 0.0422269, Train Acc: 0.9843710, Test Acc: 0.8018433\n",
            "Epoch: 071, Train Loss: 0.0418204, Train Acc: 0.9856521, Test Acc: 0.7672811\n",
            "Epoch: 072, Train Loss: 0.0448001, Train Acc: 0.9828337, Test Acc: 0.7857143\n",
            "Epoch: 073, Train Loss: 0.0294677, Train Acc: 0.9900077, Test Acc: 0.7834101\n",
            "Epoch: 074, Train Loss: 0.0225608, Train Acc: 0.9923136, Test Acc: 0.7995392\n",
            "Epoch: 075, Train Loss: 0.0210847, Train Acc: 0.9938509, Test Acc: 0.8018433\n",
            "Epoch: 076, Train Loss: 0.0187039, Train Acc: 0.9951319, Test Acc: 0.8064516\n",
            "Epoch: 077, Train Loss: 0.0366112, Train Acc: 0.9887266, Test Acc: 0.7880184\n",
            "Epoch: 078, Train Loss: 0.0156849, Train Acc: 0.9961568, Test Acc: 0.7811060\n",
            "Epoch: 079, Train Loss: 0.0329071, Train Acc: 0.9884704, Test Acc: 0.7811060\n",
            "Epoch: 080, Train Loss: 0.0220955, Train Acc: 0.9938509, Test Acc: 0.7995392\n",
            "Epoch: 081, Train Loss: 0.0206193, Train Acc: 0.9935947, Test Acc: 0.7949309\n",
            "Epoch: 082, Train Loss: 0.0204088, Train Acc: 0.9941071, Test Acc: 0.8133641\n",
            "Epoch: 083, Train Loss: 0.0153601, Train Acc: 0.9953882, Test Acc: 0.7788018\n",
            "Epoch: 084, Train Loss: 0.0472465, Train Acc: 0.9836024, Test Acc: 0.7718894\n",
            "Epoch: 085, Train Loss: 0.0189882, Train Acc: 0.9933385, Test Acc: 0.7926267\n",
            "Epoch: 086, Train Loss: 0.0317072, Train Acc: 0.9884704, Test Acc: 0.7949309\n",
            "Epoch: 087, Train Loss: 0.0296039, Train Acc: 0.9879580, Test Acc: 0.7995392\n",
            "Epoch: 088, Train Loss: 0.0144515, Train Acc: 0.9953882, Test Acc: 0.8018433\n",
            "Epoch: 089, Train Loss: 0.0220524, Train Acc: 0.9941071, Test Acc: 0.8018433\n",
            "Epoch: 090, Train Loss: 0.0298080, Train Acc: 0.9910325, Test Acc: 0.7926267\n",
            "Epoch: 091, Train Loss: 0.0183684, Train Acc: 0.9935947, Test Acc: 0.8041475\n",
            "Epoch: 092, Train Loss: 0.0676064, Train Acc: 0.9741225, Test Acc: 0.7903226\n",
            "Epoch: 093, Train Loss: 0.0641781, Train Acc: 0.9748911, Test Acc: 0.7603687\n",
            "Epoch: 094, Train Loss: 0.0176879, Train Acc: 0.9941071, Test Acc: 0.7926267\n",
            "Epoch: 095, Train Loss: 0.0161379, Train Acc: 0.9935947, Test Acc: 0.8202765\n",
            "Epoch: 096, Train Loss: 0.0168811, Train Acc: 0.9951319, Test Acc: 0.7926267\n",
            "Epoch: 097, Train Loss: 0.0195000, Train Acc: 0.9941071, Test Acc: 0.8179724\n",
            "Epoch: 098, Train Loss: 0.0058563, Train Acc: 0.9982065, Test Acc: 0.8041475\n",
            "Epoch: 099, Train Loss: 0.0089822, Train Acc: 0.9969254, Test Acc: 0.8110599\n",
            "Epoch: 100, Train Loss: 0.0173889, Train Acc: 0.9946195, Test Acc: 0.7972350\n",
            "Epoch: 101, Train Loss: 0.0051430, Train Acc: 0.9984627, Test Acc: 0.7903226\n",
            "Epoch: 102, Train Loss: 0.0014988, Train Acc: 1.0000000, Test Acc: 0.7995392\n",
            "Epoch: 103, Train Loss: 0.0034047, Train Acc: 0.9987189, Test Acc: 0.7972350\n",
            "Epoch: 104, Train Loss: 0.0016888, Train Acc: 0.9997438, Test Acc: 0.7995392\n",
            "Epoch: 105, Train Loss: 0.0017949, Train Acc: 0.9994876, Test Acc: 0.8087558\n",
            "Epoch: 106, Train Loss: 0.0021925, Train Acc: 0.9989751, Test Acc: 0.8179724\n",
            "Epoch: 107, Train Loss: 0.0015920, Train Acc: 0.9997438, Test Acc: 0.8064516\n",
            "Epoch: 108, Train Loss: 0.0014050, Train Acc: 0.9992314, Test Acc: 0.7995392\n",
            "Epoch: 109, Train Loss: 0.0073006, Train Acc: 0.9984627, Test Acc: 0.8110599\n",
            "Epoch: 110, Train Loss: 0.0030736, Train Acc: 0.9992314, Test Acc: 0.7995392\n",
            "Epoch: 111, Train Loss: 0.0019251, Train Acc: 0.9997438, Test Acc: 0.8064516\n",
            "Epoch: 112, Train Loss: 0.0019502, Train Acc: 0.9994876, Test Acc: 0.7995392\n",
            "Epoch: 113, Train Loss: 0.0052506, Train Acc: 0.9982065, Test Acc: 0.8018433\n",
            "Epoch: 114, Train Loss: 0.0020701, Train Acc: 0.9994876, Test Acc: 0.8110599\n",
            "Epoch: 115, Train Loss: 0.0020879, Train Acc: 0.9994876, Test Acc: 0.8041475\n",
            "Epoch: 116, Train Loss: 0.0106943, Train Acc: 0.9971817, Test Acc: 0.7949309\n",
            "Epoch: 117, Train Loss: 0.0033201, Train Acc: 0.9992314, Test Acc: 0.8110599\n",
            "Epoch: 118, Train Loss: 0.0023982, Train Acc: 0.9997438, Test Acc: 0.8248848\n",
            "Epoch: 119, Train Loss: 0.0107101, Train Acc: 0.9969254, Test Acc: 0.7995392\n",
            "Epoch: 120, Train Loss: 0.0130376, Train Acc: 0.9956444, Test Acc: 0.8087558\n",
            "--------\n",
            "Best Test Acc:   0.8325652841781874_0.017480501807589945, Epoch: 104\n",
            "Best Train Loss: 0.0026234549362709066_0.001311475669170508\n",
            "Best Train Acc:  0.9995729780510718_0.00031955396590434423\n",
            "FOLD3, Mutagenicity, 1668873489/_rhoE0.5P18I1_GELU_X_6_256_0_0.001_50_0.6_B64S0W8, BT=0.8325652841781874_0.017480501807589945, BE=104, ID=\n",
            "train_set : test_set = %d : %d 3903 434\n",
            "-------- FOLD3 DATASET=Mutagenicity, COMMIT_ID=\n",
            "#Params: 1332482\n",
            "Epoch: 001, Train Loss: 0.6914923, Train Acc: 0.5698181, Test Acc: 0.5806452\n",
            "Epoch: 002, Train Loss: 0.5252133, Train Acc: 0.7363566, Test Acc: 0.7165899\n",
            "Epoch: 003, Train Loss: 0.4761431, Train Acc: 0.7809377, Test Acc: 0.7557604\n",
            "Epoch: 004, Train Loss: 0.5172471, Train Acc: 0.7414809, Test Acc: 0.6797235\n",
            "Epoch: 005, Train Loss: 0.5010424, Train Acc: 0.7809377, Test Acc: 0.7373272\n",
            "Epoch: 006, Train Loss: 0.4666359, Train Acc: 0.7832437, Test Acc: 0.7718894\n",
            "Epoch: 007, Train Loss: 0.4357865, Train Acc: 0.8124520, Test Acc: 0.7834101\n",
            "Epoch: 008, Train Loss: 0.4963961, Train Acc: 0.7591596, Test Acc: 0.7304147\n",
            "Epoch: 009, Train Loss: 0.4593279, Train Acc: 0.7904176, Test Acc: 0.7695853\n",
            "Epoch: 010, Train Loss: 0.4336851, Train Acc: 0.8139892, Test Acc: 0.7880184\n",
            "Epoch: 011, Train Loss: 0.4245327, Train Acc: 0.8162952, Test Acc: 0.7880184\n",
            "Epoch: 012, Train Loss: 0.3746098, Train Acc: 0.8342301, Test Acc: 0.7972350\n",
            "Epoch: 013, Train Loss: 0.4300281, Train Acc: 0.8014348, Test Acc: 0.7695853\n",
            "Epoch: 014, Train Loss: 0.4125705, Train Acc: 0.8209070, Test Acc: 0.7695853\n",
            "Epoch: 015, Train Loss: 0.3533924, Train Acc: 0.8488342, Test Acc: 0.7926267\n",
            "Epoch: 016, Train Loss: 0.3518762, Train Acc: 0.8462721, Test Acc: 0.8018433\n",
            "Epoch: 017, Train Loss: 0.3595101, Train Acc: 0.8478094, Test Acc: 0.8041475\n",
            "Epoch: 018, Train Loss: 0.3286888, Train Acc: 0.8570331, Test Acc: 0.8133641\n",
            "Epoch: 019, Train Loss: 0.3508636, Train Acc: 0.8493467, Test Acc: 0.7972350\n",
            "Epoch: 020, Train Loss: 0.3134017, Train Acc: 0.8654881, Test Acc: 0.8064516\n",
            "Epoch: 021, Train Loss: 0.3551136, Train Acc: 0.8431975, Test Acc: 0.8064516\n",
            "Epoch: 022, Train Loss: 0.3421081, Train Acc: 0.8508839, Test Acc: 0.7972350\n",
            "Epoch: 023, Train Loss: 0.2942811, Train Acc: 0.8775301, Test Acc: 0.8179724\n",
            "Epoch: 024, Train Loss: 0.3252556, Train Acc: 0.8665129, Test Acc: 0.7995392\n",
            "Epoch: 025, Train Loss: 0.2987082, Train Acc: 0.8736869, Test Acc: 0.8064516\n",
            "Epoch: 026, Train Loss: 0.2693067, Train Acc: 0.8870100, Test Acc: 0.8133641\n",
            "Epoch: 027, Train Loss: 0.2749594, Train Acc: 0.8857289, Test Acc: 0.8133641\n",
            "Epoch: 028, Train Loss: 0.2529862, Train Acc: 0.8964899, Test Acc: 0.8156682\n",
            "Epoch: 029, Train Loss: 0.2833469, Train Acc: 0.8875224, Test Acc: 0.8018433\n",
            "Epoch: 030, Train Loss: 0.2554965, Train Acc: 0.8923905, Test Acc: 0.8271889\n",
            "Epoch: 031, Train Loss: 0.2125533, Train Acc: 0.9157059, Test Acc: 0.8433180\n",
            "Epoch: 032, Train Loss: 0.2100178, Train Acc: 0.9200615, Test Acc: 0.8179724\n",
            "Epoch: 033, Train Loss: 0.2829170, Train Acc: 0.8744555, Test Acc: 0.8087558\n",
            "Epoch: 034, Train Loss: 0.2251580, Train Acc: 0.9075070, Test Acc: 0.7995392\n",
            "Epoch: 035, Train Loss: 0.2161055, Train Acc: 0.9139124, Test Acc: 0.8502304\n",
            "Epoch: 036, Train Loss: 0.2143642, Train Acc: 0.9128875, Test Acc: 0.8294931\n",
            "Epoch: 037, Train Loss: 0.1980738, Train Acc: 0.9174994, Test Acc: 0.8294931\n",
            "Epoch: 038, Train Loss: 0.1743746, Train Acc: 0.9333846, Test Acc: 0.8087558\n",
            "Epoch: 039, Train Loss: 0.1647112, Train Acc: 0.9356905, Test Acc: 0.8433180\n",
            "Epoch: 040, Train Loss: 0.1697560, Train Acc: 0.9321035, Test Acc: 0.8341014\n",
            "Epoch: 041, Train Loss: 0.2040639, Train Acc: 0.9141686, Test Acc: 0.7972350\n",
            "Epoch: 042, Train Loss: 0.1990609, Train Acc: 0.9195491, Test Acc: 0.8294931\n",
            "Epoch: 043, Train Loss: 0.1707036, Train Acc: 0.9349219, Test Acc: 0.8156682\n",
            "Epoch: 044, Train Loss: 0.1476861, Train Acc: 0.9451704, Test Acc: 0.8387097\n",
            "Epoch: 045, Train Loss: 0.1870727, Train Acc: 0.9246733, Test Acc: 0.8041475\n",
            "Epoch: 046, Train Loss: 0.1970058, Train Acc: 0.9231360, Test Acc: 0.8156682\n",
            "Epoch: 047, Train Loss: 0.1311592, Train Acc: 0.9538816, Test Acc: 0.8156682\n",
            "Epoch: 048, Train Loss: 0.1143290, Train Acc: 0.9592621, Test Acc: 0.8502304\n",
            "Epoch: 049, Train Loss: 0.1319088, Train Acc: 0.9441455, Test Acc: 0.8479263\n",
            "Epoch: 050, Train Loss: 0.1834224, Train Acc: 0.9169869, Test Acc: 0.8271889\n",
            "Epoch: 051, Train Loss: 0.0752174, Train Acc: 0.9764284, Test Acc: 0.8571429\n",
            "Epoch: 052, Train Loss: 0.0564159, Train Acc: 0.9825775, Test Acc: 0.8456221\n",
            "Epoch: 053, Train Loss: 0.0510468, Train Acc: 0.9782219, Test Acc: 0.8410138\n",
            "Epoch: 054, Train Loss: 0.0677522, Train Acc: 0.9769408, Test Acc: 0.8456221\n",
            "Epoch: 055, Train Loss: 0.0460110, Train Acc: 0.9851396, Test Acc: 0.8341014\n",
            "Epoch: 056, Train Loss: 0.0510821, Train Acc: 0.9841148, Test Acc: 0.8433180\n",
            "Epoch: 057, Train Loss: 0.0830303, Train Acc: 0.9677171, Test Acc: 0.8364055\n",
            "Epoch: 058, Train Loss: 0.0472434, Train Acc: 0.9830899, Test Acc: 0.8479263\n",
            "Epoch: 059, Train Loss: 0.0595183, Train Acc: 0.9795029, Test Acc: 0.8317972\n",
            "Epoch: 060, Train Loss: 0.0544576, Train Acc: 0.9797592, Test Acc: 0.8433180\n",
            "Epoch: 061, Train Loss: 0.0412100, Train Acc: 0.9846272, Test Acc: 0.8479263\n",
            "Epoch: 062, Train Loss: 0.0360668, Train Acc: 0.9869331, Test Acc: 0.8525346\n",
            "Epoch: 063, Train Loss: 0.0744348, Train Acc: 0.9697668, Test Acc: 0.8364055\n",
            "Epoch: 064, Train Loss: 0.0362353, Train Acc: 0.9871893, Test Acc: 0.8502304\n",
            "Epoch: 065, Train Loss: 0.0389541, Train Acc: 0.9877018, Test Acc: 0.8341014\n",
            "Epoch: 066, Train Loss: 0.0447212, Train Acc: 0.9818089, Test Acc: 0.8594470\n",
            "Epoch: 067, Train Loss: 0.0488599, Train Acc: 0.9807840, Test Acc: 0.8525346\n",
            "Epoch: 068, Train Loss: 0.0587638, Train Acc: 0.9777095, Test Acc: 0.8248848\n",
            "Epoch: 069, Train Loss: 0.0478239, Train Acc: 0.9815527, Test Acc: 0.8271889\n",
            "Epoch: 070, Train Loss: 0.0278648, Train Acc: 0.9900077, Test Acc: 0.8594470\n",
            "Epoch: 071, Train Loss: 0.0253680, Train Acc: 0.9918012, Test Acc: 0.8571429\n",
            "Epoch: 072, Train Loss: 0.0346195, Train Acc: 0.9882142, Test Acc: 0.8640553\n",
            "Epoch: 073, Train Loss: 0.0278545, Train Acc: 0.9905201, Test Acc: 0.8548387\n",
            "Epoch: 074, Train Loss: 0.0678470, Train Acc: 0.9748911, Test Acc: 0.8341014\n",
            "Epoch: 075, Train Loss: 0.0371952, Train Acc: 0.9877018, Test Acc: 0.8387097\n",
            "Epoch: 076, Train Loss: 0.0195714, Train Acc: 0.9948757, Test Acc: 0.8548387\n",
            "Epoch: 077, Train Loss: 0.0458986, Train Acc: 0.9807840, Test Acc: 0.8433180\n",
            "Epoch: 078, Train Loss: 0.0201096, Train Acc: 0.9928260, Test Acc: 0.8571429\n",
            "Epoch: 079, Train Loss: 0.0556402, Train Acc: 0.9795029, Test Acc: 0.8387097\n",
            "Epoch: 080, Train Loss: 0.0338809, Train Acc: 0.9900077, Test Acc: 0.8548387\n",
            "Epoch: 081, Train Loss: 0.0338302, Train Acc: 0.9912888, Test Acc: 0.8387097\n",
            "Epoch: 082, Train Loss: 0.0317046, Train Acc: 0.9882142, Test Acc: 0.8548387\n",
            "Epoch: 083, Train Loss: 0.0197433, Train Acc: 0.9938509, Test Acc: 0.8594470\n",
            "Epoch: 084, Train Loss: 0.0205510, Train Acc: 0.9941071, Test Acc: 0.8410138\n",
            "Epoch: 085, Train Loss: 0.0193680, Train Acc: 0.9935947, Test Acc: 0.8341014\n",
            "Epoch: 086, Train Loss: 0.0440577, Train Acc: 0.9846272, Test Acc: 0.8202765\n",
            "Epoch: 087, Train Loss: 0.0262638, Train Acc: 0.9907763, Test Acc: 0.8341014\n",
            "Epoch: 088, Train Loss: 0.0131627, Train Acc: 0.9951319, Test Acc: 0.8341014\n",
            "Epoch: 089, Train Loss: 0.0139042, Train Acc: 0.9961568, Test Acc: 0.8594470\n",
            "Epoch: 090, Train Loss: 0.0091973, Train Acc: 0.9976941, Test Acc: 0.8387097\n",
            "Epoch: 091, Train Loss: 0.0194439, Train Acc: 0.9933385, Test Acc: 0.8502304\n",
            "Epoch: 092, Train Loss: 0.0532246, Train Acc: 0.9805278, Test Acc: 0.8225806\n",
            "Epoch: 093, Train Loss: 0.0174472, Train Acc: 0.9959006, Test Acc: 0.8525346\n",
            "Epoch: 094, Train Loss: 0.0239969, Train Acc: 0.9938509, Test Acc: 0.8456221\n",
            "Epoch: 095, Train Loss: 0.0120826, Train Acc: 0.9971817, Test Acc: 0.8548387\n",
            "Epoch: 096, Train Loss: 0.0065015, Train Acc: 0.9984627, Test Acc: 0.8479263\n",
            "Epoch: 097, Train Loss: 0.0095360, Train Acc: 0.9974379, Test Acc: 0.8548387\n",
            "Epoch: 098, Train Loss: 0.0145661, Train Acc: 0.9953882, Test Acc: 0.8525346\n",
            "Epoch: 099, Train Loss: 0.0073406, Train Acc: 0.9984627, Test Acc: 0.8617512\n",
            "Epoch: 100, Train Loss: 0.0588031, Train Acc: 0.9800154, Test Acc: 0.8364055\n",
            "Epoch: 101, Train Loss: 0.0101216, Train Acc: 0.9969254, Test Acc: 0.8525346\n",
            "Epoch: 102, Train Loss: 0.0061284, Train Acc: 0.9979503, Test Acc: 0.8502304\n",
            "Epoch: 103, Train Loss: 0.0020200, Train Acc: 1.0000000, Test Acc: 0.8548387\n",
            "Epoch: 104, Train Loss: 0.0044649, Train Acc: 0.9982065, Test Acc: 0.8479263\n",
            "Epoch: 105, Train Loss: 0.0025549, Train Acc: 0.9992314, Test Acc: 0.8617512\n",
            "Epoch: 106, Train Loss: 0.0019948, Train Acc: 0.9994876, Test Acc: 0.8571429\n",
            "Epoch: 107, Train Loss: 0.0027287, Train Acc: 0.9992314, Test Acc: 0.8617512\n",
            "Epoch: 108, Train Loss: 0.0035714, Train Acc: 0.9987189, Test Acc: 0.8617512\n",
            "Epoch: 109, Train Loss: 0.0023905, Train Acc: 0.9997438, Test Acc: 0.8502304\n",
            "Epoch: 110, Train Loss: 0.0026841, Train Acc: 0.9989751, Test Acc: 0.8433180\n",
            "Epoch: 111, Train Loss: 0.0099624, Train Acc: 0.9982065, Test Acc: 0.8433180\n",
            "Epoch: 112, Train Loss: 0.0027150, Train Acc: 0.9989751, Test Acc: 0.8548387\n",
            "Epoch: 113, Train Loss: 0.0055377, Train Acc: 0.9987189, Test Acc: 0.8456221\n",
            "Epoch: 114, Train Loss: 0.0014688, Train Acc: 1.0000000, Test Acc: 0.8594470\n",
            "Epoch: 115, Train Loss: 0.0022730, Train Acc: 0.9997438, Test Acc: 0.8479263\n",
            "Epoch: 116, Train Loss: 0.0031569, Train Acc: 0.9994876, Test Acc: 0.8502304\n",
            "Epoch: 117, Train Loss: 0.0044223, Train Acc: 0.9987189, Test Acc: 0.8364055\n",
            "Epoch: 118, Train Loss: 0.0015626, Train Acc: 0.9997438, Test Acc: 0.8433180\n",
            "Epoch: 119, Train Loss: 0.0046733, Train Acc: 0.9982065, Test Acc: 0.8479263\n",
            "Epoch: 120, Train Loss: 0.0063752, Train Acc: 0.9982065, Test Acc: 0.8410138\n",
            "--------\n",
            "Best Test Acc:   0.8398617511520737_0.019720325770303807, Epoch: 104\n",
            "Best Train Loss: 0.0023683088459720628_0.0004940490273152246\n",
            "Best Train Acc:  0.9995516269536254_0.0002792018283077584\n",
            "FOLD4, Mutagenicity, 1668873489/_rhoE0.5P18I1_GELU_X_6_256_0_0.001_50_0.6_B64S0W8, BT=0.8398617511520737_0.019720325770303807, BE=104, ID=\n",
            "train_set : test_set = %d : %d 3903 434\n",
            "-------- FOLD4 DATASET=Mutagenicity, COMMIT_ID=\n",
            "#Params: 1332482\n",
            "Epoch: 001, Train Loss: 0.5798540, Train Acc: 0.6951063, Test Acc: 0.6866359\n",
            "Epoch: 002, Train Loss: 0.6183031, Train Acc: 0.7030489, Test Acc: 0.7211982\n",
            "Epoch: 003, Train Loss: 0.5011903, Train Acc: 0.7606969, Test Acc: 0.7741935\n",
            "Epoch: 004, Train Loss: 0.4796862, Train Acc: 0.7776070, Test Acc: 0.7834101\n",
            "Epoch: 005, Train Loss: 0.5340207, Train Acc: 0.7417371, Test Acc: 0.7511521\n",
            "Epoch: 006, Train Loss: 0.4525056, Train Acc: 0.8006662, Test Acc: 0.8156682\n",
            "Epoch: 007, Train Loss: 0.4741522, Train Acc: 0.7781194, Test Acc: 0.7811060\n",
            "Epoch: 008, Train Loss: 0.4426920, Train Acc: 0.8004099, Test Acc: 0.8248848\n",
            "Epoch: 009, Train Loss: 0.4342400, Train Acc: 0.8034845, Test Acc: 0.8064516\n",
            "Epoch: 010, Train Loss: 0.4435434, Train Acc: 0.7916987, Test Acc: 0.8225806\n",
            "Epoch: 011, Train Loss: 0.4218433, Train Acc: 0.8127082, Test Acc: 0.7972350\n",
            "Epoch: 012, Train Loss: 0.4349690, Train Acc: 0.7957981, Test Acc: 0.7741935\n",
            "Epoch: 013, Train Loss: 0.3817698, Train Acc: 0.8380733, Test Acc: 0.8410138\n",
            "Epoch: 014, Train Loss: 0.3778125, Train Acc: 0.8393543, Test Acc: 0.8341014\n",
            "Epoch: 015, Train Loss: 0.3656682, Train Acc: 0.8378171, Test Acc: 0.8294931\n",
            "Epoch: 016, Train Loss: 0.4118668, Train Acc: 0.8247502, Test Acc: 0.8156682\n",
            "Epoch: 017, Train Loss: 0.3701221, Train Acc: 0.8375609, Test Acc: 0.8248848\n",
            "Epoch: 018, Train Loss: 0.3792262, Train Acc: 0.8367922, Test Acc: 0.8248848\n",
            "Epoch: 019, Train Loss: 0.4125187, Train Acc: 0.8093774, Test Acc: 0.8064516\n",
            "Epoch: 020, Train Loss: 0.3503133, Train Acc: 0.8539585, Test Acc: 0.8317972\n",
            "Epoch: 021, Train Loss: 0.3809490, Train Acc: 0.8275685, Test Acc: 0.8433180\n",
            "Epoch: 022, Train Loss: 0.3115334, Train Acc: 0.8703561, Test Acc: 0.8294931\n",
            "Epoch: 023, Train Loss: 0.3104395, Train Acc: 0.8731745, Test Acc: 0.8456221\n",
            "Epoch: 024, Train Loss: 0.3100339, Train Acc: 0.8695875, Test Acc: 0.8248848\n",
            "Epoch: 025, Train Loss: 0.3039972, Train Acc: 0.8759928, Test Acc: 0.8364055\n",
            "Epoch: 026, Train Loss: 0.3036376, Train Acc: 0.8700999, Test Acc: 0.8225806\n",
            "Epoch: 027, Train Loss: 0.2622039, Train Acc: 0.8926467, Test Acc: 0.8456221\n",
            "Epoch: 028, Train Loss: 0.2808325, Train Acc: 0.8800922, Test Acc: 0.8433180\n",
            "Epoch: 029, Train Loss: 0.2567253, Train Acc: 0.8967461, Test Acc: 0.8479263\n",
            "Epoch: 030, Train Loss: 0.2694872, Train Acc: 0.8911094, Test Acc: 0.8110599\n",
            "Epoch: 031, Train Loss: 0.2621862, Train Acc: 0.8949526, Test Acc: 0.8341014\n",
            "Epoch: 032, Train Loss: 0.3358590, Train Acc: 0.8603638, Test Acc: 0.8202765\n",
            "Epoch: 033, Train Loss: 0.2429580, Train Acc: 0.8964899, Test Acc: 0.8294931\n",
            "Epoch: 034, Train Loss: 0.2348660, Train Acc: 0.9023828, Test Acc: 0.8294931\n",
            "Epoch: 035, Train Loss: 0.2293705, Train Acc: 0.9008455, Test Acc: 0.8364055\n",
            "Epoch: 036, Train Loss: 0.2056372, Train Acc: 0.9154497, Test Acc: 0.8364055\n",
            "Epoch: 037, Train Loss: 0.1826457, Train Acc: 0.9223674, Test Acc: 0.8456221\n",
            "Epoch: 038, Train Loss: 0.2034511, Train Acc: 0.9190366, Test Acc: 0.8571429\n",
            "Epoch: 039, Train Loss: 0.1906839, Train Acc: 0.9226236, Test Acc: 0.8364055\n",
            "Epoch: 040, Train Loss: 0.2068383, Train Acc: 0.9113502, Test Acc: 0.8202765\n",
            "Epoch: 041, Train Loss: 0.1683228, Train Acc: 0.9410710, Test Acc: 0.8087558\n",
            "Epoch: 042, Train Loss: 0.1743431, Train Acc: 0.9323597, Test Acc: 0.8110599\n",
            "Epoch: 043, Train Loss: 0.1785354, Train Acc: 0.9313349, Test Acc: 0.8317972\n",
            "Epoch: 044, Train Loss: 0.1842237, Train Acc: 0.9267230, Test Acc: 0.8156682\n",
            "Epoch: 045, Train Loss: 0.1388993, Train Acc: 0.9423520, Test Acc: 0.8110599\n",
            "Epoch: 046, Train Loss: 0.1805377, Train Acc: 0.9297976, Test Acc: 0.8410138\n",
            "Epoch: 047, Train Loss: 0.1182121, Train Acc: 0.9567000, Test Acc: 0.8410138\n",
            "Epoch: 048, Train Loss: 0.1332068, Train Acc: 0.9472201, Test Acc: 0.8317972\n",
            "Epoch: 049, Train Loss: 0.1368800, Train Acc: 0.9446580, Test Acc: 0.8271889\n",
            "Epoch: 050, Train Loss: 0.1080600, Train Acc: 0.9600307, Test Acc: 0.8341014\n",
            "Epoch: 051, Train Loss: 0.0876178, Train Acc: 0.9702793, Test Acc: 0.8341014\n",
            "Epoch: 052, Train Loss: 0.0544201, Train Acc: 0.9815527, Test Acc: 0.8225806\n",
            "Epoch: 053, Train Loss: 0.0561657, Train Acc: 0.9820651, Test Acc: 0.8525346\n",
            "Epoch: 054, Train Loss: 0.0518864, Train Acc: 0.9828337, Test Acc: 0.8248848\n",
            "Epoch: 055, Train Loss: 0.0608465, Train Acc: 0.9800154, Test Acc: 0.8433180\n",
            "Epoch: 056, Train Loss: 0.0620467, Train Acc: 0.9800154, Test Acc: 0.8410138\n",
            "Epoch: 057, Train Loss: 0.0521393, Train Acc: 0.9818089, Test Acc: 0.8433180\n",
            "Epoch: 058, Train Loss: 0.0807791, Train Acc: 0.9718166, Test Acc: 0.8341014\n",
            "Epoch: 059, Train Loss: 0.0455938, Train Acc: 0.9828337, Test Acc: 0.8225806\n",
            "Epoch: 060, Train Loss: 0.0482344, Train Acc: 0.9833461, Test Acc: 0.8433180\n",
            "Epoch: 061, Train Loss: 0.0553614, Train Acc: 0.9805278, Test Acc: 0.8479263\n",
            "Epoch: 062, Train Loss: 0.0440701, Train Acc: 0.9830899, Test Acc: 0.8248848\n",
            "Epoch: 063, Train Loss: 0.1146731, Train Acc: 0.9567000, Test Acc: 0.8110599\n",
            "Epoch: 064, Train Loss: 0.0525151, Train Acc: 0.9843710, Test Acc: 0.8594470\n",
            "Epoch: 065, Train Loss: 0.0586599, Train Acc: 0.9818089, Test Acc: 0.8341014\n",
            "Epoch: 066, Train Loss: 0.0370431, Train Acc: 0.9861645, Test Acc: 0.8341014\n",
            "Epoch: 067, Train Loss: 0.0699701, Train Acc: 0.9707917, Test Acc: 0.7972350\n",
            "Epoch: 068, Train Loss: 0.0303920, Train Acc: 0.9897515, Test Acc: 0.8364055\n",
            "Epoch: 069, Train Loss: 0.0395655, Train Acc: 0.9879580, Test Acc: 0.8179724\n",
            "Epoch: 070, Train Loss: 0.0335996, Train Acc: 0.9889828, Test Acc: 0.8248848\n",
            "Epoch: 071, Train Loss: 0.0767259, Train Acc: 0.9700231, Test Acc: 0.8294931\n",
            "Epoch: 072, Train Loss: 0.0320266, Train Acc: 0.9897515, Test Acc: 0.8294931\n",
            "Epoch: 073, Train Loss: 0.0327101, Train Acc: 0.9884704, Test Acc: 0.8294931\n",
            "Epoch: 074, Train Loss: 0.0881094, Train Acc: 0.9705355, Test Acc: 0.7926267\n",
            "Epoch: 075, Train Loss: 0.0233607, Train Acc: 0.9915450, Test Acc: 0.8202765\n",
            "Epoch: 076, Train Loss: 0.0409848, Train Acc: 0.9843710, Test Acc: 0.8456221\n",
            "Epoch: 077, Train Loss: 0.0351500, Train Acc: 0.9894953, Test Acc: 0.8341014\n",
            "Epoch: 078, Train Loss: 0.0274539, Train Acc: 0.9905201, Test Acc: 0.8271889\n",
            "Epoch: 079, Train Loss: 0.0194359, Train Acc: 0.9933385, Test Acc: 0.8479263\n",
            "Epoch: 080, Train Loss: 0.0616452, Train Acc: 0.9764284, Test Acc: 0.8341014\n",
            "Epoch: 081, Train Loss: 0.0705869, Train Acc: 0.9730976, Test Acc: 0.8110599\n",
            "Epoch: 082, Train Loss: 0.0216124, Train Acc: 0.9923136, Test Acc: 0.8225806\n",
            "Epoch: 083, Train Loss: 0.0241925, Train Acc: 0.9912888, Test Acc: 0.8433180\n",
            "Epoch: 084, Train Loss: 0.0263135, Train Acc: 0.9928260, Test Acc: 0.8364055\n",
            "Epoch: 085, Train Loss: 0.0315732, Train Acc: 0.9889828, Test Acc: 0.8364055\n",
            "Epoch: 086, Train Loss: 0.0419366, Train Acc: 0.9884704, Test Acc: 0.8433180\n",
            "Epoch: 087, Train Loss: 0.0292691, Train Acc: 0.9910325, Test Acc: 0.8341014\n",
            "Epoch: 088, Train Loss: 0.0306733, Train Acc: 0.9892390, Test Acc: 0.8341014\n",
            "Epoch: 089, Train Loss: 0.0593781, Train Acc: 0.9807840, Test Acc: 0.8317972\n",
            "Epoch: 090, Train Loss: 0.0270588, Train Acc: 0.9918012, Test Acc: 0.8156682\n",
            "Epoch: 091, Train Loss: 0.0300187, Train Acc: 0.9897515, Test Acc: 0.8479263\n",
            "Epoch: 092, Train Loss: 0.0200625, Train Acc: 0.9935947, Test Acc: 0.8317972\n",
            "Epoch: 093, Train Loss: 0.0168228, Train Acc: 0.9953882, Test Acc: 0.8410138\n",
            "Epoch: 094, Train Loss: 0.0214260, Train Acc: 0.9933385, Test Acc: 0.8433180\n",
            "Epoch: 095, Train Loss: 0.0505462, Train Acc: 0.9797592, Test Acc: 0.8202765\n",
            "Epoch: 096, Train Loss: 0.0243420, Train Acc: 0.9918012, Test Acc: 0.8525346\n",
            "Epoch: 097, Train Loss: 0.0097282, Train Acc: 0.9974379, Test Acc: 0.8387097\n",
            "Epoch: 098, Train Loss: 0.0336300, Train Acc: 0.9897515, Test Acc: 0.8271889\n",
            "Epoch: 099, Train Loss: 0.0330530, Train Acc: 0.9902639, Test Acc: 0.8548387\n",
            "Epoch: 100, Train Loss: 0.0143710, Train Acc: 0.9966692, Test Acc: 0.8202765\n",
            "Epoch: 101, Train Loss: 0.0050423, Train Acc: 0.9992314, Test Acc: 0.8271889\n",
            "Epoch: 102, Train Loss: 0.0044181, Train Acc: 0.9992314, Test Acc: 0.8410138\n",
            "Epoch: 103, Train Loss: 0.0070957, Train Acc: 0.9974379, Test Acc: 0.8317972\n",
            "Epoch: 104, Train Loss: 0.0044071, Train Acc: 0.9989751, Test Acc: 0.8341014\n",
            "Epoch: 105, Train Loss: 0.0074434, Train Acc: 0.9982065, Test Acc: 0.8364055\n",
            "Epoch: 106, Train Loss: 0.0046363, Train Acc: 0.9984627, Test Acc: 0.8387097\n",
            "Epoch: 107, Train Loss: 0.0041782, Train Acc: 0.9994876, Test Acc: 0.8525346\n",
            "Epoch: 108, Train Loss: 0.0028287, Train Acc: 0.9989751, Test Acc: 0.8317972\n",
            "Epoch: 109, Train Loss: 0.0031462, Train Acc: 0.9994876, Test Acc: 0.8133641\n",
            "Epoch: 110, Train Loss: 0.0013387, Train Acc: 0.9997438, Test Acc: 0.8317972\n",
            "Epoch: 111, Train Loss: 0.0113408, Train Acc: 0.9961568, Test Acc: 0.8364055\n",
            "Epoch: 112, Train Loss: 0.0064488, Train Acc: 0.9976941, Test Acc: 0.8479263\n",
            "Epoch: 113, Train Loss: 0.0141212, Train Acc: 0.9966692, Test Acc: 0.8271889\n",
            "Epoch: 114, Train Loss: 0.0177329, Train Acc: 0.9966692, Test Acc: 0.8317972\n",
            "Epoch: 115, Train Loss: 0.0144330, Train Acc: 0.9974379, Test Acc: 0.8387097\n",
            "Epoch: 116, Train Loss: 0.0047556, Train Acc: 0.9989751, Test Acc: 0.8479263\n",
            "Epoch: 117, Train Loss: 0.0035111, Train Acc: 0.9987189, Test Acc: 0.8410138\n",
            "Epoch: 118, Train Loss: 0.0023447, Train Acc: 0.9997438, Test Acc: 0.8317972\n",
            "Epoch: 119, Train Loss: 0.0049925, Train Acc: 0.9987189, Test Acc: 0.8341014\n",
            "Epoch: 120, Train Loss: 0.0065183, Train Acc: 0.9992314, Test Acc: 0.8456221\n",
            "--------\n",
            "Best Test Acc:   0.8396313364055299_0.018467709594022004, Epoch: 98\n",
            "Best Train Loss: 0.002363590173731971_0.00044199164764023916\n",
            "Best Train Acc:  0.9995388162951576_0.0002510366121223004\n",
            "FOLD5, Mutagenicity, 1668873489/_rhoE0.5P18I1_GELU_X_6_256_0_0.001_50_0.6_B64S0W8, BT=0.8396313364055299_0.018467709594022004, BE=98, ID=\n",
            "train_set : test_set = %d : %d 3903 434\n",
            "-------- FOLD5 DATASET=Mutagenicity, COMMIT_ID=\n",
            "#Params: 1332482\n",
            "Epoch: 001, Train Loss: 0.5737399, Train Acc: 0.6907507, Test Acc: 0.7165899\n",
            "Epoch: 002, Train Loss: 0.5217537, Train Acc: 0.7486549, Test Acc: 0.7626728\n",
            "Epoch: 003, Train Loss: 0.5391466, Train Acc: 0.7330259, Test Acc: 0.7419355\n",
            "Epoch: 004, Train Loss: 0.4935630, Train Acc: 0.7814502, Test Acc: 0.7695853\n",
            "Epoch: 005, Train Loss: 0.5365640, Train Acc: 0.7450679, Test Acc: 0.7373272\n",
            "Epoch: 006, Train Loss: 0.4489872, Train Acc: 0.7965667, Test Acc: 0.8018433\n",
            "Epoch: 007, Train Loss: 0.4945175, Train Acc: 0.7747886, Test Acc: 0.7511521\n",
            "Epoch: 008, Train Loss: 0.4370058, Train Acc: 0.8114271, Test Acc: 0.8041475\n",
            "Epoch: 009, Train Loss: 0.4716820, Train Acc: 0.7837561, Test Acc: 0.7718894\n",
            "Epoch: 010, Train Loss: 0.4976181, Train Acc: 0.7568537, Test Acc: 0.7626728\n",
            "Epoch: 011, Train Loss: 0.4431219, Train Acc: 0.7891366, Test Acc: 0.7557604\n",
            "Epoch: 012, Train Loss: 0.3766170, Train Acc: 0.8390981, Test Acc: 0.8133641\n",
            "Epoch: 013, Train Loss: 0.4064456, Train Acc: 0.8201384, Test Acc: 0.8179724\n",
            "Epoch: 014, Train Loss: 0.3677070, Train Acc: 0.8403792, Test Acc: 0.8294931\n",
            "Epoch: 015, Train Loss: 0.3617391, Train Acc: 0.8475532, Test Acc: 0.8087558\n",
            "Epoch: 016, Train Loss: 0.3717346, Train Acc: 0.8414040, Test Acc: 0.8410138\n",
            "Epoch: 017, Train Loss: 0.3897151, Train Acc: 0.8316679, Test Acc: 0.8041475\n",
            "Epoch: 018, Train Loss: 0.3533289, Train Acc: 0.8455035, Test Acc: 0.8317972\n",
            "Epoch: 019, Train Loss: 0.3482349, Train Acc: 0.8462721, Test Acc: 0.8087558\n",
            "Epoch: 020, Train Loss: 0.3731871, Train Acc: 0.8378171, Test Acc: 0.8110599\n",
            "Epoch: 021, Train Loss: 0.3367684, Train Acc: 0.8593390, Test Acc: 0.8271889\n",
            "Epoch: 022, Train Loss: 0.3434778, Train Acc: 0.8554958, Test Acc: 0.8317972\n",
            "Epoch: 023, Train Loss: 0.3283290, Train Acc: 0.8542147, Test Acc: 0.8018433\n",
            "Epoch: 024, Train Loss: 0.3755219, Train Acc: 0.8293620, Test Acc: 0.7972350\n",
            "Epoch: 025, Train Loss: 0.3356101, Train Acc: 0.8547271, Test Acc: 0.8087558\n",
            "Epoch: 026, Train Loss: 0.2943864, Train Acc: 0.8829106, Test Acc: 0.8248848\n",
            "Epoch: 027, Train Loss: 0.2806092, Train Acc: 0.8780425, Test Acc: 0.8433180\n",
            "Epoch: 028, Train Loss: 0.2695107, Train Acc: 0.8872662, Test Acc: 0.8525346\n",
            "Epoch: 029, Train Loss: 0.3315430, Train Acc: 0.8572893, Test Acc: 0.7995392\n",
            "Epoch: 030, Train Loss: 0.2639162, Train Acc: 0.8867538, Test Acc: 0.8456221\n",
            "Epoch: 031, Train Loss: 0.2453318, Train Acc: 0.8975147, Test Acc: 0.8571429\n",
            "Epoch: 032, Train Loss: 0.2739757, Train Acc: 0.8913656, Test Acc: 0.8248848\n",
            "Epoch: 033, Train Loss: 0.2611035, Train Acc: 0.8852165, Test Acc: 0.8202765\n",
            "Epoch: 034, Train Loss: 0.2875535, Train Acc: 0.8780425, Test Acc: 0.8156682\n",
            "Epoch: 035, Train Loss: 0.2354895, Train Acc: 0.9023828, Test Acc: 0.8202765\n",
            "Epoch: 036, Train Loss: 0.3231301, Train Acc: 0.8667692, Test Acc: 0.8018433\n",
            "Epoch: 037, Train Loss: 0.2667417, Train Acc: 0.8829106, Test Acc: 0.7718894\n",
            "Epoch: 038, Train Loss: 0.2411749, Train Acc: 0.9003331, Test Acc: 0.8133641\n",
            "Epoch: 039, Train Loss: 0.2007925, Train Acc: 0.9154497, Test Acc: 0.8479263\n",
            "Epoch: 040, Train Loss: 0.2150132, Train Acc: 0.9131437, Test Acc: 0.8294931\n",
            "Epoch: 041, Train Loss: 0.1717811, Train Acc: 0.9310787, Test Acc: 0.8341014\n",
            "Epoch: 042, Train Loss: 0.1844277, Train Acc: 0.9269792, Test Acc: 0.8317972\n",
            "Epoch: 043, Train Loss: 0.2058166, Train Acc: 0.9095568, Test Acc: 0.8364055\n",
            "Epoch: 044, Train Loss: 0.1639137, Train Acc: 0.9328721, Test Acc: 0.8548387\n",
            "Epoch: 045, Train Loss: 0.1973165, Train Acc: 0.9241609, Test Acc: 0.8271889\n",
            "Epoch: 046, Train Loss: 0.1633986, Train Acc: 0.9379964, Test Acc: 0.8410138\n",
            "Epoch: 047, Train Loss: 0.1967407, Train Acc: 0.9167307, Test Acc: 0.8202765\n",
            "Epoch: 048, Train Loss: 0.1924008, Train Acc: 0.9272355, Test Acc: 0.8387097\n",
            "Epoch: 049, Train Loss: 0.1531003, Train Acc: 0.9354343, Test Acc: 0.8387097\n",
            "Epoch: 050, Train Loss: 0.1445192, Train Acc: 0.9408148, Test Acc: 0.8271889\n",
            "Epoch: 051, Train Loss: 0.0846829, Train Acc: 0.9638739, Test Acc: 0.8479263\n",
            "Epoch: 052, Train Loss: 0.0862966, Train Acc: 0.9682296, Test Acc: 0.8317972\n",
            "Epoch: 053, Train Loss: 0.0780963, Train Acc: 0.9692544, Test Acc: 0.8617512\n",
            "Epoch: 054, Train Loss: 0.0821728, Train Acc: 0.9715603, Test Acc: 0.8502304\n",
            "Epoch: 055, Train Loss: 0.0793164, Train Acc: 0.9702793, Test Acc: 0.8502304\n",
            "Epoch: 056, Train Loss: 0.0671361, Train Acc: 0.9764284, Test Acc: 0.8479263\n",
            "Epoch: 057, Train Loss: 0.0815501, Train Acc: 0.9659236, Test Acc: 0.8801843\n",
            "Epoch: 058, Train Loss: 0.0717699, Train Acc: 0.9761722, Test Acc: 0.8387097\n",
            "Epoch: 059, Train Loss: 0.0703860, Train Acc: 0.9743787, Test Acc: 0.8479263\n",
            "Epoch: 060, Train Loss: 0.0663022, Train Acc: 0.9730976, Test Acc: 0.8433180\n",
            "Epoch: 061, Train Loss: 0.0937395, Train Acc: 0.9666923, Test Acc: 0.8387097\n",
            "Epoch: 062, Train Loss: 0.0817351, Train Acc: 0.9692544, Test Acc: 0.8248848\n",
            "Epoch: 063, Train Loss: 0.0681274, Train Acc: 0.9789905, Test Acc: 0.8433180\n",
            "Epoch: 064, Train Loss: 0.0646687, Train Acc: 0.9764284, Test Acc: 0.8341014\n",
            "Epoch: 065, Train Loss: 0.0487953, Train Acc: 0.9823213, Test Acc: 0.8479263\n",
            "Epoch: 066, Train Loss: 0.0988769, Train Acc: 0.9605432, Test Acc: 0.8294931\n",
            "Epoch: 067, Train Loss: 0.0547581, Train Acc: 0.9828337, Test Acc: 0.8410138\n",
            "Epoch: 068, Train Loss: 0.0541462, Train Acc: 0.9800154, Test Acc: 0.8525346\n",
            "Epoch: 069, Train Loss: 0.0562190, Train Acc: 0.9805278, Test Acc: 0.8548387\n",
            "Epoch: 070, Train Loss: 0.0476899, Train Acc: 0.9818089, Test Acc: 0.8271889\n",
            "Epoch: 071, Train Loss: 0.0545467, Train Acc: 0.9797592, Test Acc: 0.8387097\n",
            "Epoch: 072, Train Loss: 0.0663493, Train Acc: 0.9743787, Test Acc: 0.8317972\n",
            "Epoch: 073, Train Loss: 0.0279917, Train Acc: 0.9910325, Test Acc: 0.8456221\n",
            "Epoch: 074, Train Loss: 0.0413130, Train Acc: 0.9815527, Test Acc: 0.8410138\n",
            "Epoch: 075, Train Loss: 0.0608466, Train Acc: 0.9782219, Test Acc: 0.8548387\n",
            "Epoch: 076, Train Loss: 0.0393010, Train Acc: 0.9838586, Test Acc: 0.8502304\n",
            "Epoch: 077, Train Loss: 0.0692130, Train Acc: 0.9718166, Test Acc: 0.8571429\n",
            "Epoch: 078, Train Loss: 0.0500375, Train Acc: 0.9812964, Test Acc: 0.8548387\n",
            "Epoch: 079, Train Loss: 0.0642136, Train Acc: 0.9748911, Test Acc: 0.8479263\n",
            "Epoch: 080, Train Loss: 0.0535388, Train Acc: 0.9807840, Test Acc: 0.8364055\n",
            "Epoch: 081, Train Loss: 0.0339709, Train Acc: 0.9884704, Test Acc: 0.8571429\n",
            "Epoch: 082, Train Loss: 0.0678351, Train Acc: 0.9736100, Test Acc: 0.8456221\n",
            "Epoch: 083, Train Loss: 0.0314527, Train Acc: 0.9884704, Test Acc: 0.8502304\n",
            "Epoch: 084, Train Loss: 0.0338652, Train Acc: 0.9892390, Test Acc: 0.8410138\n",
            "Epoch: 085, Train Loss: 0.0572960, Train Acc: 0.9759160, Test Acc: 0.8433180\n",
            "Epoch: 086, Train Loss: 0.0211593, Train Acc: 0.9933385, Test Acc: 0.8456221\n",
            "Epoch: 087, Train Loss: 0.0247600, Train Acc: 0.9928260, Test Acc: 0.8617512\n",
            "Epoch: 088, Train Loss: 0.0287618, Train Acc: 0.9902639, Test Acc: 0.8479263\n",
            "Epoch: 089, Train Loss: 0.0179841, Train Acc: 0.9951319, Test Acc: 0.8640553\n",
            "Epoch: 090, Train Loss: 0.0702199, Train Acc: 0.9705355, Test Acc: 0.8341014\n",
            "Epoch: 091, Train Loss: 0.0280309, Train Acc: 0.9902639, Test Acc: 0.8502304\n",
            "Epoch: 092, Train Loss: 0.0382004, Train Acc: 0.9892390, Test Acc: 0.8594470\n",
            "Epoch: 093, Train Loss: 0.0181850, Train Acc: 0.9941071, Test Acc: 0.8479263\n",
            "Epoch: 094, Train Loss: 0.0158764, Train Acc: 0.9953882, Test Acc: 0.8502304\n",
            "Epoch: 095, Train Loss: 0.0299994, Train Acc: 0.9907763, Test Acc: 0.8479263\n",
            "Epoch: 096, Train Loss: 0.0320713, Train Acc: 0.9905201, Test Acc: 0.8341014\n",
            "Epoch: 097, Train Loss: 0.0344670, Train Acc: 0.9902639, Test Acc: 0.8364055\n",
            "Epoch: 098, Train Loss: 0.0225889, Train Acc: 0.9930822, Test Acc: 0.8525346\n",
            "Epoch: 099, Train Loss: 0.0241691, Train Acc: 0.9938509, Test Acc: 0.8571429\n",
            "Epoch: 100, Train Loss: 0.0152418, Train Acc: 0.9953882, Test Acc: 0.8548387\n",
            "Epoch: 101, Train Loss: 0.0053228, Train Acc: 0.9989751, Test Acc: 0.8594470\n",
            "Epoch: 102, Train Loss: 0.0041329, Train Acc: 0.9992314, Test Acc: 0.8663594\n",
            "Epoch: 103, Train Loss: 0.0041250, Train Acc: 0.9992314, Test Acc: 0.8502304\n",
            "Epoch: 104, Train Loss: 0.0054663, Train Acc: 0.9987189, Test Acc: 0.8571429\n",
            "Epoch: 105, Train Loss: 0.0044683, Train Acc: 0.9989751, Test Acc: 0.8686636\n",
            "Epoch: 106, Train Loss: 0.0061125, Train Acc: 0.9987189, Test Acc: 0.8617512\n",
            "Epoch: 107, Train Loss: 0.0040394, Train Acc: 0.9992314, Test Acc: 0.8594470\n",
            "Epoch: 108, Train Loss: 0.0171092, Train Acc: 0.9953882, Test Acc: 0.8479263\n",
            "Epoch: 109, Train Loss: 0.0076492, Train Acc: 0.9982065, Test Acc: 0.8548387\n",
            "Epoch: 110, Train Loss: 0.0028672, Train Acc: 0.9992314, Test Acc: 0.8548387\n",
            "Epoch: 111, Train Loss: 0.0041307, Train Acc: 0.9987189, Test Acc: 0.8594470\n",
            "Epoch: 112, Train Loss: 0.0062266, Train Acc: 0.9982065, Test Acc: 0.8617512\n",
            "Epoch: 113, Train Loss: 0.0040101, Train Acc: 0.9992314, Test Acc: 0.8617512\n",
            "Epoch: 114, Train Loss: 0.0038522, Train Acc: 0.9992314, Test Acc: 0.8456221\n",
            "Epoch: 115, Train Loss: 0.0099086, Train Acc: 0.9969254, Test Acc: 0.8341014\n",
            "Epoch: 116, Train Loss: 0.0054952, Train Acc: 0.9984627, Test Acc: 0.8502304\n",
            "Epoch: 117, Train Loss: 0.0064015, Train Acc: 0.9987189, Test Acc: 0.8387097\n",
            "Epoch: 118, Train Loss: 0.0052568, Train Acc: 0.9982065, Test Acc: 0.8410138\n",
            "Epoch: 119, Train Loss: 0.0027569, Train Acc: 0.9994876, Test Acc: 0.8594470\n",
            "Epoch: 120, Train Loss: 0.0063288, Train Acc: 0.9984627, Test Acc: 0.8617512\n",
            "--------\n",
            "Best Test Acc:   0.8440860215053764_0.0195362478371381, Epoch: 104\n",
            "Best Train Loss: 0.002845798383957342_0.0011512692867136783\n",
            "Best Train Acc:  0.9993167648817151_0.0005468549182195616\n",
            "FOLD6, Mutagenicity, 1668873489/_rhoE0.5P18I1_GELU_X_6_256_0_0.001_50_0.6_B64S0W8, BT=0.8440860215053764_0.0195362478371381, BE=104, ID=\n",
            "train_set : test_set = %d : %d 3903 434\n",
            "-------- FOLD6 DATASET=Mutagenicity, COMMIT_ID=\n",
            "#Params: 1332482\n",
            "Epoch: 001, Train Loss: 0.7000204, Train Acc: 0.5472713, Test Acc: 0.5576037\n",
            "Epoch: 002, Train Loss: 0.5295307, Train Acc: 0.7378939, Test Acc: 0.7465438\n",
            "Epoch: 003, Train Loss: 0.6081287, Train Acc: 0.7104791, Test Acc: 0.7442396\n",
            "Epoch: 004, Train Loss: 0.4726886, Train Acc: 0.7916987, Test Acc: 0.7811060\n",
            "Epoch: 005, Train Loss: 0.4643366, Train Acc: 0.7963105, Test Acc: 0.7949309\n",
            "Epoch: 006, Train Loss: 0.5503579, Train Acc: 0.7255957, Test Acc: 0.7119816\n",
            "Epoch: 007, Train Loss: 0.4929619, Train Acc: 0.7794005, Test Acc: 0.7442396\n",
            "Epoch: 008, Train Loss: 0.4527025, Train Acc: 0.7960543, Test Acc: 0.7764977\n",
            "Epoch: 009, Train Loss: 0.4150570, Train Acc: 0.8180886, Test Acc: 0.8133641\n",
            "Epoch: 010, Train Loss: 0.4473117, Train Acc: 0.7945170, Test Acc: 0.7903226\n",
            "Epoch: 011, Train Loss: 0.4278506, Train Acc: 0.8073277, Test Acc: 0.7649770\n",
            "Epoch: 012, Train Loss: 0.4188201, Train Acc: 0.8137330, Test Acc: 0.8110599\n",
            "Epoch: 013, Train Loss: 0.4125254, Train Acc: 0.8160389, Test Acc: 0.7949309\n",
            "Epoch: 014, Train Loss: 0.3953057, Train Acc: 0.8273123, Test Acc: 0.8179724\n",
            "Epoch: 015, Train Loss: 0.3808197, Train Acc: 0.8250064, Test Acc: 0.8018433\n",
            "Epoch: 016, Train Loss: 0.3770520, Train Acc: 0.8344863, Test Acc: 0.7949309\n",
            "Epoch: 017, Train Loss: 0.3545422, Train Acc: 0.8516526, Test Acc: 0.8248848\n",
            "Epoch: 018, Train Loss: 0.3776120, Train Acc: 0.8403792, Test Acc: 0.8110599\n",
            "Epoch: 019, Train Loss: 0.3501877, Train Acc: 0.8580579, Test Acc: 0.8018433\n",
            "Epoch: 020, Train Loss: 0.3616114, Train Acc: 0.8452472, Test Acc: 0.8271889\n",
            "Epoch: 021, Train Loss: 0.3551881, Train Acc: 0.8524212, Test Acc: 0.8202765\n",
            "Epoch: 022, Train Loss: 0.2979074, Train Acc: 0.8767615, Test Acc: 0.8433180\n",
            "Epoch: 023, Train Loss: 0.3366256, Train Acc: 0.8475532, Test Acc: 0.7880184\n",
            "Epoch: 024, Train Loss: 0.3167749, Train Acc: 0.8588265, Test Acc: 0.8341014\n",
            "Epoch: 025, Train Loss: 0.3007258, Train Acc: 0.8731745, Test Acc: 0.8271889\n",
            "Epoch: 026, Train Loss: 0.2935523, Train Acc: 0.8736869, Test Acc: 0.8064516\n",
            "Epoch: 027, Train Loss: 0.2764672, Train Acc: 0.8800922, Test Acc: 0.8294931\n",
            "Epoch: 028, Train Loss: 0.2771506, Train Acc: 0.8847041, Test Acc: 0.8133641\n",
            "Epoch: 029, Train Loss: 0.2815777, Train Acc: 0.8821419, Test Acc: 0.8225806\n",
            "Epoch: 030, Train Loss: 0.2438331, Train Acc: 0.9008455, Test Acc: 0.8433180\n",
            "Epoch: 031, Train Loss: 0.2443135, Train Acc: 0.8954650, Test Acc: 0.8225806\n",
            "Epoch: 032, Train Loss: 0.2821187, Train Acc: 0.8780425, Test Acc: 0.8110599\n",
            "Epoch: 033, Train Loss: 0.2325436, Train Acc: 0.9003331, Test Acc: 0.8179724\n",
            "Epoch: 034, Train Loss: 0.2555940, Train Acc: 0.8895721, Test Acc: 0.8133641\n",
            "Epoch: 035, Train Loss: 0.2134828, Train Acc: 0.9205739, Test Acc: 0.8225806\n",
            "Epoch: 036, Train Loss: 0.2118725, Train Acc: 0.9085319, Test Acc: 0.8317972\n",
            "Epoch: 037, Train Loss: 0.1803516, Train Acc: 0.9272355, Test Acc: 0.8387097\n",
            "Epoch: 038, Train Loss: 0.2393033, Train Acc: 0.8970023, Test Acc: 0.8387097\n",
            "Epoch: 039, Train Loss: 0.2033764, Train Acc: 0.9126313, Test Acc: 0.8110599\n",
            "Epoch: 040, Train Loss: 0.1861232, Train Acc: 0.9228798, Test Acc: 0.8087558\n",
            "Epoch: 041, Train Loss: 0.1713546, Train Acc: 0.9290290, Test Acc: 0.8341014\n",
            "Epoch: 042, Train Loss: 0.1891460, Train Acc: 0.9264668, Test Acc: 0.8341014\n",
            "Epoch: 043, Train Loss: 0.1949076, Train Acc: 0.9210863, Test Acc: 0.8248848\n",
            "Epoch: 044, Train Loss: 0.1616067, Train Acc: 0.9395337, Test Acc: 0.8317972\n",
            "Epoch: 045, Train Loss: 0.1514372, Train Acc: 0.9362029, Test Acc: 0.8248848\n",
            "Epoch: 046, Train Loss: 0.1764363, Train Acc: 0.9254420, Test Acc: 0.8202765\n",
            "Epoch: 047, Train Loss: 0.1632547, Train Acc: 0.9377402, Test Acc: 0.8387097\n",
            "Epoch: 048, Train Loss: 0.1599205, Train Acc: 0.9323597, Test Acc: 0.8179724\n",
            "Epoch: 049, Train Loss: 0.1267250, Train Acc: 0.9472201, Test Acc: 0.8294931\n",
            "Epoch: 050, Train Loss: 0.1244649, Train Acc: 0.9513195, Test Acc: 0.8548387\n",
            "Epoch: 051, Train Loss: 0.0678442, Train Acc: 0.9787343, Test Acc: 0.8179724\n",
            "Epoch: 052, Train Loss: 0.0636829, Train Acc: 0.9787343, Test Acc: 0.8271889\n",
            "Epoch: 053, Train Loss: 0.0612997, Train Acc: 0.9820651, Test Acc: 0.8387097\n",
            "Epoch: 054, Train Loss: 0.0450652, Train Acc: 0.9853958, Test Acc: 0.8179724\n",
            "Epoch: 055, Train Loss: 0.0514433, Train Acc: 0.9846272, Test Acc: 0.8317972\n",
            "Epoch: 056, Train Loss: 0.0653478, Train Acc: 0.9774532, Test Acc: 0.8156682\n",
            "Epoch: 057, Train Loss: 0.0416878, Train Acc: 0.9887266, Test Acc: 0.8271889\n",
            "Epoch: 058, Train Loss: 0.0444005, Train Acc: 0.9866769, Test Acc: 0.8271889\n",
            "Epoch: 059, Train Loss: 0.0576599, Train Acc: 0.9797592, Test Acc: 0.8294931\n",
            "Epoch: 060, Train Loss: 0.0855469, Train Acc: 0.9733538, Test Acc: 0.8617512\n",
            "Epoch: 061, Train Loss: 0.0607322, Train Acc: 0.9779657, Test Acc: 0.8202765\n",
            "Epoch: 062, Train Loss: 0.0400203, Train Acc: 0.9871893, Test Acc: 0.8456221\n",
            "Epoch: 063, Train Loss: 0.0458619, Train Acc: 0.9848834, Test Acc: 0.8271889\n",
            "Epoch: 064, Train Loss: 0.0307024, Train Acc: 0.9900077, Test Acc: 0.8133641\n",
            "Epoch: 065, Train Loss: 0.0462767, Train Acc: 0.9830899, Test Acc: 0.8387097\n",
            "Epoch: 066, Train Loss: 0.0797167, Train Acc: 0.9687420, Test Acc: 0.8387097\n",
            "Epoch: 067, Train Loss: 0.0408568, Train Acc: 0.9879580, Test Acc: 0.8433180\n",
            "Epoch: 068, Train Loss: 0.0298499, Train Acc: 0.9907763, Test Acc: 0.8410138\n",
            "Epoch: 069, Train Loss: 0.0227539, Train Acc: 0.9941071, Test Acc: 0.8202765\n",
            "Epoch: 070, Train Loss: 0.0424808, Train Acc: 0.9859083, Test Acc: 0.8433180\n",
            "Epoch: 071, Train Loss: 0.0275255, Train Acc: 0.9907763, Test Acc: 0.8294931\n",
            "Epoch: 072, Train Loss: 0.0317793, Train Acc: 0.9884704, Test Acc: 0.8271889\n",
            "Epoch: 073, Train Loss: 0.0459638, Train Acc: 0.9841148, Test Acc: 0.8087558\n",
            "Epoch: 074, Train Loss: 0.0291278, Train Acc: 0.9900077, Test Acc: 0.8341014\n",
            "Epoch: 075, Train Loss: 0.0423444, Train Acc: 0.9864207, Test Acc: 0.8225806\n",
            "Epoch: 076, Train Loss: 0.0232276, Train Acc: 0.9928260, Test Acc: 0.8387097\n",
            "Epoch: 077, Train Loss: 0.0415058, Train Acc: 0.9848834, Test Acc: 0.8271889\n",
            "Epoch: 078, Train Loss: 0.0238806, Train Acc: 0.9920574, Test Acc: 0.8456221\n",
            "Epoch: 079, Train Loss: 0.0296712, Train Acc: 0.9892390, Test Acc: 0.8571429\n",
            "Epoch: 080, Train Loss: 0.0352269, Train Acc: 0.9879580, Test Acc: 0.8317972\n",
            "Epoch: 081, Train Loss: 0.0157591, Train Acc: 0.9946195, Test Acc: 0.8364055\n",
            "Epoch: 082, Train Loss: 0.0291627, Train Acc: 0.9907763, Test Acc: 0.8387097\n",
            "Epoch: 083, Train Loss: 0.0244172, Train Acc: 0.9928260, Test Acc: 0.8133641\n",
            "Epoch: 084, Train Loss: 0.0226088, Train Acc: 0.9930822, Test Acc: 0.8317972\n",
            "Epoch: 085, Train Loss: 0.0159048, Train Acc: 0.9948757, Test Acc: 0.8364055\n",
            "Epoch: 086, Train Loss: 0.0354334, Train Acc: 0.9884704, Test Acc: 0.8433180\n",
            "Epoch: 087, Train Loss: 0.0199468, Train Acc: 0.9933385, Test Acc: 0.8479263\n",
            "Epoch: 088, Train Loss: 0.0363335, Train Acc: 0.9866769, Test Acc: 0.8248848\n",
            "Epoch: 089, Train Loss: 0.0288267, Train Acc: 0.9892390, Test Acc: 0.8248848\n",
            "Epoch: 090, Train Loss: 0.0235390, Train Acc: 0.9925698, Test Acc: 0.8410138\n",
            "Epoch: 091, Train Loss: 0.0412776, Train Acc: 0.9853958, Test Acc: 0.8156682\n",
            "Epoch: 092, Train Loss: 0.0180209, Train Acc: 0.9946195, Test Acc: 0.8410138\n",
            "Epoch: 093, Train Loss: 0.0134468, Train Acc: 0.9953882, Test Acc: 0.8456221\n",
            "Epoch: 094, Train Loss: 0.0219191, Train Acc: 0.9928260, Test Acc: 0.8502304\n",
            "Epoch: 095, Train Loss: 0.0104231, Train Acc: 0.9969254, Test Acc: 0.8456221\n",
            "Epoch: 096, Train Loss: 0.0145307, Train Acc: 0.9951319, Test Acc: 0.8341014\n",
            "Epoch: 097, Train Loss: 0.0134468, Train Acc: 0.9953882, Test Acc: 0.8317972\n",
            "Epoch: 098, Train Loss: 0.0341221, Train Acc: 0.9871893, Test Acc: 0.8341014\n",
            "Epoch: 099, Train Loss: 0.0353858, Train Acc: 0.9869331, Test Acc: 0.8387097\n",
            "Epoch: 100, Train Loss: 0.0204968, Train Acc: 0.9928260, Test Acc: 0.8502304\n",
            "Epoch: 101, Train Loss: 0.0039215, Train Acc: 0.9994876, Test Acc: 0.8410138\n",
            "Epoch: 102, Train Loss: 0.0030738, Train Acc: 0.9997438, Test Acc: 0.8525346\n",
            "Epoch: 103, Train Loss: 0.0082783, Train Acc: 0.9984627, Test Acc: 0.8387097\n",
            "Epoch: 104, Train Loss: 0.0040347, Train Acc: 0.9992314, Test Acc: 0.8456221\n",
            "Epoch: 105, Train Loss: 0.0019759, Train Acc: 1.0000000, Test Acc: 0.8433180\n",
            "Epoch: 106, Train Loss: 0.0020447, Train Acc: 0.9997438, Test Acc: 0.8479263\n",
            "Epoch: 107, Train Loss: 0.0055369, Train Acc: 0.9989751, Test Acc: 0.8525346\n",
            "Epoch: 108, Train Loss: 0.0089158, Train Acc: 0.9969254, Test Acc: 0.8433180\n",
            "Epoch: 109, Train Loss: 0.0044229, Train Acc: 0.9987189, Test Acc: 0.8317972\n",
            "Epoch: 110, Train Loss: 0.0042612, Train Acc: 0.9984627, Test Acc: 0.8341014\n",
            "Epoch: 111, Train Loss: 0.0028212, Train Acc: 0.9994876, Test Acc: 0.8456221\n",
            "Epoch: 112, Train Loss: 0.0023700, Train Acc: 0.9994876, Test Acc: 0.8364055\n",
            "Epoch: 113, Train Loss: 0.0024012, Train Acc: 0.9997438, Test Acc: 0.8341014\n",
            "Epoch: 114, Train Loss: 0.0019460, Train Acc: 0.9989751, Test Acc: 0.8364055\n",
            "Epoch: 115, Train Loss: 0.0091973, Train Acc: 0.9982065, Test Acc: 0.8317972\n",
            "Epoch: 116, Train Loss: 0.0100571, Train Acc: 0.9979503, Test Acc: 0.8294931\n",
            "Epoch: 117, Train Loss: 0.0172016, Train Acc: 0.9946195, Test Acc: 0.8502304\n",
            "Epoch: 118, Train Loss: 0.0034634, Train Acc: 0.9989751, Test Acc: 0.8456221\n",
            "Epoch: 119, Train Loss: 0.0055284, Train Acc: 0.9984627, Test Acc: 0.8387097\n",
            "Epoch: 120, Train Loss: 0.0017525, Train Acc: 1.0000000, Test Acc: 0.8456221\n",
            "--------\n",
            "Best Test Acc:   0.8443054641211323_0.017627949755096642, Epoch: 106\n",
            "Best Train Loss: 0.002934024144752243_0.0010875558959448325\n",
            "Best Train Acc:  0.9992679623732661_0.0005202104755884434\n",
            "FOLD7, Mutagenicity, 1668873489/_rhoE0.5P18I1_GELU_X_6_256_0_0.001_50_0.6_B64S0W8, BT=0.8443054641211323_0.017627949755096642, BE=106, ID=\n",
            "train_set : test_set = %d : %d 3904 433\n",
            "-------- FOLD7 DATASET=Mutagenicity, COMMIT_ID=\n",
            "#Params: 1332482\n",
            "Epoch: 001, Train Loss: 0.5890202, Train Acc: 0.6698258, Test Acc: 0.6951501\n",
            "Epoch: 002, Train Loss: 0.5206862, Train Acc: 0.7392418, Test Acc: 0.7344111\n",
            "Epoch: 003, Train Loss: 0.5095259, Train Acc: 0.7569160, Test Acc: 0.7551963\n",
            "Epoch: 004, Train Loss: 0.5308614, Train Acc: 0.7389857, Test Acc: 0.7251732\n",
            "Epoch: 005, Train Loss: 0.4692274, Train Acc: 0.7794570, Test Acc: 0.7690531\n",
            "Epoch: 006, Train Loss: 0.4826188, Train Acc: 0.7758709, Test Acc: 0.7321016\n",
            "Epoch: 007, Train Loss: 0.4545854, Train Acc: 0.7889344, Test Acc: 0.7621247\n",
            "Epoch: 008, Train Loss: 0.4200198, Train Acc: 0.8186475, Test Acc: 0.8013857\n",
            "Epoch: 009, Train Loss: 0.5085824, Train Acc: 0.7512807, Test Acc: 0.7251732\n",
            "Epoch: 010, Train Loss: 0.5159391, Train Acc: 0.7715164, Test Acc: 0.7528868\n",
            "Epoch: 011, Train Loss: 0.4315997, Train Acc: 0.8002049, Test Acc: 0.7759815\n",
            "Epoch: 012, Train Loss: 0.3949805, Train Acc: 0.8347848, Test Acc: 0.8152425\n",
            "Epoch: 013, Train Loss: 0.3652651, Train Acc: 0.8447746, Test Acc: 0.8060046\n",
            "Epoch: 014, Train Loss: 0.3804726, Train Acc: 0.8324795, Test Acc: 0.8060046\n",
            "Epoch: 015, Train Loss: 0.4007444, Train Acc: 0.8235143, Test Acc: 0.8036952\n",
            "Epoch: 016, Train Loss: 0.3474065, Train Acc: 0.8555328, Test Acc: 0.7990762\n",
            "Epoch: 017, Train Loss: 0.3680730, Train Acc: 0.8411885, Test Acc: 0.7898383\n",
            "Epoch: 018, Train Loss: 0.3678675, Train Acc: 0.8347848, Test Acc: 0.8083141\n",
            "Epoch: 019, Train Loss: 0.3609811, Train Acc: 0.8411885, Test Acc: 0.7990762\n",
            "Epoch: 020, Train Loss: 0.3246461, Train Acc: 0.8670594, Test Acc: 0.8106236\n",
            "Epoch: 021, Train Loss: 0.2887001, Train Acc: 0.8808914, Test Acc: 0.8337182\n",
            "Epoch: 022, Train Loss: 0.4197114, Train Acc: 0.8281250, Test Acc: 0.7829099\n",
            "Epoch: 023, Train Loss: 0.2864544, Train Acc: 0.8783299, Test Acc: 0.7898383\n",
            "Epoch: 024, Train Loss: 0.3330710, Train Acc: 0.8460553, Test Acc: 0.8106236\n",
            "Epoch: 025, Train Loss: 0.2955314, Train Acc: 0.8714139, Test Acc: 0.7829099\n",
            "Epoch: 026, Train Loss: 0.2500363, Train Acc: 0.8944672, Test Acc: 0.8129330\n",
            "Epoch: 027, Train Loss: 0.2594172, Train Acc: 0.8967725, Test Acc: 0.8106236\n",
            "Epoch: 028, Train Loss: 0.2771732, Train Acc: 0.8934426, Test Acc: 0.8083141\n",
            "Epoch: 029, Train Loss: 0.2573935, Train Acc: 0.8993340, Test Acc: 0.8013857\n",
            "Epoch: 030, Train Loss: 0.2589923, Train Acc: 0.8936988, Test Acc: 0.7944573\n",
            "Epoch: 031, Train Loss: 0.2983438, Train Acc: 0.8668033, Test Acc: 0.8036952\n",
            "Epoch: 032, Train Loss: 0.2392816, Train Acc: 0.9044570, Test Acc: 0.7990762\n",
            "Epoch: 033, Train Loss: 0.2301257, Train Acc: 0.9049693, Test Acc: 0.8060046\n",
            "Epoch: 034, Train Loss: 0.2175959, Train Acc: 0.9126537, Test Acc: 0.8244804\n",
            "Epoch: 035, Train Loss: 0.2389187, Train Acc: 0.9039447, Test Acc: 0.8198614\n",
            "Epoch: 036, Train Loss: 0.2594186, Train Acc: 0.8947234, Test Acc: 0.8244804\n",
            "Epoch: 037, Train Loss: 0.1863459, Train Acc: 0.9249488, Test Acc: 0.8152425\n",
            "Epoch: 038, Train Loss: 0.1524892, Train Acc: 0.9415984, Test Acc: 0.8383372\n",
            "Epoch: 039, Train Loss: 0.1835764, Train Acc: 0.9310963, Test Acc: 0.8152425\n",
            "Epoch: 040, Train Loss: 0.1953526, Train Acc: 0.9252049, Test Acc: 0.7852194\n",
            "Epoch: 041, Train Loss: 0.1571807, Train Acc: 0.9369877, Test Acc: 0.8198614\n",
            "Epoch: 042, Train Loss: 0.1801677, Train Acc: 0.9290471, Test Acc: 0.8013857\n",
            "Epoch: 043, Train Loss: 0.1646895, Train Acc: 0.9357070, Test Acc: 0.7898383\n",
            "Epoch: 044, Train Loss: 0.1253064, Train Acc: 0.9477459, Test Acc: 0.8244804\n",
            "Epoch: 045, Train Loss: 0.1531465, Train Acc: 0.9410861, Test Acc: 0.8083141\n",
            "Epoch: 046, Train Loss: 0.1745303, Train Acc: 0.9269980, Test Acc: 0.8106236\n",
            "Epoch: 047, Train Loss: 0.1339236, Train Acc: 0.9444160, Test Acc: 0.7898383\n",
            "Epoch: 048, Train Loss: 0.1653285, Train Acc: 0.9308402, Test Acc: 0.7944573\n",
            "Epoch: 049, Train Loss: 0.1201025, Train Acc: 0.9526127, Test Acc: 0.8129330\n",
            "Epoch: 050, Train Loss: 0.1317895, Train Acc: 0.9482582, Test Acc: 0.8129330\n",
            "Epoch: 051, Train Loss: 0.0623581, Train Acc: 0.9815574, Test Acc: 0.8290993\n",
            "Epoch: 052, Train Loss: 0.0493004, Train Acc: 0.9833504, Test Acc: 0.8452656\n",
            "Epoch: 053, Train Loss: 0.0546536, Train Acc: 0.9853996, Test Acc: 0.8221709\n",
            "Epoch: 054, Train Loss: 0.0486390, Train Acc: 0.9838627, Test Acc: 0.8198614\n",
            "Epoch: 055, Train Loss: 0.0441170, Train Acc: 0.9820697, Test Acc: 0.8383372\n",
            "Epoch: 056, Train Loss: 0.0468717, Train Acc: 0.9843750, Test Acc: 0.8290993\n",
            "Epoch: 057, Train Loss: 0.0558181, Train Acc: 0.9818135, Test Acc: 0.8129330\n",
            "Epoch: 058, Train Loss: 0.0753850, Train Acc: 0.9707992, Test Acc: 0.8152425\n",
            "Epoch: 059, Train Loss: 0.0633805, Train Acc: 0.9756660, Test Acc: 0.8244804\n",
            "Epoch: 060, Train Loss: 0.0451297, Train Acc: 0.9871926, Test Acc: 0.8360277\n",
            "Epoch: 061, Train Loss: 0.0470748, Train Acc: 0.9830943, Test Acc: 0.7967667\n",
            "Epoch: 062, Train Loss: 0.0464752, Train Acc: 0.9828381, Test Acc: 0.8129330\n",
            "Epoch: 063, Train Loss: 0.0675583, Train Acc: 0.9779713, Test Acc: 0.8152425\n",
            "Epoch: 064, Train Loss: 0.0449302, Train Acc: 0.9851434, Test Acc: 0.8129330\n",
            "Epoch: 065, Train Loss: 0.0542796, Train Acc: 0.9815574, Test Acc: 0.8175520\n",
            "Epoch: 066, Train Loss: 0.0467994, Train Acc: 0.9820697, Test Acc: 0.8083141\n",
            "Epoch: 067, Train Loss: 0.0381436, Train Acc: 0.9859119, Test Acc: 0.8290993\n",
            "Epoch: 068, Train Loss: 0.0621922, Train Acc: 0.9761783, Test Acc: 0.7875289\n",
            "Epoch: 069, Train Loss: 0.0281381, Train Acc: 0.9928279, Test Acc: 0.8152425\n",
            "Epoch: 070, Train Loss: 0.0305687, Train Acc: 0.9910348, Test Acc: 0.8290993\n",
            "Epoch: 071, Train Loss: 0.0229398, Train Acc: 0.9920594, Test Acc: 0.8267898\n",
            "Epoch: 072, Train Loss: 0.0453614, Train Acc: 0.9838627, Test Acc: 0.8244804\n",
            "Epoch: 073, Train Loss: 0.0514184, Train Acc: 0.9805328, Test Acc: 0.8175520\n",
            "Epoch: 074, Train Loss: 0.0297874, Train Acc: 0.9894980, Test Acc: 0.8337182\n",
            "Epoch: 075, Train Loss: 0.0371373, Train Acc: 0.9877049, Test Acc: 0.8083141\n",
            "Epoch: 076, Train Loss: 0.0232062, Train Acc: 0.9930840, Test Acc: 0.8314088\n",
            "Epoch: 077, Train Loss: 0.0409309, Train Acc: 0.9841189, Test Acc: 0.8013857\n",
            "Epoch: 078, Train Loss: 0.0535972, Train Acc: 0.9833504, Test Acc: 0.8244804\n",
            "Epoch: 079, Train Loss: 0.0311913, Train Acc: 0.9902664, Test Acc: 0.8221709\n",
            "Epoch: 080, Train Loss: 0.0333176, Train Acc: 0.9877049, Test Acc: 0.8221709\n",
            "Epoch: 081, Train Loss: 0.0350407, Train Acc: 0.9902664, Test Acc: 0.8106236\n",
            "Epoch: 082, Train Loss: 0.0173195, Train Acc: 0.9943648, Test Acc: 0.8360277\n",
            "Epoch: 083, Train Loss: 0.0191785, Train Acc: 0.9935963, Test Acc: 0.8060046\n",
            "Epoch: 084, Train Loss: 0.0189467, Train Acc: 0.9935963, Test Acc: 0.8314088\n",
            "Epoch: 085, Train Loss: 0.0249549, Train Acc: 0.9920594, Test Acc: 0.8290993\n",
            "Epoch: 086, Train Loss: 0.0161032, Train Acc: 0.9953893, Test Acc: 0.8244804\n",
            "Epoch: 087, Train Loss: 0.0309780, Train Acc: 0.9920594, Test Acc: 0.8129330\n",
            "Epoch: 088, Train Loss: 0.0440748, Train Acc: 0.9828381, Test Acc: 0.8198614\n",
            "Epoch: 089, Train Loss: 0.0184947, Train Acc: 0.9941086, Test Acc: 0.8267898\n",
            "Epoch: 090, Train Loss: 0.0312866, Train Acc: 0.9894980, Test Acc: 0.8244804\n",
            "Epoch: 091, Train Loss: 0.0910708, Train Acc: 0.9702869, Test Acc: 0.7829099\n",
            "Epoch: 092, Train Loss: 0.0254698, Train Acc: 0.9928279, Test Acc: 0.8267898\n",
            "Epoch: 093, Train Loss: 0.0191586, Train Acc: 0.9951332, Test Acc: 0.8360277\n",
            "Epoch: 094, Train Loss: 0.0193856, Train Acc: 0.9943648, Test Acc: 0.8152425\n",
            "Epoch: 095, Train Loss: 0.0303376, Train Acc: 0.9905225, Test Acc: 0.8267898\n",
            "Epoch: 096, Train Loss: 0.0100828, Train Acc: 0.9974385, Test Acc: 0.8267898\n",
            "Epoch: 097, Train Loss: 0.0342580, Train Acc: 0.9871926, Test Acc: 0.8198614\n",
            "Epoch: 098, Train Loss: 0.0126971, Train Acc: 0.9953893, Test Acc: 0.8221709\n",
            "Epoch: 099, Train Loss: 0.0275805, Train Acc: 0.9923156, Test Acc: 0.8244804\n",
            "Epoch: 100, Train Loss: 0.0779772, Train Acc: 0.9715676, Test Acc: 0.8060046\n",
            "Epoch: 101, Train Loss: 0.0074092, Train Acc: 0.9976947, Test Acc: 0.8198614\n",
            "Epoch: 102, Train Loss: 0.0057038, Train Acc: 0.9984631, Test Acc: 0.8152425\n",
            "Epoch: 103, Train Loss: 0.0034735, Train Acc: 0.9994877, Test Acc: 0.8175520\n",
            "Epoch: 104, Train Loss: 0.0024258, Train Acc: 0.9994877, Test Acc: 0.8175520\n",
            "Epoch: 105, Train Loss: 0.0060404, Train Acc: 0.9984631, Test Acc: 0.8106236\n",
            "Epoch: 106, Train Loss: 0.0032086, Train Acc: 0.9992316, Test Acc: 0.8314088\n",
            "Epoch: 107, Train Loss: 0.0037926, Train Acc: 0.9994877, Test Acc: 0.8314088\n",
            "Epoch: 108, Train Loss: 0.0110825, Train Acc: 0.9969262, Test Acc: 0.8244804\n",
            "Epoch: 109, Train Loss: 0.0042444, Train Acc: 0.9989754, Test Acc: 0.8290993\n",
            "Epoch: 110, Train Loss: 0.0067057, Train Acc: 0.9982070, Test Acc: 0.8244804\n",
            "Epoch: 111, Train Loss: 0.0032716, Train Acc: 0.9992316, Test Acc: 0.8106236\n",
            "Epoch: 112, Train Loss: 0.0049700, Train Acc: 0.9987193, Test Acc: 0.8221709\n",
            "Epoch: 113, Train Loss: 0.0026857, Train Acc: 0.9994877, Test Acc: 0.8290993\n",
            "Epoch: 114, Train Loss: 0.0017543, Train Acc: 0.9994877, Test Acc: 0.8244804\n",
            "Epoch: 115, Train Loss: 0.0035842, Train Acc: 0.9989754, Test Acc: 0.8221709\n",
            "Epoch: 116, Train Loss: 0.0041147, Train Acc: 0.9989754, Test Acc: 0.8175520\n",
            "Epoch: 117, Train Loss: 0.0117552, Train Acc: 0.9971824, Test Acc: 0.8152425\n",
            "Epoch: 118, Train Loss: 0.0034848, Train Acc: 0.9989754, Test Acc: 0.8360277\n",
            "Epoch: 119, Train Loss: 0.0069751, Train Acc: 0.9979508, Test Acc: 0.8129330\n",
            "Epoch: 120, Train Loss: 0.0042891, Train Acc: 0.9989754, Test Acc: 0.8106236\n",
            "--------\n",
            "Best Test Acc:   0.8426933781036813_0.017032125096147353, Epoch: 106\n",
            "Best Train Loss: 0.003002868189398663_0.0010334926781727914\n",
            "Best Train Acc:  0.999231393306116_0.000496137731132733\n",
            "FOLD8, Mutagenicity, 1668873489/_rhoE0.5P18I1_GELU_X_6_256_0_0.001_50_0.6_B64S0W8, BT=0.8426933781036813_0.017032125096147353, BE=106, ID=\n",
            "train_set : test_set = %d : %d 3904 433\n",
            "-------- FOLD8 DATASET=Mutagenicity, COMMIT_ID=\n",
            "#Params: 1332482\n",
            "Epoch: 001, Train Loss: 0.5725930, Train Acc: 0.6946721, Test Acc: 0.6859122\n",
            "Epoch: 002, Train Loss: 0.5190949, Train Acc: 0.7310451, Test Acc: 0.7274827\n",
            "Epoch: 003, Train Loss: 0.5065477, Train Acc: 0.7525615, Test Acc: 0.7621247\n",
            "Epoch: 004, Train Loss: 0.4931901, Train Acc: 0.7669057, Test Acc: 0.7667436\n",
            "Epoch: 005, Train Loss: 0.4794994, Train Acc: 0.7763832, Test Acc: 0.7505774\n",
            "Epoch: 006, Train Loss: 0.4481465, Train Acc: 0.7996926, Test Acc: 0.7898383\n",
            "Epoch: 007, Train Loss: 0.5193754, Train Acc: 0.7679303, Test Acc: 0.7690531\n",
            "Epoch: 008, Train Loss: 0.4463689, Train Acc: 0.7978996, Test Acc: 0.7667436\n",
            "Epoch: 009, Train Loss: 0.4615744, Train Acc: 0.7835553, Test Acc: 0.7551963\n",
            "Epoch: 010, Train Loss: 0.4737613, Train Acc: 0.7702357, Test Acc: 0.7413395\n",
            "Epoch: 011, Train Loss: 0.4361071, Train Acc: 0.8066086, Test Acc: 0.7852194\n",
            "Epoch: 012, Train Loss: 0.4541564, Train Acc: 0.8004611, Test Acc: 0.7967667\n",
            "Epoch: 013, Train Loss: 0.4089441, Train Acc: 0.8199283, Test Acc: 0.7782910\n",
            "Epoch: 014, Train Loss: 0.3857917, Train Acc: 0.8370902, Test Acc: 0.8129330\n",
            "Epoch: 015, Train Loss: 0.3703848, Train Acc: 0.8393955, Test Acc: 0.8036952\n",
            "Epoch: 016, Train Loss: 0.4144410, Train Acc: 0.8137807, Test Acc: 0.7829099\n",
            "Epoch: 017, Train Loss: 0.3636551, Train Acc: 0.8378586, Test Acc: 0.8106236\n",
            "Epoch: 018, Train Loss: 0.3681620, Train Acc: 0.8486168, Test Acc: 0.8083141\n",
            "Epoch: 019, Train Loss: 0.3459253, Train Acc: 0.8509221, Test Acc: 0.8129330\n",
            "Epoch: 020, Train Loss: 0.3972981, Train Acc: 0.8245389, Test Acc: 0.7990762\n",
            "Epoch: 021, Train Loss: 0.3229280, Train Acc: 0.8660348, Test Acc: 0.8290993\n",
            "Epoch: 022, Train Loss: 0.3183792, Train Acc: 0.8680840, Test Acc: 0.8013857\n",
            "Epoch: 023, Train Loss: 0.3266384, Train Acc: 0.8650102, Test Acc: 0.8244804\n",
            "Epoch: 024, Train Loss: 0.3330210, Train Acc: 0.8598873, Test Acc: 0.8106236\n",
            "Epoch: 025, Train Loss: 0.3199666, Train Acc: 0.8688525, Test Acc: 0.8267898\n",
            "Epoch: 026, Train Loss: 0.2700831, Train Acc: 0.8885758, Test Acc: 0.8314088\n",
            "Epoch: 027, Train Loss: 0.2917800, Train Acc: 0.8829406, Test Acc: 0.8106236\n",
            "Epoch: 028, Train Loss: 0.3164626, Train Acc: 0.8660348, Test Acc: 0.7875289\n",
            "Epoch: 029, Train Loss: 0.2602138, Train Acc: 0.8926742, Test Acc: 0.8129330\n",
            "Epoch: 030, Train Loss: 0.2625547, Train Acc: 0.8944672, Test Acc: 0.8083141\n",
            "Epoch: 031, Train Loss: 0.2661038, Train Acc: 0.8842213, Test Acc: 0.8198614\n",
            "Epoch: 032, Train Loss: 0.2310396, Train Acc: 0.9057377, Test Acc: 0.8060046\n",
            "Epoch: 033, Train Loss: 0.2417120, Train Acc: 0.9062500, Test Acc: 0.8175520\n",
            "Epoch: 034, Train Loss: 0.2565979, Train Acc: 0.8924180, Test Acc: 0.8198614\n",
            "Epoch: 035, Train Loss: 0.2619835, Train Acc: 0.8939549, Test Acc: 0.7921478\n",
            "Epoch: 036, Train Loss: 0.2683300, Train Acc: 0.8931865, Test Acc: 0.8314088\n",
            "Epoch: 037, Train Loss: 0.2615443, Train Acc: 0.8929303, Test Acc: 0.8406467\n",
            "Epoch: 038, Train Loss: 0.4011929, Train Acc: 0.8409324, Test Acc: 0.7436490\n",
            "Epoch: 039, Train Loss: 0.2258104, Train Acc: 0.9065061, Test Acc: 0.8429561\n",
            "Epoch: 040, Train Loss: 0.2213151, Train Acc: 0.9103484, Test Acc: 0.8060046\n",
            "Epoch: 041, Train Loss: 0.1855636, Train Acc: 0.9272541, Test Acc: 0.8152425\n",
            "Epoch: 042, Train Loss: 0.1814171, Train Acc: 0.9295594, Test Acc: 0.8221709\n",
            "Epoch: 043, Train Loss: 0.1614683, Train Acc: 0.9423668, Test Acc: 0.8036952\n",
            "Epoch: 044, Train Loss: 0.1819356, Train Acc: 0.9277664, Test Acc: 0.8129330\n",
            "Epoch: 045, Train Loss: 0.1491189, Train Acc: 0.9405738, Test Acc: 0.8337182\n",
            "Epoch: 046, Train Loss: 0.1858108, Train Acc: 0.9305840, Test Acc: 0.8036952\n",
            "Epoch: 047, Train Loss: 0.1893328, Train Acc: 0.9144467, Test Acc: 0.8175520\n",
            "Epoch: 048, Train Loss: 0.1874534, Train Acc: 0.9177766, Test Acc: 0.8083141\n",
            "Epoch: 049, Train Loss: 0.1430899, Train Acc: 0.9428791, Test Acc: 0.8429561\n",
            "Epoch: 050, Train Loss: 0.1417100, Train Acc: 0.9413422, Test Acc: 0.8406467\n",
            "Epoch: 051, Train Loss: 0.0944947, Train Acc: 0.9654201, Test Acc: 0.8198614\n",
            "Epoch: 052, Train Loss: 0.0952924, Train Acc: 0.9684939, Test Acc: 0.8036952\n",
            "Epoch: 053, Train Loss: 0.0859521, Train Acc: 0.9690061, Test Acc: 0.8083141\n",
            "Epoch: 054, Train Loss: 0.0706434, Train Acc: 0.9751537, Test Acc: 0.8244804\n",
            "Epoch: 055, Train Loss: 0.0733832, Train Acc: 0.9746414, Test Acc: 0.8083141\n",
            "Epoch: 056, Train Loss: 0.0657064, Train Acc: 0.9761783, Test Acc: 0.8290993\n",
            "Epoch: 057, Train Loss: 0.0485689, Train Acc: 0.9851434, Test Acc: 0.8337182\n",
            "Epoch: 058, Train Loss: 0.0668684, Train Acc: 0.9756660, Test Acc: 0.7990762\n",
            "Epoch: 059, Train Loss: 0.0671725, Train Acc: 0.9748975, Test Acc: 0.8267898\n",
            "Epoch: 060, Train Loss: 0.0669027, Train Acc: 0.9748975, Test Acc: 0.8337182\n",
            "Epoch: 061, Train Loss: 0.0452441, Train Acc: 0.9869365, Test Acc: 0.8244804\n",
            "Epoch: 062, Train Loss: 0.0497204, Train Acc: 0.9820697, Test Acc: 0.8175520\n",
            "Epoch: 063, Train Loss: 0.0609783, Train Acc: 0.9795082, Test Acc: 0.8221709\n",
            "Epoch: 064, Train Loss: 0.0534350, Train Acc: 0.9815574, Test Acc: 0.8083141\n",
            "Epoch: 065, Train Loss: 0.0723663, Train Acc: 0.9705430, Test Acc: 0.8129330\n",
            "Epoch: 066, Train Loss: 0.0469107, Train Acc: 0.9838627, Test Acc: 0.8221709\n",
            "Epoch: 067, Train Loss: 0.0581740, Train Acc: 0.9784836, Test Acc: 0.8267898\n",
            "Epoch: 068, Train Loss: 0.0448791, Train Acc: 0.9843750, Test Acc: 0.8244804\n",
            "Epoch: 069, Train Loss: 0.0508924, Train Acc: 0.9838627, Test Acc: 0.8129330\n",
            "Epoch: 070, Train Loss: 0.0309161, Train Acc: 0.9907787, Test Acc: 0.8337182\n",
            "Epoch: 071, Train Loss: 0.0956169, Train Acc: 0.9638832, Test Acc: 0.8060046\n",
            "Epoch: 072, Train Loss: 0.0329019, Train Acc: 0.9905225, Test Acc: 0.8198614\n",
            "Epoch: 073, Train Loss: 0.0551510, Train Acc: 0.9792520, Test Acc: 0.8175520\n",
            "Epoch: 074, Train Loss: 0.0371342, Train Acc: 0.9859119, Test Acc: 0.8198614\n",
            "Epoch: 075, Train Loss: 0.0550797, Train Acc: 0.9784836, Test Acc: 0.8337182\n",
            "Epoch: 076, Train Loss: 0.0319595, Train Acc: 0.9910348, Test Acc: 0.8129330\n",
            "Epoch: 077, Train Loss: 0.0453490, Train Acc: 0.9841189, Test Acc: 0.8152425\n",
            "Epoch: 078, Train Loss: 0.0466246, Train Acc: 0.9830943, Test Acc: 0.8221709\n",
            "Epoch: 079, Train Loss: 0.0257403, Train Acc: 0.9912910, Test Acc: 0.8314088\n",
            "Epoch: 080, Train Loss: 0.0197287, Train Acc: 0.9948770, Test Acc: 0.8083141\n",
            "Epoch: 081, Train Loss: 0.0307013, Train Acc: 0.9905225, Test Acc: 0.8244804\n",
            "Epoch: 082, Train Loss: 0.0543443, Train Acc: 0.9789959, Test Acc: 0.8106236\n",
            "Epoch: 083, Train Loss: 0.0219068, Train Acc: 0.9941086, Test Acc: 0.8314088\n",
            "Epoch: 084, Train Loss: 0.0198525, Train Acc: 0.9933402, Test Acc: 0.8083141\n",
            "Epoch: 085, Train Loss: 0.0207074, Train Acc: 0.9946209, Test Acc: 0.8267898\n",
            "Epoch: 086, Train Loss: 0.0383744, Train Acc: 0.9859119, Test Acc: 0.8244804\n",
            "Epoch: 087, Train Loss: 0.0416369, Train Acc: 0.9859119, Test Acc: 0.7990762\n",
            "Epoch: 088, Train Loss: 0.0336455, Train Acc: 0.9884734, Test Acc: 0.8290993\n",
            "Epoch: 089, Train Loss: 0.0195182, Train Acc: 0.9953893, Test Acc: 0.8221709\n",
            "Epoch: 090, Train Loss: 0.0121568, Train Acc: 0.9966701, Test Acc: 0.8383372\n",
            "Epoch: 091, Train Loss: 0.0268529, Train Acc: 0.9912910, Test Acc: 0.8267898\n",
            "Epoch: 092, Train Loss: 0.0240906, Train Acc: 0.9918033, Test Acc: 0.8314088\n",
            "Epoch: 093, Train Loss: 0.0169179, Train Acc: 0.9966701, Test Acc: 0.8337182\n",
            "Epoch: 094, Train Loss: 0.0223635, Train Acc: 0.9920594, Test Acc: 0.8106236\n",
            "Epoch: 095, Train Loss: 0.0162574, Train Acc: 0.9948770, Test Acc: 0.8198614\n",
            "Epoch: 096, Train Loss: 0.0274943, Train Acc: 0.9912910, Test Acc: 0.8152425\n",
            "Epoch: 097, Train Loss: 0.0220871, Train Acc: 0.9933402, Test Acc: 0.8290993\n",
            "Epoch: 098, Train Loss: 0.0269120, Train Acc: 0.9915471, Test Acc: 0.8013857\n",
            "Epoch: 099, Train Loss: 0.0375057, Train Acc: 0.9874488, Test Acc: 0.8152425\n",
            "Epoch: 100, Train Loss: 0.0339948, Train Acc: 0.9897541, Test Acc: 0.8337182\n",
            "Epoch: 101, Train Loss: 0.0078785, Train Acc: 0.9984631, Test Acc: 0.8244804\n",
            "Epoch: 102, Train Loss: 0.0051439, Train Acc: 0.9987193, Test Acc: 0.8244804\n",
            "Epoch: 103, Train Loss: 0.0036456, Train Acc: 0.9987193, Test Acc: 0.8267898\n",
            "Epoch: 104, Train Loss: 0.0026941, Train Acc: 0.9997439, Test Acc: 0.8244804\n",
            "Epoch: 105, Train Loss: 0.0047160, Train Acc: 0.9992316, Test Acc: 0.8337182\n",
            "Epoch: 106, Train Loss: 0.0021729, Train Acc: 0.9997439, Test Acc: 0.8221709\n",
            "Epoch: 107, Train Loss: 0.0045975, Train Acc: 0.9987193, Test Acc: 0.8267898\n",
            "Epoch: 108, Train Loss: 0.0032316, Train Acc: 0.9997439, Test Acc: 0.8267898\n",
            "Epoch: 109, Train Loss: 0.0040931, Train Acc: 0.9992316, Test Acc: 0.8244804\n",
            "Epoch: 110, Train Loss: 0.0019440, Train Acc: 0.9994877, Test Acc: 0.8129330\n",
            "Epoch: 111, Train Loss: 0.0016446, Train Acc: 0.9997439, Test Acc: 0.8175520\n",
            "Epoch: 112, Train Loss: 0.0042673, Train Acc: 0.9987193, Test Acc: 0.8198614\n",
            "Epoch: 113, Train Loss: 0.0077062, Train Acc: 0.9974385, Test Acc: 0.8129330\n",
            "Epoch: 114, Train Loss: 0.0020008, Train Acc: 1.0000000, Test Acc: 0.8267898\n",
            "Epoch: 115, Train Loss: 0.0012350, Train Acc: 1.0000000, Test Acc: 0.8314088\n",
            "Epoch: 116, Train Loss: 0.0030645, Train Acc: 0.9994877, Test Acc: 0.8267898\n",
            "Epoch: 117, Train Loss: 0.0063444, Train Acc: 0.9976947, Test Acc: 0.8290993\n",
            "Epoch: 118, Train Loss: 0.0032674, Train Acc: 0.9992316, Test Acc: 0.8406467\n",
            "Epoch: 119, Train Loss: 0.0240344, Train Acc: 0.9910348, Test Acc: 0.8106236\n",
            "Epoch: 120, Train Loss: 0.0053620, Train Acc: 0.9979508, Test Acc: 0.8314088\n",
            "--------\n",
            "Best Test Acc:   0.8409263181296258_0.016817868641761595, Epoch: 106\n",
            "Best Train Loss: 0.003032257225157488_0.0009779254934774578\n",
            "Best Train Acc:  0.9992314115362196_0.00046776314162387425\n",
            "FOLD9, Mutagenicity, 1668873489/_rhoE0.5P18I1_GELU_X_6_256_0_0.001_50_0.6_B64S0W8, BT=0.8409263181296258_0.016817868641761595, BE=106, ID=\n",
            "train_set : test_set = %d : %d 3904 433\n",
            "-------- FOLD9 DATASET=Mutagenicity, COMMIT_ID=\n",
            "#Params: 1332482\n",
            "Epoch: 001, Train Loss: 0.5649121, Train Acc: 0.6992828, Test Acc: 0.6882217\n",
            "Epoch: 002, Train Loss: 0.5026596, Train Acc: 0.7653689, Test Acc: 0.7551963\n",
            "Epoch: 003, Train Loss: 0.5147102, Train Acc: 0.7423156, Test Acc: 0.6951501\n",
            "Epoch: 004, Train Loss: 0.5034493, Train Acc: 0.7610143, Test Acc: 0.7205543\n",
            "Epoch: 005, Train Loss: 0.4734197, Train Acc: 0.7789447, Test Acc: 0.7551963\n",
            "Epoch: 006, Train Loss: 0.4860017, Train Acc: 0.7697234, Test Acc: 0.7459584\n",
            "Epoch: 007, Train Loss: 0.4546081, Train Acc: 0.7976434, Test Acc: 0.7528868\n",
            "Epoch: 008, Train Loss: 0.4964820, Train Acc: 0.7617828, Test Acc: 0.7436490\n",
            "Epoch: 009, Train Loss: 0.4315747, Train Acc: 0.8055840, Test Acc: 0.7575058\n",
            "Epoch: 010, Train Loss: 0.4103108, Train Acc: 0.8168545, Test Acc: 0.7736721\n",
            "Epoch: 011, Train Loss: 0.4312927, Train Acc: 0.8094262, Test Acc: 0.7713626\n",
            "Epoch: 012, Train Loss: 0.4029930, Train Acc: 0.8314549, Test Acc: 0.7990762\n",
            "Epoch: 013, Train Loss: 0.3895095, Train Acc: 0.8329918, Test Acc: 0.7875289\n",
            "Epoch: 014, Train Loss: 0.3989895, Train Acc: 0.8199283, Test Acc: 0.7736721\n",
            "Epoch: 015, Train Loss: 0.3540657, Train Acc: 0.8527152, Test Acc: 0.8106236\n",
            "Epoch: 016, Train Loss: 0.3583409, Train Acc: 0.8537398, Test Acc: 0.7875289\n",
            "Epoch: 017, Train Loss: 0.3551064, Train Acc: 0.8491291, Test Acc: 0.8036952\n",
            "Epoch: 018, Train Loss: 0.3170349, Train Acc: 0.8675717, Test Acc: 0.7829099\n",
            "Epoch: 019, Train Loss: 0.3286140, Train Acc: 0.8639857, Test Acc: 0.8129330\n",
            "Epoch: 020, Train Loss: 0.3189968, Train Acc: 0.8691086, Test Acc: 0.8013857\n",
            "Epoch: 021, Train Loss: 0.3451242, Train Acc: 0.8588627, Test Acc: 0.7782910\n",
            "Epoch: 022, Train Loss: 0.2968579, Train Acc: 0.8719262, Test Acc: 0.8106236\n",
            "Epoch: 023, Train Loss: 0.3218584, Train Acc: 0.8627049, Test Acc: 0.7990762\n",
            "Epoch: 024, Train Loss: 0.3074370, Train Acc: 0.8773053, Test Acc: 0.8106236\n",
            "Epoch: 025, Train Loss: 0.2709614, Train Acc: 0.8862705, Test Acc: 0.8036952\n",
            "Epoch: 026, Train Loss: 0.2771294, Train Acc: 0.8896004, Test Acc: 0.8060046\n",
            "Epoch: 027, Train Loss: 0.2878818, Train Acc: 0.8814037, Test Acc: 0.7967667\n",
            "Epoch: 028, Train Loss: 0.2576206, Train Acc: 0.8993340, Test Acc: 0.8152425\n",
            "Epoch: 029, Train Loss: 0.2550539, Train Acc: 0.8954918, Test Acc: 0.7921478\n",
            "Epoch: 030, Train Loss: 0.2368241, Train Acc: 0.9036885, Test Acc: 0.8036952\n",
            "Epoch: 031, Train Loss: 0.2380045, Train Acc: 0.9082992, Test Acc: 0.7944573\n",
            "Epoch: 032, Train Loss: 0.2654586, Train Acc: 0.8883197, Test Acc: 0.7944573\n",
            "Epoch: 033, Train Loss: 0.2326670, Train Acc: 0.9082992, Test Acc: 0.8036952\n",
            "Epoch: 034, Train Loss: 0.2106136, Train Acc: 0.9172643, Test Acc: 0.8267898\n",
            "Epoch: 035, Train Loss: 0.2226971, Train Acc: 0.9090676, Test Acc: 0.7990762\n",
            "Epoch: 036, Train Loss: 0.2922007, Train Acc: 0.8683402, Test Acc: 0.7921478\n",
            "Epoch: 037, Train Loss: 0.2362135, Train Acc: 0.9039447, Test Acc: 0.8129330\n",
            "Epoch: 038, Train Loss: 0.2723376, Train Acc: 0.8829406, Test Acc: 0.7806005\n",
            "Epoch: 039, Train Loss: 0.1749756, Train Acc: 0.9364754, Test Acc: 0.8083141\n",
            "Epoch: 040, Train Loss: 0.1544151, Train Acc: 0.9372439, Test Acc: 0.7898383\n",
            "Epoch: 041, Train Loss: 0.1675899, Train Acc: 0.9351947, Test Acc: 0.8152425\n",
            "Epoch: 042, Train Loss: 0.1510766, Train Acc: 0.9403176, Test Acc: 0.8036952\n",
            "Epoch: 043, Train Loss: 0.1912874, Train Acc: 0.9195697, Test Acc: 0.7921478\n",
            "Epoch: 044, Train Loss: 0.1863333, Train Acc: 0.9236680, Test Acc: 0.7944573\n",
            "Epoch: 045, Train Loss: 0.2102326, Train Acc: 0.9070184, Test Acc: 0.8198614\n",
            "Epoch: 046, Train Loss: 0.1679962, Train Acc: 0.9305840, Test Acc: 0.7806005\n",
            "Epoch: 047, Train Loss: 0.1349316, Train Acc: 0.9474898, Test Acc: 0.7967667\n",
            "Epoch: 048, Train Loss: 0.1287458, Train Acc: 0.9490266, Test Acc: 0.8129330\n",
            "Epoch: 049, Train Loss: 0.1738259, Train Acc: 0.9287910, Test Acc: 0.7690531\n",
            "Epoch: 050, Train Loss: 0.1060461, Train Acc: 0.9631148, Test Acc: 0.8060046\n",
            "Epoch: 051, Train Loss: 0.0726888, Train Acc: 0.9728484, Test Acc: 0.8314088\n",
            "Epoch: 052, Train Loss: 0.0726453, Train Acc: 0.9736168, Test Acc: 0.8106236\n",
            "Epoch: 053, Train Loss: 0.0937780, Train Acc: 0.9646516, Test Acc: 0.8129330\n",
            "Epoch: 054, Train Loss: 0.0621904, Train Acc: 0.9769467, Test Acc: 0.7967667\n",
            "Epoch: 055, Train Loss: 0.0649941, Train Acc: 0.9774590, Test Acc: 0.8060046\n",
            "Epoch: 056, Train Loss: 0.0557813, Train Acc: 0.9833504, Test Acc: 0.8106236\n",
            "Epoch: 057, Train Loss: 0.0540086, Train Acc: 0.9810451, Test Acc: 0.8221709\n",
            "Epoch: 058, Train Loss: 0.0967493, Train Acc: 0.9631148, Test Acc: 0.8036952\n",
            "Epoch: 059, Train Loss: 0.0540926, Train Acc: 0.9815574, Test Acc: 0.7944573\n",
            "Epoch: 060, Train Loss: 0.0827697, Train Acc: 0.9692623, Test Acc: 0.8175520\n",
            "Epoch: 061, Train Loss: 0.0685742, Train Acc: 0.9725922, Test Acc: 0.7898383\n",
            "Epoch: 062, Train Loss: 0.0730854, Train Acc: 0.9710553, Test Acc: 0.8175520\n",
            "Epoch: 063, Train Loss: 0.0437724, Train Acc: 0.9851434, Test Acc: 0.7944573\n",
            "Epoch: 064, Train Loss: 0.0515915, Train Acc: 0.9828381, Test Acc: 0.8060046\n",
            "Epoch: 065, Train Loss: 0.0584219, Train Acc: 0.9759221, Test Acc: 0.7990762\n",
            "Epoch: 066, Train Loss: 0.0410991, Train Acc: 0.9879611, Test Acc: 0.8221709\n",
            "Epoch: 067, Train Loss: 0.0347874, Train Acc: 0.9882172, Test Acc: 0.8221709\n",
            "Epoch: 068, Train Loss: 0.0477540, Train Acc: 0.9833504, Test Acc: 0.8036952\n",
            "Epoch: 069, Train Loss: 0.0396602, Train Acc: 0.9871926, Test Acc: 0.8175520\n",
            "Epoch: 070, Train Loss: 0.0286642, Train Acc: 0.9887295, Test Acc: 0.8106236\n",
            "Epoch: 071, Train Loss: 0.0297516, Train Acc: 0.9894980, Test Acc: 0.8129330\n",
            "Epoch: 072, Train Loss: 0.0495415, Train Acc: 0.9843750, Test Acc: 0.7875289\n",
            "Epoch: 073, Train Loss: 0.0296727, Train Acc: 0.9907787, Test Acc: 0.8175520\n",
            "Epoch: 074, Train Loss: 0.0296167, Train Acc: 0.9918033, Test Acc: 0.7967667\n",
            "Epoch: 075, Train Loss: 0.0323999, Train Acc: 0.9894980, Test Acc: 0.8036952\n",
            "Epoch: 076, Train Loss: 0.0280021, Train Acc: 0.9910348, Test Acc: 0.8129330\n",
            "Epoch: 077, Train Loss: 0.0377222, Train Acc: 0.9869365, Test Acc: 0.8013857\n",
            "Epoch: 078, Train Loss: 0.0425129, Train Acc: 0.9836066, Test Acc: 0.8221709\n",
            "Epoch: 079, Train Loss: 0.0292161, Train Acc: 0.9905225, Test Acc: 0.7736721\n",
            "Epoch: 080, Train Loss: 0.0283614, Train Acc: 0.9877049, Test Acc: 0.8083141\n",
            "Epoch: 081, Train Loss: 0.0424326, Train Acc: 0.9856557, Test Acc: 0.7944573\n",
            "Epoch: 082, Train Loss: 0.0246206, Train Acc: 0.9918033, Test Acc: 0.7990762\n",
            "Epoch: 083, Train Loss: 0.0155012, Train Acc: 0.9964139, Test Acc: 0.7967667\n",
            "Epoch: 084, Train Loss: 0.0164598, Train Acc: 0.9953893, Test Acc: 0.8129330\n",
            "Epoch: 085, Train Loss: 0.0196790, Train Acc: 0.9938525, Test Acc: 0.8106236\n",
            "Epoch: 086, Train Loss: 0.0289662, Train Acc: 0.9905225, Test Acc: 0.8083141\n",
            "Epoch: 087, Train Loss: 0.0440802, Train Acc: 0.9841189, Test Acc: 0.8013857\n",
            "Epoch: 088, Train Loss: 0.0312115, Train Acc: 0.9877049, Test Acc: 0.8175520\n",
            "Epoch: 089, Train Loss: 0.0231476, Train Acc: 0.9923156, Test Acc: 0.8152425\n",
            "Epoch: 090, Train Loss: 0.0181585, Train Acc: 0.9946209, Test Acc: 0.8083141\n",
            "Epoch: 091, Train Loss: 0.0205472, Train Acc: 0.9920594, Test Acc: 0.8013857\n",
            "Epoch: 092, Train Loss: 0.0186419, Train Acc: 0.9935963, Test Acc: 0.8013857\n",
            "Epoch: 093, Train Loss: 0.0152106, Train Acc: 0.9953893, Test Acc: 0.8129330\n",
            "Epoch: 094, Train Loss: 0.0205136, Train Acc: 0.9920594, Test Acc: 0.8175520\n",
            "Epoch: 095, Train Loss: 0.0124269, Train Acc: 0.9969262, Test Acc: 0.8083141\n",
            "Epoch: 096, Train Loss: 0.0159798, Train Acc: 0.9946209, Test Acc: 0.8060046\n",
            "Epoch: 097, Train Loss: 0.0262284, Train Acc: 0.9910348, Test Acc: 0.8013857\n",
            "Epoch: 098, Train Loss: 0.0372135, Train Acc: 0.9882172, Test Acc: 0.8083141\n",
            "Epoch: 099, Train Loss: 0.0214846, Train Acc: 0.9923156, Test Acc: 0.7967667\n",
            "Epoch: 100, Train Loss: 0.0631413, Train Acc: 0.9761783, Test Acc: 0.7736721\n",
            "Epoch: 101, Train Loss: 0.0159957, Train Acc: 0.9946209, Test Acc: 0.8129330\n",
            "Epoch: 102, Train Loss: 0.0052519, Train Acc: 0.9987193, Test Acc: 0.8175520\n",
            "Epoch: 103, Train Loss: 0.0043415, Train Acc: 0.9989754, Test Acc: 0.8106236\n",
            "Epoch: 104, Train Loss: 0.0032157, Train Acc: 0.9994877, Test Acc: 0.8152425\n",
            "Epoch: 105, Train Loss: 0.0028927, Train Acc: 0.9987193, Test Acc: 0.8175520\n",
            "Epoch: 106, Train Loss: 0.0032378, Train Acc: 0.9989754, Test Acc: 0.8152425\n",
            "Epoch: 107, Train Loss: 0.0039016, Train Acc: 0.9989754, Test Acc: 0.8013857\n",
            "Epoch: 108, Train Loss: 0.0038998, Train Acc: 0.9987193, Test Acc: 0.8221709\n",
            "Epoch: 109, Train Loss: 0.0020455, Train Acc: 0.9997439, Test Acc: 0.8106236\n",
            "Epoch: 110, Train Loss: 0.0029603, Train Acc: 0.9994877, Test Acc: 0.8129330\n",
            "Epoch: 111, Train Loss: 0.0051118, Train Acc: 0.9982070, Test Acc: 0.7967667\n",
            "Epoch: 112, Train Loss: 0.0045642, Train Acc: 0.9989754, Test Acc: 0.8175520\n",
            "Epoch: 113, Train Loss: 0.0033094, Train Acc: 0.9994877, Test Acc: 0.8129330\n",
            "Epoch: 114, Train Loss: 0.0090507, Train Acc: 0.9974385, Test Acc: 0.8106236\n",
            "Epoch: 115, Train Loss: 0.0080151, Train Acc: 0.9964139, Test Acc: 0.8060046\n",
            "Epoch: 116, Train Loss: 0.0038524, Train Acc: 0.9982070, Test Acc: 0.8036952\n",
            "Epoch: 117, Train Loss: 0.0055257, Train Acc: 0.9989754, Test Acc: 0.8013857\n",
            "Epoch: 118, Train Loss: 0.0144626, Train Acc: 0.9953893, Test Acc: 0.7990762\n",
            "Epoch: 119, Train Loss: 0.0123097, Train Acc: 0.9964139, Test Acc: 0.7921478\n",
            "Epoch: 120, Train Loss: 0.0095635, Train Acc: 0.9969262, Test Acc: 0.8198614\n",
            "--------\n",
            "Best Test Acc:   0.836972786581667_0.019276623865989738, Epoch: 104\n",
            "Best Train Loss: 0.003296456017965943_0.001307445950913305\n",
            "Best Train Acc:  0.9991289277268851_0.0005275790384111465\n",
            "FOLD10, Mutagenicity, 1668873489/_rhoE0.5P18I1_GELU_X_6_256_0_0.001_50_0.6_B64S0W8, BT=0.836972786581667_0.019276623865989738, BE=104, ID=\n"
          ]
        }
      ],
      "source": [
        "!python -u ./main.py --config=\"./configs/Mutagenicity.json\""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "2Vajv7mirM1c",
        "cwllrExEyrmK",
        "84DuCmMwqmJ5",
        "Jv2-1nMlI6dP",
        "v-OjpbwNryl3",
        "Qe1gwFrss2ug",
        "OlxjXIOHvs9C",
        "6WxpI76ss-lR",
        "IHgUFdidmWYH",
        "FFiGuayjuXfj",
        "7LdsNxcEvW2v",
        "_CyMDlsv_TeT",
        "n9bkUAmD_am_",
        "zijOLV2iGllS",
        "xE1dlDA__sN9",
        "FfXPzpsx-nS5",
        "XZvgHVJ5X832",
        "8zRSDfJW-Cov",
        "ujSGqIlh-Egm",
        "hPO-s3FhbM1W",
        "2ppe8BGSbdeS",
        "taf9wQ5peOsD",
        "2-mhje3jfgWS",
        "nDMNGaSBl0np",
        "rcuPmrpy6STP",
        "9bfH6SRQinBA",
        "QzORbkWuaPig",
        "oX6mNpIeoNUK",
        "rc6kvXJZfzNX",
        "ZU36zG43f-P1",
        "2qL-z5N92JFB",
        "4n6zFWO7d8aQ",
        "JcKMQwtWel_C",
        "eG6tmQzolcdY",
        "Za0ZdINim7Oe",
        "T9PwnbNJ4irI",
        "Qq5CCNuY4mia",
        "sZ_ArOYHpdjj",
        "V3piLlA1olB7",
        "UXwNcDOFCZPv",
        "dM4eOqpHCZPy",
        "HekNT_rCU6Dh",
        "05vESKWppM2M",
        "lvQlpsB-pdjs",
        "OK8R0pc4pdj4",
        "XL-x5jgRlXW3",
        "p-jnD_cKlc0u",
        "ZlE1-Hilpdj_",
        "Jden4ZZLpdkC",
        "wX2pFDwepdkF",
        "g3_-Vg6oxuXu",
        "DVdR3UoB_GoA",
        "Zrn2u_fUElYi",
        "EKjCtAbYg3sC",
        "XxLH-riEgriQ",
        "UZJY5Mq8nR99",
        "_LUcsqWO1QVs",
        "IGL2Xrsd1VtP",
        "k6oQHt_s1kde"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
